{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>《Kaggle Top 1%方案精讲与实践》"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font face=\"仿宋\">课程说明："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">小伙伴好呀\\~欢迎来到《2021机器学习实战训练营》试学体验课！我是课程主讲老师，九天。       \n",
    "&emsp;&emsp;本次体验课为期三天（12月8-10号），期间每晚8点在我的B站直播间公开直播，直播间地址:https://live.bilibili.com/22678166      \n",
    "&emsp;&emsp;本期公开课的内容是在上一轮Kaggle竞赛公开课（《Elo Merchant Category Recommendation》）的基础上，进一步探讨如何进一步将排名提升至1%。在接下来的三天内容中，我们将进一步尝试多模型建模与优化、模型融合方法、特征优化与Trick优化等方法，从而在此前的结果基础上大幅提高模型预测准确率。        \n",
    "&emsp;&emsp;当然，没有参与上一轮公开课的小伙伴也不用担心，本轮公开课将在开始时我们将快速回顾上一轮公开课内容，并将提供上一轮公开课最终完成的数据处理结果与相关代码，帮助大家无门槛进入本轮课程内容的学习中。当然，如果时间允许，也希望大家能够通过观看上一轮公开课的直播录屏，以巩固相关知识。上一轮公开课直播录屏地址：https://www.bilibili.com/video/BV1QU4y1u7Ph      \n",
    "&emsp;&emsp;课程资料/赛题数据/课程代码/付费课程信息，扫码添加客服“小可爱”回复【kaggle】即可领取哦~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.loli.net/2021/10/20/ZWTgxSiNY1db9eL.png\" alt=\"二维码\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">另外，双十二年终大促持续进行中，十八周80+课时体系大课限时七折，扫码咨询小可爱回复“优惠”，还可领取额外折上折优惠，课程主页：https://appze9inzwc2314.pc.xiaoe-tech.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>【Kaggle】Elo Merchant Category Recommendation    \n",
    "# <center> 竞赛案例解析公开课 Part II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Day 6.特征优化与模型融合优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm,tqdm_notebook \n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from scipy import sparse\n",
    "import warnings\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import log_loss\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('max_colwidth',100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>一、优化思路梳理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 课前准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在昨天的内容中，我们通过使用更强的集成模型以及模型融合的方法，已经顺利将比赛分数提高至前20%。但正如此前所说，之前的一系列操作只不过是遵循了常规操作流程进行的数据处理与建模，若希望能够更进一步提高模型分数，则需要因地制宜、活学活用，在考虑到当前数据及特殊情况下进行有针对性的策略调整。本节内容我们将结合此前搜集到的所有数据集信息及业务背景信息，进行最后一轮的特征优化与模型优化，并最终将排名提升至前1%。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然，这个过程并不简单，若需要跟上本节内容的讨论，需要非常熟悉当前数据集的基本情况，也就是需要深度掌握Day1-2所介绍内容，从而才能理解接下来的特征优化相关内容；此外也需要对Day 5中介绍的集成学习建模策略，即在使用原生算法库情况下，如何配合交叉验证过程、借助贝叶斯优化器进行超参数搜索，并最终输出交叉验证的测试集预测均值作为最终预测结果的一整个流程，从而才能更快速的理解本节开始我们对模型训练流程进行的优化与调整；此外，我还将在本节介绍非常适用于竞赛场景的融合技巧，亦可作为同学日后参与竞赛时的有力工具。不过没有跟上此前的内容的同学也不用担心，本节内容将更加强调优化过程的整体逻辑，并尽可能从一个更加通俗且准确的角度进行解释，大家也可以在听完本节内容后再去回顾此前Day1-5的相关内容，以终为始、未尝不可，通过反复观看，也相信大家会对本节内容有一个更深刻的理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 整体优化思路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;对于机器学习来说，总的来看有两种建模思路，其一是通过特征工程方法进一步提升数据质量，其二则是通过更加复杂的模型或更加有效的模型融合技巧来提升建模效果，并且就二者的关系来看，正如时下流行的观点所说，特征工程将决定模型效果上界，而建模过程则会不断逼近这个上界。但无论如何，在优化的过程中，需要二者配合执行才能达到更好的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/10/yfdwsnt2okYpxmR.png\" alt=\"image-20211210125340547\" style=\"zoom:20%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.特征优化思路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先，先来看特征优化思路。在此前的建模过程中，我们曾不止一次的对特征进行了处理，首先是在数据聚合时（以card_id进行聚合），为了尽可能提取更多的交易数据信息与商户信息带入进行模型，我们围绕交易数据表和商户数据表进行了工程化批量特征衍生，彼时信息提取流程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/07/9jwd1meblXaoUzh.png\" alt=\"image-20211207235615308\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体的特征衍生方案是采用了交叉组合特征创建与业务统计特征创建两种方案："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/08/gsfO4t5cBa7q61p.png\" alt=\"image-20211208002218370\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;该过程的详细讲解，可参考Day 3-Day 4的课程内容。总而言之，通过该过程，我们顺利的提取了交易信息表和商户信息表中的数据带入进行建模，并且借助随机森林模型，顺利跑通Baseline。但值得一提的是，在上述流程中，我们其实只是采用了一些工程化的通用做法，这些方法是可以快速适用于任何数据集的特征衍生环节，同时这样的方法也应该是所有建模开始前必须尝试的做法，但既然是“通用”方法，那必然无法帮我们在实际竞赛中脱颖而出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然，我们也曾尝试过进行有针对性的特征优化，在Day 5的内容中，我们曾采用NLP方法用于提取特征ID列的信息，并得到了一系列能够更加细致描述用户行为信息与商品偏好的特征，借助该特征，我们最终训练得出了一个效果更好的模型，该结果也进一步验证了特征优化对模型效果提升所能起到的作用。接下来我怕们也将尝试进一步进行有针对性的特征优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;总体来看，特征优化需要结合数据集当前的实际情况来制定，在已有批量衍生的特征及NLP特征的基础上，针对上述数据集，还可以有以下几点优化方向："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用户行为特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先，我们注意到，每一笔信用卡的交易记录都有交易时间，而对于时间字段和文本字段，普通的批量创建特征的方法都是无法较好的挖掘其全部信息的，因此我们需要围绕交易字段中的交易时间进行额外的特征衍生。此处我们可以考虑构造一些用于描述用户行为习惯的特征（经过反复验证，用户行为特征是最为有效的提高预测结果的特征类），包括最近一次交易与首次交易的时间差、信用卡激活日期与首次交易的时间差、用户两次交易平均时间间隔、按照不同交易地点/商品品类进行聚合（并统计均值、方差等统计量）。      \n",
    "&emsp;&emsp;此外，我们也知道越是接近当前时间点的用户行为越有价值，因此我们还需要重点关注用户最近两个月（实际时间跨度可以自行决定）的行为特征，以两个月为跨度，进一步统计该时间周期内用户的上述交易行为特点，并带入模型进行训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 二阶交叉特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在此前的特征衍生过程中，我们曾进行了交叉特征衍生，但只是进行了一阶交叉衍生，例如交易额在不同商品上的汇总，但实际上还可以进一步构造二阶衍生，例如交易额在不同商品组合上的汇总。通常来说更高阶的衍生会导致特征矩阵变得更加稀疏，并且由于每一阶的衍生都会创造大量特征，因此更高阶的衍生往往也会造成维度爆炸，因此高阶交叉特征衍生需要谨慎。不过正如此前我们考虑的，由于用户行为特征对模型结果有更大的影响，因此我们可以单独围绕用户行为数据进行二阶交叉特征衍生，并在后续建模前进行特征筛选。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 异常值识别特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在Day 1的数据探索中我们就发现，训练数据集的标签中存在少量极端异常值的情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/10/neGfhCq5xBcO9uL.png\" alt=\"image-20211210155414409\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而若我们更进一步就此前的建模结果展开分析的话，我们会发现，此前的模型误差大多数都源于对异常值用户(card_id)的预测结果。而根据Day 1的讨论我们知道，实际上用户评分是通过某套公式人工计算得出的，因此这些异常值极有可能是某类特殊用户的标记，因此我们不妨在实际建模过程中进行两阶段建模，即先预测每个输入样本是否是异常样本，并根据分类预测结果进行后续的回归建模，基本流程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/10/PtW9zf6EpchjK4o.png\" alt=\"image-20211210161251760\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而为了保证后续两阶段建模时第一阶段的分类模型能够更加准确的识别异常用户，我们需要创建一些基于异常用户的特征聚合字段，例如异常用户平均消费次数、消费金额等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.模型优化思路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 新增CatBoost模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;相比特征优化思路，模型优化的思路相对简单，首先从模型选择来看，相较LightGBM和XGBoost，随机森林对当前数据集的预测能力较弱，可以考虑将其换成集成算法新秀：CatBoost。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;CatBoost是由俄罗斯搜索引擎Yandex在2017年7月开源的一个GBM算法，自开源之日起，CatBoost就因为其强大的效果与极快的执行效率广受算法工程人员青睐。在诸多CatBoost的算法优势中，最引人关注的当属该模型能够自主采用独热编码和平均编码的混合策略来处理类别特征，也就是说CatBoost将一些经过实践验证的、普遍有效的特征工程方法融入了实际模型训练过程；此外，CatBoost还提出了一种全新的梯度提升的机制，能够非常好的在经验风险和结构风险中做出权衡（即能够很好的提升精度、同时又能够避免过拟合问题）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而在后续的建模环节中，我们就将使用CatBoost替换随机森林，并最终带入CatBoost、XGBoost和LightGBM三个模型进行模型融合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 二阶段建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而从模型训练流程角度出发，则可以考虑进行二阶段建模，基本流程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/10/ioPh1C5uzVgE9dZ.png\" alt=\"image-20211210164131623\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;并且，需要注意的是，在实际二阶段建模过程时，我们需要在每个建模阶段都进行交叉验证与模型融合，才能最大化提升模型效果。也就是说我们需要训练三组模型（以及对应进行三次模型融合），来完成分类预测问题、普通用户回归预测问题和异常用户回归预测问题。三轮建模关系如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/10/a19mJpQOkMunDbf.png\" alt=\"image-20211210165529875\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不难发现，整体建模与融合过程都将变得更加复杂。不过这也是更加贴近真实状态的一种情况，很多时候算法和融合过程都只是元素，如何构建一个更加精准、高效的训练流程，才是进阶的算法工程人员更需要考虑的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.其他注意事项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 内存管理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;由于接下来我们需要反复读取数据文件并进行计算，因此需要时刻注意进行内存管理，除了可以通过及时删除不用的变量并使用动态垃圾回收机制来清理内存外，还可以使用如下方式在定义数据类型时尽可能在不影响数值运算的前提下给出更加合适的数据类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 48.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "new_transactions = pd.read_csv('data/new_merchant_transactions.csv', parse_dates=['purchase_date'])\n",
    "historical_transactions = pd.read_csv('data/historical_transactions.csv', parse_dates=['purchase_date'])\n",
    "for col in ['authorized_flag', 'category_1']:\n",
    "    historical_transactions[col] = historical_transactions[col].map({'Y':1, 'N':0})\n",
    "    new_transactions[col]        = new_transactions[col].map({'Y':1, 'N':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 代码管理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;本部分代码量较大，尽管课上是通过Notebook进行展示，但如果在实际建模过程时，更建议通过自定义模块来存储和调用大段代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>二、数据预处理与特征优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;此处由于需要进行特征优化，因此数据处理工作需要从头开始执行。首先需要进行数据读取，并进行缺失值处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## 加载训练集，测试集，基本处理\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "target = train['target']\n",
    "for df in [train, test]:    \n",
    "    df['year']  = df['first_active_month'].fillna('0-0').apply(lambda x:int(str(x).split('-')[0]))\n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "    df['elapsed_time'] = (datetime.date(2018,3, 1) - df['first_active_month'].dt.date).dt.days\n",
    "    \n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "    df['weekofyear'] = df['first_active_month'].dt.weekofyear\n",
    "    df['dayofyear'] = df['first_active_month'].dt.dayofyear\n",
    "    df['month'] = df['first_active_month'].dt.month\n",
    "    \n",
    "## 交易表合并train test\n",
    "train_test = pd.concat([train[['card_id','first_active_month']], test[['card_id','first_active_month']] ], axis=0, ignore_index=True)\n",
    "historical_transactions   = historical_transactions.merge(train_test[['card_id','first_active_month']], on=['card_id'], how='left')\n",
    "new_transactions = new_transactions.merge(train_test[['card_id','first_active_month']], on=['card_id'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后需要进行时间字段的更细粒度的呈现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 3192.83 Mb (63.6% reduction)\n",
      "Mem. usage decreased to 211.55 Mb (64.2% reduction)\n",
      "Wall time: 3min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def month_trans(x): \n",
    "    return x // 30\n",
    "\n",
    "def week_trans(x): \n",
    "    return x // 7\n",
    "\n",
    "## 交易表预处理\n",
    "def get_expand_common(df_):\n",
    "    df = df_.copy()\n",
    "    \n",
    "    df['category_2'].fillna(1.0,inplace=True)\n",
    "    df['category_3'].fillna('A',inplace=True)\n",
    "    df['category_3'] = df['category_3'].map({'A':0, 'B':1, 'C':2})\n",
    "    df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "    df['installments'].replace(-1, np.nan,inplace=True)\n",
    "    df['installments'].replace(999, np.nan,inplace=True)\n",
    "    df['installments'].replace(0, 1,inplace=True)\n",
    "    \n",
    "    df['purchase_amount'] = np.round(df['purchase_amount'] / 0.00150265118 + 497.06,8)\n",
    "    df['purchase_amount'] = df.purchase_amount.apply(lambda x: np.round(x))\n",
    "    \n",
    "    df['purchase_date']          =  pd.to_datetime(df['purchase_date']) \n",
    "    df['first_active_month']     =  pd.to_datetime(df['first_active_month']) \n",
    "    df['purchase_hour']          =  df['purchase_date'].dt.hour\n",
    "    df['year']                   = df['purchase_date'].dt.year\n",
    "    df['month']                  =  df['purchase_date'].dt.month\n",
    "    df['day']                    = df['purchase_date'].dt.day\n",
    "    df['hour']                   = df['purchase_date'].dt.hour\n",
    "    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n",
    "    df['dayofweek']              =  df['purchase_date'].dt.dayofweek\n",
    "    df['weekend']                =  (df.purchase_date.dt.weekday >=5).astype(int) \n",
    "    df                           =  df.sort_values(['card_id','purchase_date']) \n",
    "    df['purchase_date_floorday'] =  df['purchase_date'].dt.floor('d')  #删除小于day的时间\n",
    "    \n",
    "    # 距离激活时间的相对时间,0, 1,2,3,...,max-act\n",
    "    df['purchase_day_since_active_day']   = df['purchase_date_floorday'] - df['first_active_month']  #ht_card_id_gp['purchase_date_floorday'].transform('min')\n",
    "    df['purchase_day_since_active_day']   = df['purchase_day_since_active_day'].dt.days  #.astype('timedelta64[D]') \n",
    "    df['purchase_month_since_active_day'] = df['purchase_day_since_active_day'].agg(month_trans).values\n",
    "    df['purchase_week_since_active_day']  = df['purchase_day_since_active_day'].agg(week_trans).values\n",
    "    \n",
    "    # 距离最后一天时间的相对时间,0,1,2,3,...,max-act\n",
    "    ht_card_id_gp = df.groupby('card_id')\n",
    "    df['purchase_day_since_reference_day']   =  ht_card_id_gp['purchase_date_floorday'].transform('max') - df['purchase_date_floorday']\n",
    "    df['purchase_day_since_reference_day']   =  df['purchase_day_since_reference_day'].dt.days\n",
    "    # 一个粗粒度的特征(距离最近购买过去了几周，几月)\n",
    "    df['purchase_week_since_reference_day']  = df['purchase_day_since_reference_day'].agg(week_trans).values\n",
    "    df['purchase_month_since_reference_day'] = df['purchase_day_since_reference_day'].agg(month_trans).values\n",
    "    \n",
    "    df['purchase_day_diff']   =  df['purchase_date_floorday'].shift()\n",
    "    df['purchase_day_diff']   =  df['purchase_date_floorday'].values - df['purchase_day_diff'].values\n",
    "    df['purchase_day_diff']   =  df['purchase_day_diff'].dt.days\n",
    "    df['purchase_week_diff']  =  df['purchase_day_diff'].agg(week_trans).values\n",
    "    df['purchase_month_diff'] =  df['purchase_day_diff'].agg(month_trans).values \n",
    "    \n",
    "    df['purchase_amount_ddgd_98']  = df['purchase_amount'].values * df['purchase_day_since_reference_day'].apply(lambda x:0.98**x).values\n",
    "    df['purchase_amount_ddgd_99']  = df['purchase_amount'].values * df['purchase_day_since_reference_day'].apply(lambda x:0.99**x).values    \n",
    "    df['purchase_amount_wdgd_96']  = df['purchase_amount'].values * df['purchase_week_since_reference_day'].apply(lambda x:0.96**x).values \n",
    "    df['purchase_amount_wdgd_97']  = df['purchase_amount'].values * df['purchase_week_since_reference_day'].apply(lambda x:0.97**x).values \n",
    "    df['purchase_amount_mdgd_90']  = df['purchase_amount'].values * df['purchase_month_since_reference_day'].apply(lambda x:0.9**x).values\n",
    "    df['purchase_amount_mdgd_80']  = df['purchase_amount'].values * df['purchase_month_since_reference_day'].apply(lambda x:0.8**x).values \n",
    "    \n",
    "    df = reduce_mem_usage(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "historical_transactions = get_expand_common(historical_transactions)\n",
    "new_transactions        = get_expand_common(new_transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.特征优化部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在执行完数据清洗与时间字段的处理之后，接下来我们需要开始执行特征优化。根据此前介绍的思路，首先我们需要进行基础行为特征字段衍生："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate statistics features...\n",
      "generate statistics features...\n",
      "generate statistics features...\n",
      "Wall time: 2min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## 构造基本统计特征\n",
    "def aggregate_transactions(df_, prefix): \n",
    "    \n",
    "    df = df_.copy()\n",
    "    \n",
    "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] = df['month_diff'].astype(int)\n",
    "    df['month_diff'] += df['month_lag']\n",
    "    \n",
    "    df['price'] = df['purchase_amount'] / df['installments']\n",
    "    df['duration'] = df['purchase_amount'] * df['month_diff']\n",
    "    df['amount_month_ratio'] = df['purchase_amount'] / df['month_diff']\n",
    "    \n",
    "    df.loc[:, 'purchase_date'] = pd.DatetimeIndex(df['purchase_date']).\\\n",
    "                                      astype(np.int64) * 1e-9\n",
    "    \n",
    "    agg_func = {\n",
    "        'category_1':      ['mean'],\n",
    "        'category_2':      ['mean'],\n",
    "        'category_3':      ['mean'],\n",
    "        'installments':    ['mean', 'max', 'min', 'std'],\n",
    "        'month_lag':       ['nunique', 'mean', 'max', 'min', 'std'],\n",
    "        'month':           ['nunique', 'mean', 'max', 'min', 'std'],\n",
    "        'hour':            ['nunique', 'mean', 'max', 'min', 'std'],\n",
    "        'weekofyear':      ['nunique', 'mean', 'max', 'min', 'std'],\n",
    "        'dayofweek':       ['nunique', 'mean'],\n",
    "        'weekend':         ['mean'],\n",
    "        'year':            ['nunique'],\n",
    "        'card_id':         ['size','count'],\n",
    "        'purchase_date':   ['max', 'min'],\n",
    "        ###\n",
    "        'price':             ['mean','max','min','std'],\n",
    "        'duration':          ['mean','min','max','std','skew'],\n",
    "        'amount_month_ratio':['mean','min','max','std','skew'],\n",
    "        } \n",
    "    \n",
    "    for col in ['category_2','category_3']:\n",
    "        df[col+'_mean'] = df.groupby([col])['purchase_amount'].transform('mean')\n",
    "        agg_func[col+'_mean'] = ['mean']\n",
    "    \n",
    "    agg_df = df.groupby(['card_id']).agg(agg_func)\n",
    "    agg_df.columns = [prefix + '_'.join(col).strip() for col in agg_df.columns.values]\n",
    "    agg_df.reset_index(drop=False, inplace=True)\n",
    "  \n",
    "    return agg_df\n",
    "print('generate statistics features...')\n",
    "auth_base_stat = aggregate_transactions(historical_transactions[historical_transactions['authorized_flag']==1], prefix='auth_')\n",
    "print('generate statistics features...')\n",
    "hist_base_stat = aggregate_transactions(historical_transactions[historical_transactions['authorized_flag']==0], prefix='hist_')\n",
    "print('generate statistics features...')\n",
    "new_base_stat  = aggregate_transactions(new_transactions, prefix='new_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auth...\n",
      "****************************** Part1, whole data ******************************\n",
      "****************************** Traditional Features ******************************\n",
      "reference_day 不存在！！！\n",
      "first_day 不存在！！！\n",
      "last_day 不存在！！！\n",
      "activation_day 不存在！！！\n",
      "card id(city_id,installments,merchant_category_id,.......):nunique, cnt/nunique\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09609478e29a4ad18ab322e1e330be61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "card_id(purchase_amount & degrade version ):mean,sum,std,median,quantile(10,25,75,90)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2286a4b97f9a46659becd3f6e0585c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Pivot Features ******************************\n",
      "Count  Pivot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a317fec695442ca1f773f6c1bf317a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 112.08 Mb (73.3% reduction)\n",
      "hist...\n",
      "****************************** Part1, whole data ******************************\n",
      "****************************** Traditional Features ******************************\n",
      "card_id(month_lag, min to reference day):min\n",
      "last_day 不存在！！！\n",
      "card id(city_id,installments,merchant_category_id,.......):nunique, cnt/nunique\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c9fd4b90a44f8a8292106659042e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "card_id(purchase_amount & degrade version ):mean,sum,std,median,quantile(10,25,75,90)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ae2fbeeeff4499810255bf482b64d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Pivot Features ******************************\n",
      "Count  Pivot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "172ee70bce614bfa98c17583b5a6333b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 121.08 Mb (72.7% reduction)\n",
      "new...\n",
      "****************************** Part1, whole data ******************************\n",
      "****************************** Traditional Features ******************************\n",
      " Eight time features, \n",
      "card_id(month_lag, min to reference day):min\n",
      "card id(city_id,installments,merchant_category_id,.......):nunique, cnt/nunique\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e946aefa76a84b22bffb1946a7c1e3c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "card_id(purchase_amount & degrade version ):mean,sum,std,median,quantile(10,25,75,90)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697665f5827144e1bbc20cf02ead9dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Pivot Features ******************************\n",
      "Count  Pivot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a73339fffa44beac7cd848a009146e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Part2， data with time less than activation day ******************************\n",
      "card_id(purchase_amount): sum\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08fdf4b633df49a287d96cac111de1ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 92.65 Mb (73.3% reduction)\n",
      "Wall time: 9min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_quantile(x, percentiles = [0.1, 0.25, 0.75, 0.9]):\n",
    "    x_len = len(x)\n",
    "    x = np.sort(x)\n",
    "    sts_feas = []  \n",
    "    for per_ in percentiles:\n",
    "        if per_ == 1:\n",
    "            sts_feas.append(x[x_len - 1]) \n",
    "        else:\n",
    "            sts_feas.append(x[int(x_len * per_)]) \n",
    "    return sts_feas \n",
    "\n",
    "def get_cardf_tran(df_, month = 3, prefix = '_'):\n",
    "    \n",
    "    df = df_.copy() \n",
    "    if prefix == 'hist_cardf_':\n",
    "        df['month_to_now']  =  (datetime.date(2018, month, 1) - df['purchase_date_floorday'].dt.date).dt.days\n",
    "    \n",
    "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] = df['month_diff'].astype(int)\n",
    "    df['month_diff'] += df['month_lag']\n",
    "    \n",
    "    print('*'*30,'Part1, whole data','*'*30)\n",
    "    cardid_features = pd.DataFrame()\n",
    "    cardid_features['card_id'] = df['card_id'].unique()   \n",
    "    print( '*' * 30, 'Traditional Features', '*' * 30)\n",
    "    ht_card_id_gp = df.groupby('card_id') \n",
    "    cardid_features['card_id_cnt'] = ht_card_id_gp['authorized_flag'].count().values\n",
    "    \n",
    "    if  prefix == 'hist_cardf_':\n",
    "        cardid_features['card_id_isau_mean'] = ht_card_id_gp['authorized_flag'].mean().values\n",
    "        cardid_features['card_id_isau_sum'] = ht_card_id_gp['authorized_flag'].sum().values \n",
    "    \n",
    "    cardid_features['month_diff_mean']   = ht_card_id_gp['month_diff'].mean().values\n",
    "    cardid_features['month_diff_median'] = ht_card_id_gp['month_diff'].median().values\n",
    "    \n",
    "    if prefix == 'hist_cardf_':\n",
    "        cardid_features['reference_day']           =  ht_card_id_gp['purchase_date_floorday'].max().values\n",
    "        cardid_features['first_day']               =  ht_card_id_gp['purchase_date_floorday'].min().values \n",
    "        cardid_features['activation_day']          =  ht_card_id_gp['first_active_month'].max().values\n",
    "       \n",
    "        # first to activation day\n",
    "        cardid_features['first_to_activation_day']  =  (cardid_features['first_day'] - cardid_features['activation_day']).dt.days\n",
    "        # activation to reference day \n",
    "        cardid_features['activation_to_reference_day']  =  (cardid_features['reference_day'] - cardid_features['activation_day']).dt.days\n",
    "        # first to last day \n",
    "        cardid_features['first_to_reference_day']  =  (cardid_features['reference_day'] - cardid_features['first_day']).dt.days\n",
    "        # reference day to now  \n",
    "        cardid_features['reference_day_to_now']  =  (datetime.date(2018, month, 1) - cardid_features['reference_day'].dt.date).dt.days \n",
    "        # first day to now\n",
    "        cardid_features['first_day_to_now']  =  (datetime.date(2018, month, 1) - cardid_features['first_day'].dt.date).dt.days \n",
    "        \n",
    "        print('card_id(month_lag, min to reference day):min')\n",
    "        cardid_features['card_id_month_lag_min'] = ht_card_id_gp['month_lag'].agg('min').values   \n",
    "        # is_purchase_before_activation,first_to_reference_day_divide_activation_to_reference_day\n",
    "        cardid_features['is_purchase_before_activation'] = cardid_features['first_to_activation_day'] < 0 \n",
    "        cardid_features['is_purchase_before_activation'] = cardid_features['is_purchase_before_activation'].astype(int)\n",
    "        cardid_features['first_to_reference_day_divide_activation_to_reference_day'] = cardid_features['first_to_reference_day']  / (cardid_features['activation_to_reference_day']  + 0.01)\n",
    "        cardid_features['days_per_count'] = cardid_features['first_to_reference_day'].values / cardid_features['card_id_cnt'].values\n",
    "   \n",
    "    if prefix == 'new_cardf_':\n",
    "        print(' Eight time features, ') \n",
    "        cardid_features['reference_day']           =  ht_card_id_gp['reference_day'].last().values\n",
    "        cardid_features['first_day']               =  ht_card_id_gp['purchase_date_floorday'].min().values \n",
    "        cardid_features['last_day']                =  ht_card_id_gp['purchase_date_floorday'].max().values\n",
    "        cardid_features['activation_day']          =  ht_card_id_gp['first_active_month'].max().values\n",
    "        # reference to first day\n",
    "        cardid_features['reference_day_to_first_day']  =  (cardid_features['first_day'] - cardid_features['reference_day']).dt.days\n",
    "        # reference to last day\n",
    "        cardid_features['reference_day_to_last_day']  =  (cardid_features['last_day'] - cardid_features['reference_day']).dt.days  \n",
    "        # first to last day \n",
    "        cardid_features['first_to_last_day']  =  (cardid_features['last_day'] - cardid_features['first_day']).dt.days\n",
    "        # activation to first day \n",
    "        cardid_features['activation_to_first_day']  =  (cardid_features['first_day'] - cardid_features['activation_day']).dt.days\n",
    "        # activation to first day \n",
    "        cardid_features['activation_to_last_day']  =  (cardid_features['last_day'] - cardid_features['activation_day']).dt.days\n",
    "        # last day to now  \n",
    "        cardid_features['reference_day_to_now']  =  (datetime.date(2018, month, 1) - cardid_features['reference_day'].dt.date).dt.days \n",
    "        # first day to now\n",
    "        cardid_features['first_day_to_now']  =  (datetime.date(2018, month, 1) - cardid_features['first_day'].dt.date).dt.days \n",
    "        \n",
    "        print('card_id(month_lag, min to reference day):min')\n",
    "        cardid_features['card_id_month_lag_max'] = ht_card_id_gp['month_lag'].agg('max').values  \n",
    "        cardid_features['first_to_last_day_divide_reference_to_last_day'] = cardid_features['first_to_last_day']  / (cardid_features['reference_day_to_last_day']  + 0.01)\n",
    "        cardid_features['days_per_count'] = cardid_features['first_to_last_day'].values / cardid_features['card_id_cnt'].values\n",
    "    \n",
    "    for f in ['reference_day', 'first_day', 'last_day', 'activation_day']:\n",
    "        try:\n",
    "            del cardid_features[f]\n",
    "        except:\n",
    "            print(f, '不存在！！！')\n",
    "\n",
    "    print('card id(city_id,installments,merchant_category_id,.......):nunique, cnt/nunique') \n",
    "    for col in tqdm_notebook(['category_1','category_2','category_3','state_id','city_id','installments','merchant_id', 'merchant_category_id','subsector_id','month_lag','purchase_date_floorday']):\n",
    "        cardid_features['card_id_%s_nunique'%col]            =  ht_card_id_gp[col].nunique().values\n",
    "        cardid_features['card_id_cnt_divide_%s_nunique'%col] =  cardid_features['card_id_cnt'].values / cardid_features['card_id_%s_nunique'%col].values\n",
    "         \n",
    "    print('card_id(purchase_amount & degrade version ):mean,sum,std,median,quantile(10,25,75,90)') \n",
    "    for col in tqdm_notebook(['installments','purchase_amount','purchase_amount_ddgd_98','purchase_amount_ddgd_99','purchase_amount_wdgd_96','purchase_amount_wdgd_97','purchase_amount_mdgd_90','purchase_amount_mdgd_80']):\n",
    "        if col =='purchase_amount':\n",
    "            for opt in ['sum','mean','std','median','max','min']:\n",
    "                cardid_features['card_id_' +col+ '_' + opt] = ht_card_id_gp[col].agg(opt).values\n",
    "            \n",
    "            cardid_features['card_id_' +col+ '_range'] =  cardid_features['card_id_' +col+ '_max'].values - cardid_features['card_id_' +col+ '_min'].values\n",
    "            percentiles = ht_card_id_gp[col].apply(lambda x:get_quantile(x,percentiles = [0.025, 0.25, 0.75, 0.975])) \n",
    "\n",
    "            cardid_features[col + '_2.5_quantile']  = percentiles.map(lambda x:x[0]).values\n",
    "            cardid_features[col + '_25_quantile'] = percentiles.map(lambda x:x[1]).values\n",
    "            cardid_features[col + '_75_quantile'] = percentiles.map(lambda x:x[2]).values\n",
    "            cardid_features[col + '_97.5_quantile'] = percentiles.map(lambda x:x[3]).values\n",
    "            cardid_features['card_id_' +col+ '_range2'] =  cardid_features[col+ '_97.5_quantile'].values - cardid_features[col+ '_2.5_quantile'].values\n",
    "            del cardid_features[col + '_2.5_quantile'],cardid_features[col + '_97.5_quantile']\n",
    "            gc.collect()\n",
    "        else:\n",
    "            for opt in ['sum']:\n",
    "                cardid_features['card_id_' +col+ '_' + opt] = ht_card_id_gp[col].agg(opt).values          \n",
    "    \n",
    "    print( '*' * 30, 'Pivot Features', '*' * 30)\n",
    "    print('Count  Pivot') #purchase_month_since_reference_day(可能和month_lag重复),百分比降分,暂时忽略 (dayofweek,merchant_cate,state_id)作用不大installments\n",
    "    for pivot_col in tqdm_notebook(['category_1','category_2','category_3','month_lag','subsector_id','weekend']): #'city_id',,\n",
    "    \n",
    "        tmp     = df.groupby(['card_id',pivot_col])['merchant_id'].count().to_frame(pivot_col + '_count')\n",
    "        tmp.reset_index(inplace =True)  \n",
    "         \n",
    "        tmp_pivot = pd.pivot_table(data=tmp,index = 'card_id',columns=pivot_col,values=pivot_col + '_count',fill_value=0)\n",
    "        tmp_pivot.columns = [tmp_pivot.columns.names[0] + '_cnt_pivot_'+ str(col) for col in tmp_pivot.columns]\n",
    "        tmp_pivot.reset_index(inplace = True)\n",
    "        cardid_features = cardid_features.merge(tmp_pivot, on = 'card_id', how='left')\n",
    "      \n",
    "        if  pivot_col!='weekend' and  pivot_col!='installments':\n",
    "            tmp            = df.groupby(['card_id',pivot_col])['purchase_date_floorday'].nunique().to_frame(pivot_col + '_purchase_date_floorday_nunique') \n",
    "            tmp1           = df.groupby(['card_id'])['purchase_date_floorday'].nunique().to_frame('purchase_date_floorday_nunique') \n",
    "            tmp.reset_index(inplace =True)  \n",
    "            tmp1.reset_index(inplace =True)   \n",
    "            tmp  = tmp.merge(tmp1, on ='card_id', how='left')\n",
    "            tmp[pivot_col + '_day_nunique_pct'] = tmp[pivot_col + '_purchase_date_floorday_nunique'].values / tmp['purchase_date_floorday_nunique'].values\n",
    "         \n",
    "            tmp_pivot = pd.pivot_table(data=tmp,index = 'card_id',columns=pivot_col,values=pivot_col + '_day_nunique_pct',fill_value=0)\n",
    "            tmp_pivot.columns = [tmp_pivot.columns.names[0] + '_day_nunique_pct_'+ str(col) for col in tmp_pivot.columns]\n",
    "            tmp_pivot.reset_index(inplace = True)\n",
    "            cardid_features = cardid_features.merge(tmp_pivot, on = 'card_id', how='left')\n",
    "    \n",
    "    if prefix == 'new_cardf_':\n",
    "    ######## 在卡未激活之前就有过消费的记录  ##############   \n",
    "        print('*'*30,'Part2， data with time less than activation day','*'*30)\n",
    "        df_part = df.loc[df.purchase_date < df.first_active_month]\n",
    "\n",
    "        cardid_features_part = pd.DataFrame()\n",
    "        cardid_features_part['card_id'] = df_part['card_id'].unique()   \n",
    "        ht_card_id_part_gp = df_part.groupby('card_id')\n",
    "        cardid_features_part['card_id_part_cnt'] = ht_card_id_part_gp['authorized_flag'].count().values\n",
    "\n",
    "        print('card_id(purchase_amount): sum') \n",
    "        for col in tqdm_notebook(['purchase_amount']): \n",
    "            for opt in ['sum','mean']:\n",
    "                cardid_features_part['card_id_part_' +col+ '_' + opt] = ht_card_id_part_gp[col].agg(opt).values\n",
    "\n",
    "        cardid_features = cardid_features.merge(cardid_features_part, on ='card_id', how='left')\n",
    "        cardid_features['card_id_part_purchase_amount_sum_percent'] = cardid_features['card_id_part_purchase_amount_sum'] / (cardid_features['card_id_purchase_amount_sum'] + 0.01)\n",
    "\n",
    "    cardid_features = reduce_mem_usage(cardid_features)\n",
    "    \n",
    "    new_col_names = []\n",
    "    for col in cardid_features.columns:\n",
    "        if col == 'card_id':\n",
    "            new_col_names.append(col)\n",
    "        else:\n",
    "            new_col_names.append(prefix + col)\n",
    "    cardid_features.columns = new_col_names\n",
    "    \n",
    "    return cardid_features\n",
    "print('auth...')\n",
    "authorized_transactions = historical_transactions.loc[historical_transactions['authorized_flag'] == 1]\n",
    "auth_cardf_tran = get_cardf_tran(authorized_transactions, 3, prefix='auth_cardf_')\n",
    "print('hist...')\n",
    "hist_cardf_tran = get_cardf_tran(historical_transactions, 3, prefix='hist_cardf_')\n",
    "print('new...')\n",
    "reference_days = historical_transactions.groupby('card_id')['purchase_date'].last().to_frame('reference_day')\n",
    "reference_days.reset_index(inplace = True)\n",
    "new_transactions = new_transactions.merge(reference_days, on ='card_id', how='left')\n",
    "new_cardf_tran  = get_cardf_tran(new_transactions, 5, prefix='new_cardf_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，需要进一步考虑最近两个月的用户行为特征："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Part1, whole data ******************************\n",
      "****************************** Traditional Features ******************************\n",
      " card id : count\n",
      "card id(city_id,installments,merchant_category_id,.......):nunique, cnt/nunique\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf22c32eee824c4da92f9a17fde49032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6203afafca04c228e1cab6192267d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Pivot Features ******************************\n",
      "Count  Pivot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c145962f344e519045e84ef3dd4c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 84.45 Mb (74.0% reduction)\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_cardf_tran_last2(df_, month = 3, prefix = 'last2_'): \n",
    "    \n",
    "    df = df_.loc[df_.month_lag >= -2].copy()\n",
    "    print('*'*30,'Part1, whole data','*'*30)\n",
    "    cardid_features = pd.DataFrame()\n",
    "    cardid_features['card_id'] = df['card_id'].unique()   \n",
    "    \n",
    "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] = df['month_diff'].astype(int)\n",
    "    df['month_diff'] += df['month_lag']\n",
    "    \n",
    "    print( '*' * 30, 'Traditional Features', '*' * 30)\n",
    "    ht_card_id_gp = df.groupby('card_id')\n",
    "    print(' card id : count')\n",
    "    cardid_features['card_id_cnt'] = ht_card_id_gp['authorized_flag'].count().values\n",
    "    \n",
    "    cardid_features['card_id_isau_mean'] = ht_card_id_gp['authorized_flag'].mean().values \n",
    "    cardid_features['card_id_isau_sum']  = ht_card_id_gp['authorized_flag'].sum().values\n",
    "    \n",
    "    cardid_features['month_diff_mean']   = ht_card_id_gp['month_diff'].mean().values\n",
    "\n",
    "    print('card id(city_id,installments,merchant_category_id,.......):nunique, cnt/nunique') \n",
    "    for col in tqdm_notebook(['state_id','city_id','installments','merchant_id', 'merchant_category_id','purchase_date_floorday']):\n",
    "        cardid_features['card_id_%s_nunique'%col] = ht_card_id_gp[col].nunique().values\n",
    "        cardid_features['card_id_cnt_divide_%s_nunique'%col] = cardid_features['card_id_cnt'].values / cardid_features['card_id_%s_nunique'%col].values\n",
    "         \n",
    "    for col in tqdm_notebook(['purchase_amount','purchase_amount_ddgd_98','purchase_amount_wdgd_96','purchase_amount_mdgd_90','purchase_amount_mdgd_80']): #,'purchase_amount_ddgd_98','purchase_amount_ddgd_99','purchase_amount_wdgd_96','purchase_amount_wdgd_97','purchase_amount_mdgd_90','purchase_amount_mdgd_80']):\n",
    "        if col =='purchase_amount':\n",
    "            for opt in ['sum','mean','std','median']:\n",
    "                cardid_features['card_id_' +col+ '_' + opt] = ht_card_id_gp[col].agg(opt).values  \n",
    "        else:\n",
    "            for opt in ['sum']:\n",
    "                cardid_features['card_id_' +col+ '_' + opt] = ht_card_id_gp[col].agg(opt).values \n",
    "    \n",
    "    print( '*' * 30, 'Pivot Features', '*' * 30)\n",
    "    print('Count  Pivot') #purchase_month_since_reference_day(可能和month_lag重复),百分比降分,暂时忽略 (dayofweek,merchant_cate,state_id)作用不大\n",
    "    \n",
    "    for pivot_col in tqdm_notebook(['category_1','category_2','category_3','month_lag','subsector_id','weekend']): #'city_id', \n",
    "    \n",
    "        tmp     = df.groupby(['card_id',pivot_col])['merchant_id'].count().to_frame(pivot_col + '_count')\n",
    "        tmp.reset_index(inplace =True)  \n",
    "         \n",
    "        tmp_pivot = pd.pivot_table(data=tmp,index = 'card_id',columns=pivot_col,values=pivot_col + '_count',fill_value=0)\n",
    "        tmp_pivot.columns = [tmp_pivot.columns.names[0] + '_cnt_pivot_'+ str(col) for col in tmp_pivot.columns]\n",
    "        tmp_pivot.reset_index(inplace = True)\n",
    "        cardid_features = cardid_features.merge(tmp_pivot, on = 'card_id', how='left')\n",
    "      \n",
    "        if  pivot_col!='weekend' and  pivot_col!='installments':\n",
    "            tmp            = df.groupby(['card_id',pivot_col])['purchase_date_floorday'].nunique().to_frame(pivot_col + '_purchase_date_floorday_nunique') \n",
    "            tmp1           = df.groupby(['card_id'])['purchase_date_floorday'].nunique().to_frame('purchase_date_floorday_nunique') \n",
    "            tmp.reset_index(inplace =True)  \n",
    "            tmp1.reset_index(inplace =True)   \n",
    "            tmp  = tmp.merge(tmp1, on ='card_id', how='left')\n",
    "            tmp[pivot_col + '_day_nunique_pct'] = tmp[pivot_col + '_purchase_date_floorday_nunique'].values / tmp['purchase_date_floorday_nunique'].values\n",
    "         \n",
    "            tmp_pivot = pd.pivot_table(data=tmp,index = 'card_id',columns=pivot_col,values=pivot_col + '_day_nunique_pct',fill_value=0)\n",
    "            tmp_pivot.columns = [tmp_pivot.columns.names[0] + '_day_nunique_pct_'+ str(col) for col in tmp_pivot.columns]\n",
    "            tmp_pivot.reset_index(inplace = True)\n",
    "            cardid_features = cardid_features.merge(tmp_pivot, on = 'card_id', how='left')\n",
    "     \n",
    "    cardid_features = reduce_mem_usage(cardid_features)\n",
    "    \n",
    "    new_col_names = []\n",
    "    for col in cardid_features.columns:\n",
    "        if col == 'card_id':\n",
    "            new_col_names.append(col)\n",
    "        else:\n",
    "            new_col_names.append(prefix + col)\n",
    "    cardid_features.columns = new_col_names\n",
    "    \n",
    "    return cardid_features  \n",
    "\n",
    "hist_cardf_tran_last2 = get_cardf_tran_last2(historical_transactions, month = 3, prefix = 'hist_last2_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以及进一步进行二阶交叉特征衍生："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hist...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc26f25c2c85434f812d486b78ca5a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cdf536b26a44e288cf9a823d5750274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab95adcdc1f44136a14c9fca2f57e04e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 70.16 Mb (66.8% reduction)\n",
      "Wall time: 5min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def successive_aggregates(df_, prefix = 'levelAB_'):\n",
    "    df = df_.copy()\n",
    "    cardid_features = pd.DataFrame()\n",
    "    cardid_features['card_id'] = df['card_id'].unique()    \n",
    "     \n",
    "    level12_nunique = [('month_lag','state_id'),('month_lag','city_id'),('month_lag','subsector_id'),('month_lag','merchant_category_id'),('month_lag','merchant_id'),('month_lag','purchase_date_floorday'),\\\n",
    "                       ('subsector_id','merchant_category_id'),('subsector_id','merchant_id'),('subsector_id','purchase_date_floorday'),('subsector_id','month_lag'),\\\n",
    "                       ('merchant_category_id', 'merchant_id'),('merchant_category_id','purchase_date_floorday'),('merchant_category_id','month_lag'),\\\n",
    "                       ('purchase_date_floorday', 'merchant_id'),('purchase_date_floorday','merchant_category_id'),('purchase_date_floorday','subsector_id')]    \n",
    "    for col_level1,col_level2 in tqdm_notebook(level12_nunique):  \n",
    "        \n",
    "        level1  = df.groupby(['card_id',col_level1])[col_level2].nunique().to_frame(col_level2 + '_nunique')\n",
    "        level1.reset_index(inplace =True)  \n",
    "         \n",
    "        level2 = level1.groupby('card_id')[col_level2 + '_nunique'].agg(['mean', 'max', 'std'])\n",
    "        level2 = pd.DataFrame(level2)\n",
    "        level2.columns = [col_level1 + '_' + col_level2 + '_nunique_' + col for col in level2.columns.values]\n",
    "        level2.reset_index(inplace = True)\n",
    "        \n",
    "        cardid_features = cardid_features.merge(level2, on='card_id', how='left') \n",
    "    \n",
    "    level12_count = ['month_lag','state_id','city_id','subsector_id','merchant_category_id','merchant_id','purchase_date_floorday']\n",
    "    for col_level in tqdm_notebook(level12_count): \n",
    "    \n",
    "        level1  = df.groupby(['card_id',col_level])['merchant_id'].count().to_frame(col_level + '_count')\n",
    "        level1.reset_index(inplace =True)  \n",
    "         \n",
    "        level2 = level1.groupby('card_id')[col_level + '_count'].agg(['mean', 'max', 'std'])\n",
    "        level2 = pd.DataFrame(level2)\n",
    "        level2.columns = [col_level + '_count_' + col for col in level2.columns.values]\n",
    "        level2.reset_index(inplace = True)\n",
    "        \n",
    "        cardid_features = cardid_features.merge(level2, on='card_id', how='left') \n",
    "    \n",
    "    level12_meansum = [('month_lag','purchase_amount'),('state_id','purchase_amount'),('city_id','purchase_amount'),('subsector_id','purchase_amount'),\\\n",
    "                       ('merchant_category_id','purchase_amount'),('merchant_id','purchase_amount'),('purchase_date_floorday','purchase_amount')]\n",
    "    for col_level1,col_level2 in tqdm_notebook(level12_meansum): \n",
    "    \n",
    "        level1  = df.groupby(['card_id',col_level1])[col_level2].sum().to_frame(col_level2 + '_sum')\n",
    "        level1.reset_index(inplace =True)  \n",
    "         \n",
    "        level2 = level1.groupby('card_id')[col_level2 + '_sum'].agg(['mean', 'max', 'std'])\n",
    "        level2 = pd.DataFrame(level2)\n",
    "        level2.columns = [col_level1 + '_' + col_level2 + '_sum_' + col for col in level2.columns.values]\n",
    "        level2.reset_index(inplace = True)\n",
    "\n",
    "        cardid_features = cardid_features.merge(level2, on='card_id', how='left')           \n",
    "    \n",
    "    cardid_features = reduce_mem_usage(cardid_features)\n",
    "    \n",
    "    new_col_names = []\n",
    "    for col in cardid_features.columns:\n",
    "        if col == 'card_id':\n",
    "            new_col_names.append(col)\n",
    "        else:\n",
    "            new_col_names.append(prefix + col)\n",
    "    cardid_features.columns = new_col_names\n",
    "    \n",
    "    return cardid_features  \n",
    "\n",
    "print('hist...')\n",
    "hist_levelAB = successive_aggregates(historical_transactions, prefix = 'hist_levelAB_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，将上述衍生特征进行合并："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201917, 11)\n",
      "(123623, 10)\n",
      "#_____基础统计特征\n",
      "(201917, 164)\n",
      "(123623, 163)\n",
      "#_____全局cardid特征\n",
      "(201917, 687)\n",
      "(123623, 686)\n",
      "#_____最近两月cardid特征\n",
      "(201917, 821)\n",
      "(123623, 820)\n",
      "#_____补充二阶特征\n",
      "(201917, 911)\n",
      "(123623, 910)\n",
      "Wall time: 32.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "## 合并到训练集和测试集\n",
    "print('#_____基础统计特征')\n",
    "train = pd.merge(train, auth_base_stat, on='card_id', how='left')\n",
    "test  = pd.merge(test,  auth_base_stat, on='card_id', how='left')\n",
    "train = pd.merge(train, hist_base_stat, on='card_id', how='left')\n",
    "test  = pd.merge(test,  hist_base_stat, on='card_id', how='left')\n",
    "train = pd.merge(train, new_base_stat , on='card_id', how='left')\n",
    "test  = pd.merge(test,  new_base_stat , on='card_id', how='left')\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print('#_____全局cardid特征')\n",
    "train = pd.merge(train, auth_cardf_tran, on='card_id', how='left')\n",
    "test  = pd.merge(test,  auth_cardf_tran, on='card_id', how='left')\n",
    "train = pd.merge(train, hist_cardf_tran, on='card_id', how='left')\n",
    "test  = pd.merge(test,  hist_cardf_tran, on='card_id', how='left')\n",
    "train = pd.merge(train, new_cardf_tran , on='card_id', how='left')\n",
    "test  = pd.merge(test,  new_cardf_tran , on='card_id', how='left')\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print('#_____最近两月cardid特征')\n",
    "train = pd.merge(train, hist_cardf_tran_last2, on='card_id', how='left')\n",
    "test  = pd.merge(test,  hist_cardf_tran_last2, on='card_id', how='left')\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print('#_____补充二阶特征')\n",
    "train = pd.merge(train, hist_levelAB, on='card_id', how='left')\n",
    "test  = pd.merge(test,  hist_levelAB, on='card_id', how='left')\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "并在此基础上补充部分简单四折运算后的衍生特征："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train['outliers'] = 0\n",
    "train.loc[train['target'] < -30, 'outliers'] = 1\n",
    "train['outliers'].value_counts()\n",
    "for f in ['feature_1','feature_2','feature_3']:\n",
    "    colname = f+'_outliers_mean'\n",
    "    order_label = train.groupby([f])['outliers'].mean()\n",
    "    for df in [train, test]:\n",
    "        df[colname] = df[f].map(order_label)\n",
    "\n",
    "for df in [train, test]:\n",
    "    \n",
    "    df['days_feature1'] = df['elapsed_time'] * df['feature_1']\n",
    "    df['days_feature2'] = df['elapsed_time'] * df['feature_2']\n",
    "    df['days_feature3'] = df['elapsed_time'] * df['feature_3']\n",
    "\n",
    "    df['days_feature1_ratio'] = df['feature_1'] / df['elapsed_time']\n",
    "    df['days_feature2_ratio'] = df['feature_2'] / df['elapsed_time']\n",
    "    df['days_feature3_ratio'] = df['feature_3'] / df['elapsed_time']\n",
    "\n",
    "    df['feature_sum'] = df['feature_1'] + df['feature_2'] + df['feature_3']\n",
    "    df['feature_mean'] = df['feature_sum']/3\n",
    "    df['feature_max'] = df[['feature_1', 'feature_2', 'feature_3']].max(axis=1)\n",
    "    df['feature_min'] = df[['feature_1', 'feature_2', 'feature_3']].min(axis=1)\n",
    "    df['feature_var'] = df[['feature_1', 'feature_2', 'feature_3']].std(axis=1)\n",
    "    \n",
    "    df['card_id_total'] = df['hist_card_id_size']+df['new_card_id_size']\n",
    "    df['card_id_cnt_total'] = df['hist_card_id_count']+df['new_card_id_count']\n",
    "    df['card_id_cnt_ratio'] = df['new_card_id_count']/df['hist_card_id_count']\n",
    "    df['purchase_amount_total'] = df['hist_cardf_card_id_purchase_amount_sum']+df['new_cardf_card_id_purchase_amount_sum']\n",
    "    df['purchase_amount_ratio'] = df['new_cardf_card_id_purchase_amount_sum']/df['hist_cardf_card_id_purchase_amount_sum']\n",
    "    df['month_diff_ratio'] = df['new_cardf_month_diff_mean']/df['hist_cardf_month_diff_mean']\n",
    "    df['installments_total'] = df['new_cardf_card_id_installments_sum']+df['auth_cardf_card_id_installments_sum']\n",
    "    df['installments_ratio'] = df['new_cardf_card_id_installments_sum']/df['auth_cardf_card_id_installments_sum']\n",
    "    df['price_total'] = df['purchase_amount_total']/df['installments_total']\n",
    "    df['new_CLV'] = df['new_card_id_count'] * df['new_cardf_card_id_purchase_amount_sum'] / df['new_cardf_month_diff_mean']\n",
    "    df['hist_CLV'] = df['hist_card_id_count'] * df['hist_cardf_card_id_purchase_amount_sum'] / df['hist_cardf_month_diff_mean']\n",
    "    df['CLV_ratio'] = df['new_CLV'] / df['hist_CLV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.特征筛选"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在创建完全部特征后即可进行特征筛选了。此处我们考虑手动进行特征筛选，排除部分过于稀疏的特征后即可将数据保存在本地："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "运用杰哥方法...\n",
      "删除前: 938\n",
      "删除后: 770\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "del_cols = []\n",
    "for col in train.columns:\n",
    "    if 'subsector_id_cnt_' in col and 'new_cardf': \n",
    "        del_cols.append(col)\n",
    "del_cols1 = []\n",
    "for col in train.columns:\n",
    "    if 'subsector_id_cnt_' in col and 'hist_last2_' in col:\n",
    "        del_cols1.append(col)\n",
    "del_cols2 = []\n",
    "for col in train.columns:\n",
    "    if 'subsector_id_cnt_' in col and 'auth_cardf' in col:\n",
    "        del_cols2.append(col)\n",
    "del_cols3 = []\n",
    "for col in train.columns:\n",
    "    if 'merchant_category_id_month_lag_nunique_' in col and '_pivot_supp' in col:\n",
    "        del_cols3.append(col)\n",
    "    if 'city_id' in col and '_pivot_supp' in col:\n",
    "        del_cols3.append(col)\n",
    "    if 'month_diff' in col and 'hist_last2_' in col:\n",
    "        del_cols3.append(col)\n",
    "    if 'month_diff_std' in col or 'month_diff_gap' in col:\n",
    "        del_cols3.append(col) \n",
    "fea_cols = [col for col in train.columns if train[col].dtypes!='object' and train[col].dtypes != '<M8[ns]' and col!='target' not in col and col!='min_num'\\\n",
    "            and col not in del_cols and col not in del_cols1 and col not in del_cols2 and col!='target1' and col!='card_id_cnt_ht_pivot_supp'  and col not in del_cols3]   \n",
    "print('删除前:',train.shape[1])\n",
    "print('删除后:',len(fea_cols))\n",
    "\n",
    "train = train[fea_cols+['target']]\n",
    "fea_cols.remove('outliers')\n",
    "test = test[fea_cols]\n",
    "\n",
    "train.to_csv('./data/all_train_features.csv',index=False)\n",
    "test.to_csv('./data/all_test_features.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际执行过程中，可以按照如下方式进行读取："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (201917, 771)\n",
      "ntrain: (199710, 771)\n",
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## load all features\n",
    "train = pd.read_csv('./data/all_train_features.csv')\n",
    "test  = pd.read_csv('./data/all_test_features.csv')\n",
    "\n",
    "inf_cols = ['new_cardf_card_id_cnt_divide_installments_nunique', 'hist_last2_card_id_cnt_divide_installments_nunique']\n",
    "train[inf_cols] = train[inf_cols].replace(np.inf, train[inf_cols].replace(np.inf, -99).max().max())\n",
    "ntrain[inf_cols] = ntrain[inf_cols].replace(np.inf, ntrain[inf_cols].replace(np.inf, -99).max().max())\n",
    "test[inf_cols] = test[inf_cols].replace(np.inf, test[inf_cols].replace(np.inf, -99).max().max())\n",
    "\n",
    "# ## load sparse\n",
    "# train_tags = sparse.load_npz('train_tags.npz')\n",
    "# test_tags  = sparse.load_npz('test_tags.npz')\n",
    "\n",
    "## 获取非异常值的index\n",
    "normal_index = train[train['outliers']==0].index.tolist()\n",
    "## without outliers\n",
    "ntrain = train[train['outliers'] == 0]\n",
    "\n",
    "target        = train['target'].values\n",
    "ntarget       = ntrain['target'].values\n",
    "target_binary = train['outliers'].values\n",
    "###\n",
    "y_train        = target\n",
    "y_ntrain       = ntarget\n",
    "y_train_binary = target_binary\n",
    "\n",
    "print('train:',train.shape)\n",
    "print('ntrain:',ntrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>二、两阶段建模优化\n",
    "## <center> LightGBM+XGBoost+CatBoost两阶段建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/10/a19mJpQOkMunDbf.png\" alt=\"image-20211210165529875\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;为了方便更快速的调用三种不同的模型，并且同时要求能够完成分类和回归预测，此处通过定义一个函数来完成所有模型的训练过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, X_test, y, params, folds, model_type='lgb', eval_type='regression'):\n",
    "    oof = np.zeros(X.shape[0])\n",
    "    predictions = np.zeros(X_test.shape[0])\n",
    "    scores = []\n",
    "    for fold_n, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            trn_data = lgb.Dataset(X[trn_idx], y[trn_idx])\n",
    "            val_data = lgb.Dataset(X[val_idx], y[val_idx])\n",
    "            clf = lgb.train(params, trn_data, num_boost_round=20000, \n",
    "                            valid_sets=[trn_data, val_data], \n",
    "                            verbose_eval=100, early_stopping_rounds=300)\n",
    "            oof[val_idx] = clf.predict(X[val_idx], num_iteration=clf.best_iteration)\n",
    "            predictions += clf.predict(X_test, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "        \n",
    "        if model_type == 'xgb':\n",
    "            trn_data = xgb.DMatrix(X[trn_idx], y[trn_idx])\n",
    "            val_data = xgb.DMatrix(X[val_idx], y[val_idx])\n",
    "            watchlist = [(trn_data, 'train'), (val_data, 'valid_data')]\n",
    "            clf = xgb.train(dtrain=trn_data, num_boost_round=20000, \n",
    "                            evals=watchlist, early_stopping_rounds=200, \n",
    "                            verbose_eval=100, params=params)\n",
    "            oof[val_idx] = clf.predict(xgb.DMatrix(X[val_idx]), ntree_limit=clf.best_ntree_limit)\n",
    "            predictions += clf.predict(xgb.DMatrix(X_test), ntree_limit=clf.best_ntree_limit) / folds.n_splits\n",
    "        \n",
    "        if (model_type == 'cat') and (eval_type == 'regression'):\n",
    "            clf = CatBoostRegressor(iterations=20000, eval_metric='RMSE', **params)\n",
    "            clf.fit(X[trn_idx], y[trn_idx], \n",
    "                    eval_set=(X[val_idx], y[val_idx]),\n",
    "                    cat_features=[], use_best_model=True, verbose=100)\n",
    "            oof[val_idx] = clf.predict(X[val_idx])\n",
    "            predictions += clf.predict(X_test) / folds.n_splits\n",
    "            \n",
    "        if (model_type == 'cat') and (eval_type == 'binary'):\n",
    "            clf = CatBoostClassifier(iterations=20000, eval_metric='Logloss', **params)\n",
    "            clf.fit(X[trn_idx], y[trn_idx], \n",
    "                    eval_set=(X[val_idx], y[val_idx]),\n",
    "                    cat_features=[], use_best_model=True, verbose=100)\n",
    "            oof[val_idx] = clf.predict_proba(X[val_idx])[:,1]\n",
    "            predictions += clf.predict_proba(X_test)[:,1] / folds.n_splits\n",
    "        print(predictions)\n",
    "        if eval_type == 'regression':\n",
    "            scores.append(mean_squared_error(oof[val_idx], y[val_idx])**0.5)\n",
    "        if eval_type == 'binary':\n",
    "            scores.append(log_loss(y[val_idx], oof[val_idx]))\n",
    "        \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    return oof, predictions, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LightGBM模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来即可进行LGB模型训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 回归模型 ==========\n",
      "Fold 0 started at Fri Dec 10 17:25:17 2021\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 3.53938\tvalid_1's rmse: 3.7875\n",
      "[200]\ttraining's rmse: 3.3968\tvalid_1's rmse: 3.74815\n",
      "[300]\ttraining's rmse: 3.30586\tvalid_1's rmse: 3.73366\n",
      "[400]\ttraining's rmse: 3.23525\tvalid_1's rmse: 3.72721\n",
      "[500]\ttraining's rmse: 3.18027\tvalid_1's rmse: 3.7246\n",
      "[600]\ttraining's rmse: 3.13114\tvalid_1's rmse: 3.72374\n",
      "[700]\ttraining's rmse: 3.08883\tvalid_1's rmse: 3.72287\n",
      "[800]\ttraining's rmse: 3.05027\tvalid_1's rmse: 3.72258\n",
      "[900]\ttraining's rmse: 3.01313\tvalid_1's rmse: 3.7222\n",
      "[1000]\ttraining's rmse: 2.97868\tvalid_1's rmse: 3.72209\n",
      "[1100]\ttraining's rmse: 2.94588\tvalid_1's rmse: 3.72194\n",
      "[1200]\ttraining's rmse: 2.91346\tvalid_1's rmse: 3.72269\n",
      "[1300]\ttraining's rmse: 2.88382\tvalid_1's rmse: 3.72258\n",
      "Early stopping, best iteration is:\n",
      "[1076]\ttraining's rmse: 2.95376\tvalid_1's rmse: 3.72174\n",
      "[-0.27398405 -0.07034482 -0.16190932 ...  0.12370174 -0.62806241\n",
      "  0.01831661]\n",
      "Fold 1 started at Fri Dec 10 17:27:31 2021\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 3.5713\tvalid_1's rmse: 3.65646\n",
      "[200]\ttraining's rmse: 3.42522\tvalid_1's rmse: 3.61969\n",
      "[300]\ttraining's rmse: 3.33359\tvalid_1's rmse: 3.60873\n",
      "[400]\ttraining's rmse: 3.26677\tvalid_1's rmse: 3.6026\n",
      "[500]\ttraining's rmse: 3.2103\tvalid_1's rmse: 3.59843\n",
      "[600]\ttraining's rmse: 3.16146\tvalid_1's rmse: 3.59726\n",
      "[700]\ttraining's rmse: 3.11776\tvalid_1's rmse: 3.59642\n",
      "[800]\ttraining's rmse: 3.0776\tvalid_1's rmse: 3.59613\n",
      "[900]\ttraining's rmse: 3.04105\tvalid_1's rmse: 3.59617\n",
      "[1000]\ttraining's rmse: 3.00754\tvalid_1's rmse: 3.59619\n",
      "Early stopping, best iteration is:\n",
      "[788]\ttraining's rmse: 3.08177\tvalid_1's rmse: 3.59595\n",
      "[-0.72811523 -0.13348695 -0.29919495 ...  0.29648335 -1.301152\n",
      "  0.02036033]\n",
      "Fold 2 started at Fri Dec 10 17:29:22 2021\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 3.56215\tvalid_1's rmse: 3.6938\n",
      "[200]\ttraining's rmse: 3.41815\tvalid_1's rmse: 3.65986\n",
      "[300]\ttraining's rmse: 3.32374\tvalid_1's rmse: 3.64806\n",
      "[400]\ttraining's rmse: 3.25715\tvalid_1's rmse: 3.64273\n",
      "[500]\ttraining's rmse: 3.20233\tvalid_1's rmse: 3.63902\n",
      "[600]\ttraining's rmse: 3.15444\tvalid_1's rmse: 3.6379\n",
      "[700]\ttraining's rmse: 3.11158\tvalid_1's rmse: 3.63711\n",
      "[800]\ttraining's rmse: 3.07195\tvalid_1's rmse: 3.63683\n",
      "[900]\ttraining's rmse: 3.03584\tvalid_1's rmse: 3.63651\n",
      "[1000]\ttraining's rmse: 3.00269\tvalid_1's rmse: 3.6368\n",
      "[1100]\ttraining's rmse: 2.97077\tvalid_1's rmse: 3.63688\n",
      "Early stopping, best iteration is:\n",
      "[873]\ttraining's rmse: 3.04587\tvalid_1's rmse: 3.6363\n",
      "[-1.09026428 -0.19881015 -0.44315367 ...  0.44752332 -1.85504279\n",
      "  0.00580476]\n",
      "Fold 3 started at Fri Dec 10 17:31:20 2021\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 3.58308\tvalid_1's rmse: 3.59741\n",
      "[200]\ttraining's rmse: 3.43887\tvalid_1's rmse: 3.56662\n",
      "[300]\ttraining's rmse: 3.34543\tvalid_1's rmse: 3.55832\n",
      "[400]\ttraining's rmse: 3.27687\tvalid_1's rmse: 3.5544\n",
      "[500]\ttraining's rmse: 3.22142\tvalid_1's rmse: 3.55332\n",
      "[600]\ttraining's rmse: 3.1733\tvalid_1's rmse: 3.55362\n",
      "[700]\ttraining's rmse: 3.12917\tvalid_1's rmse: 3.55297\n",
      "[800]\ttraining's rmse: 3.0897\tvalid_1's rmse: 3.55341\n",
      "[900]\ttraining's rmse: 3.0524\tvalid_1's rmse: 3.55374\n",
      "Early stopping, best iteration is:\n",
      "[682]\ttraining's rmse: 3.13733\tvalid_1's rmse: 3.55269\n",
      "[-1.38429609 -0.23610997 -0.5503076  ...  0.59689564 -2.5291073\n",
      "  0.0212343 ]\n",
      "Fold 4 started at Fri Dec 10 17:33:02 2021\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 3.54652\tvalid_1's rmse: 3.75845\n",
      "[200]\ttraining's rmse: 3.40101\tvalid_1's rmse: 3.7188\n",
      "[300]\ttraining's rmse: 3.31012\tvalid_1's rmse: 3.70682\n",
      "[400]\ttraining's rmse: 3.24284\tvalid_1's rmse: 3.70201\n",
      "[500]\ttraining's rmse: 3.18779\tvalid_1's rmse: 3.69977\n",
      "[600]\ttraining's rmse: 3.13972\tvalid_1's rmse: 3.69841\n",
      "[700]\ttraining's rmse: 3.09662\tvalid_1's rmse: 3.69747\n",
      "[800]\ttraining's rmse: 3.05818\tvalid_1's rmse: 3.69692\n",
      "[900]\ttraining's rmse: 3.02174\tvalid_1's rmse: 3.69671\n",
      "[1000]\ttraining's rmse: 2.98781\tvalid_1's rmse: 3.69704\n",
      "[1100]\ttraining's rmse: 2.95624\tvalid_1's rmse: 3.69697\n",
      "[1200]\ttraining's rmse: 2.92422\tvalid_1's rmse: 3.69742\n",
      "Early stopping, best iteration is:\n",
      "[931]\ttraining's rmse: 3.01071\tvalid_1's rmse: 3.69649\n",
      "[-1.74081706 -0.29046085 -0.74337167 ...  0.76994018 -3.45417076\n",
      "  0.02146201]\n",
      "CV mean score: 3.6406, std: 0.0624.\n"
     ]
    }
   ],
   "source": [
    "#### lgb\n",
    "lgb_params = {'num_leaves': 63,\n",
    "             'min_data_in_leaf': 32, \n",
    "             'objective':'regression',\n",
    "             'max_depth': -1,\n",
    "             'learning_rate': 0.01,\n",
    "             \"min_child_samples\": 20,\n",
    "             \"boosting\": \"gbdt\",\n",
    "             \"feature_fraction\": 0.9,\n",
    "             \"bagging_freq\": 1,\n",
    "             \"bagging_fraction\": 0.9 ,\n",
    "             \"bagging_seed\": 11,\n",
    "             \"metric\": 'rmse',\n",
    "             \"lambda_l1\": 0.1,\n",
    "             \"verbosity\": -1}\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=4096)\n",
    "X_ntrain = ntrain[fea_cols].values\n",
    "X_train  = train[fea_cols].values\n",
    "X_test   = test[fea_cols].values\n",
    "print('='*10,'回归模型','='*10)\n",
    "oof_lgb , predictions_lgb , scores_lgb  = train_model(X_train , X_test, y_train, params=lgb_params, folds=folds, model_type='lgb', eval_type='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== without outliers 回归模型 ==========\n",
      "Fold 0 started at Fri Dec 10 17:35:07 2021\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 1.58507\tvalid_1's rmse: 1.57352\n",
      "[200]\ttraining's rmse: 1.54438\tvalid_1's rmse: 1.54804\n",
      "[300]\ttraining's rmse: 1.52302\tvalid_1's rmse: 1.5404\n",
      "[400]\ttraining's rmse: 1.5073\tvalid_1's rmse: 1.53709\n",
      "[500]\ttraining's rmse: 1.4939\tvalid_1's rmse: 1.53534\n",
      "[600]\ttraining's rmse: 1.48186\tvalid_1's rmse: 1.53435\n",
      "[700]\ttraining's rmse: 1.47102\tvalid_1's rmse: 1.53392\n",
      "[800]\ttraining's rmse: 1.46108\tvalid_1's rmse: 1.53367\n",
      "[900]\ttraining's rmse: 1.45134\tvalid_1's rmse: 1.53348\n",
      "[1000]\ttraining's rmse: 1.44205\tvalid_1's rmse: 1.53335\n",
      "[1100]\ttraining's rmse: 1.43309\tvalid_1's rmse: 1.53335\n",
      "[1200]\ttraining's rmse: 1.42441\tvalid_1's rmse: 1.53339\n",
      "[1300]\ttraining's rmse: 1.41588\tvalid_1's rmse: 1.53345\n",
      "Early stopping, best iteration is:\n",
      "[1029]\ttraining's rmse: 1.43938\tvalid_1's rmse: 1.53331\n",
      "[-0.06508893 -0.06521363 -0.07806157 ...  0.19169839 -0.14883775\n",
      "  0.02124167]\n",
      "Fold 1 started at Fri Dec 10 17:37:34 2021\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 1.57476\tvalid_1's rmse: 1.62101\n",
      "[200]\ttraining's rmse: 1.53491\tvalid_1's rmse: 1.59176\n",
      "[300]\ttraining's rmse: 1.51377\tvalid_1's rmse: 1.58229\n",
      "[400]\ttraining's rmse: 1.49806\tvalid_1's rmse: 1.57817\n",
      "[500]\ttraining's rmse: 1.48479\tvalid_1's rmse: 1.57626\n",
      "[600]\ttraining's rmse: 1.47292\tvalid_1's rmse: 1.57515\n",
      "[700]\ttraining's rmse: 1.46207\tvalid_1's rmse: 1.57441\n",
      "[800]\ttraining's rmse: 1.45205\tvalid_1's rmse: 1.57402\n",
      "[900]\ttraining's rmse: 1.44246\tvalid_1's rmse: 1.57371\n",
      "[1000]\ttraining's rmse: 1.43341\tvalid_1's rmse: 1.57369\n",
      "[1100]\ttraining's rmse: 1.42455\tvalid_1's rmse: 1.57362\n",
      "[1200]\ttraining's rmse: 1.41607\tvalid_1's rmse: 1.5735\n",
      "[1300]\ttraining's rmse: 1.40768\tvalid_1's rmse: 1.5735\n",
      "[1400]\ttraining's rmse: 1.39948\tvalid_1's rmse: 1.57356\n",
      "[1500]\ttraining's rmse: 1.39155\tvalid_1's rmse: 1.57347\n",
      "[1600]\ttraining's rmse: 1.38389\tvalid_1's rmse: 1.57352\n",
      "[1700]\ttraining's rmse: 1.37582\tvalid_1's rmse: 1.57351\n",
      "[1800]\ttraining's rmse: 1.36817\tvalid_1's rmse: 1.57339\n",
      "[1900]\ttraining's rmse: 1.36059\tvalid_1's rmse: 1.5734\n",
      "[2000]\ttraining's rmse: 1.35306\tvalid_1's rmse: 1.57349\n",
      "[2100]\ttraining's rmse: 1.34556\tvalid_1's rmse: 1.57352\n",
      "Early stopping, best iteration is:\n",
      "[1831]\ttraining's rmse: 1.36581\tvalid_1's rmse: 1.57335\n",
      "[-0.12027284 -0.11705953 -0.13617869 ...  0.38632728 -0.299064\n",
      "  0.0358265 ]\n",
      "Fold 2 started at Fri Dec 10 17:41:05 2021\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 1.58246\tvalid_1's rmse: 1.58716\n",
      "[200]\ttraining's rmse: 1.54251\tvalid_1's rmse: 1.5598\n",
      "[300]\ttraining's rmse: 1.52135\tvalid_1's rmse: 1.55098\n",
      "[400]\ttraining's rmse: 1.50577\tvalid_1's rmse: 1.54723\n",
      "[500]\ttraining's rmse: 1.49251\tvalid_1's rmse: 1.54527\n",
      "[600]\ttraining's rmse: 1.48073\tvalid_1's rmse: 1.54422\n",
      "[700]\ttraining's rmse: 1.47\tvalid_1's rmse: 1.54362\n",
      "[800]\ttraining's rmse: 1.46008\tvalid_1's rmse: 1.54302\n",
      "[900]\ttraining's rmse: 1.45073\tvalid_1's rmse: 1.54287\n",
      "[1000]\ttraining's rmse: 1.44151\tvalid_1's rmse: 1.54266\n",
      "[1100]\ttraining's rmse: 1.43267\tvalid_1's rmse: 1.54267\n",
      "[1200]\ttraining's rmse: 1.42393\tvalid_1's rmse: 1.54253\n",
      "[1300]\ttraining's rmse: 1.41568\tvalid_1's rmse: 1.54243\n",
      "[1400]\ttraining's rmse: 1.40738\tvalid_1's rmse: 1.54245\n",
      "[1500]\ttraining's rmse: 1.39932\tvalid_1's rmse: 1.54237\n",
      "[1600]\ttraining's rmse: 1.39153\tvalid_1's rmse: 1.54248\n",
      "[1700]\ttraining's rmse: 1.38371\tvalid_1's rmse: 1.54251\n",
      "[1800]\ttraining's rmse: 1.37612\tvalid_1's rmse: 1.5424\n",
      "Early stopping, best iteration is:\n",
      "[1509]\ttraining's rmse: 1.39862\tvalid_1's rmse: 1.54236\n",
      "[-0.18933571 -0.16397353 -0.25624219 ...  0.56699363 -0.42652128\n",
      "  0.07710602]\n",
      "Fold 3 started at Fri Dec 10 17:44:10 2021\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 1.57974\tvalid_1's rmse: 1.59438\n",
      "[200]\ttraining's rmse: 1.53938\tvalid_1's rmse: 1.56853\n",
      "[300]\ttraining's rmse: 1.51799\tvalid_1's rmse: 1.56078\n",
      "[400]\ttraining's rmse: 1.50236\tvalid_1's rmse: 1.5573\n",
      "[500]\ttraining's rmse: 1.48892\tvalid_1's rmse: 1.55546\n",
      "[600]\ttraining's rmse: 1.47686\tvalid_1's rmse: 1.55456\n",
      "[700]\ttraining's rmse: 1.4659\tvalid_1's rmse: 1.55421\n",
      "[800]\ttraining's rmse: 1.45562\tvalid_1's rmse: 1.55387\n",
      "[900]\ttraining's rmse: 1.446\tvalid_1's rmse: 1.55368\n",
      "[1000]\ttraining's rmse: 1.43696\tvalid_1's rmse: 1.55356\n",
      "[1100]\ttraining's rmse: 1.42805\tvalid_1's rmse: 1.55338\n",
      "[1200]\ttraining's rmse: 1.41942\tvalid_1's rmse: 1.5535\n",
      "[1300]\ttraining's rmse: 1.4111\tvalid_1's rmse: 1.5535\n",
      "[1400]\ttraining's rmse: 1.40249\tvalid_1's rmse: 1.55351\n",
      "Early stopping, best iteration is:\n",
      "[1101]\ttraining's rmse: 1.42797\tvalid_1's rmse: 1.55337\n",
      "[-0.24594063 -0.23030825 -0.34432175 ...  0.72062154 -0.5556942\n",
      "  0.10734624]\n",
      "Fold 4 started at Fri Dec 10 17:46:44 2021\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 1.58187\tvalid_1's rmse: 1.58911\n",
      "[200]\ttraining's rmse: 1.54164\tvalid_1's rmse: 1.56133\n",
      "[300]\ttraining's rmse: 1.52052\tvalid_1's rmse: 1.55252\n",
      "[400]\ttraining's rmse: 1.50497\tvalid_1's rmse: 1.54862\n",
      "[500]\ttraining's rmse: 1.49161\tvalid_1's rmse: 1.54637\n",
      "[600]\ttraining's rmse: 1.47963\tvalid_1's rmse: 1.54515\n",
      "[700]\ttraining's rmse: 1.46893\tvalid_1's rmse: 1.54444\n",
      "[800]\ttraining's rmse: 1.4588\tvalid_1's rmse: 1.54376\n",
      "[900]\ttraining's rmse: 1.44924\tvalid_1's rmse: 1.54355\n",
      "[1000]\ttraining's rmse: 1.43993\tvalid_1's rmse: 1.54321\n",
      "[1100]\ttraining's rmse: 1.43109\tvalid_1's rmse: 1.54309\n",
      "[1200]\ttraining's rmse: 1.42232\tvalid_1's rmse: 1.54305\n",
      "[1300]\ttraining's rmse: 1.41395\tvalid_1's rmse: 1.54302\n",
      "[1400]\ttraining's rmse: 1.40547\tvalid_1's rmse: 1.54296\n",
      "[1500]\ttraining's rmse: 1.3974\tvalid_1's rmse: 1.54289\n",
      "[1600]\ttraining's rmse: 1.3892\tvalid_1's rmse: 1.54281\n",
      "[1700]\ttraining's rmse: 1.38134\tvalid_1's rmse: 1.54285\n",
      "[1800]\ttraining's rmse: 1.37349\tvalid_1's rmse: 1.54286\n",
      "[1900]\ttraining's rmse: 1.36574\tvalid_1's rmse: 1.54282\n",
      "[2000]\ttraining's rmse: 1.35823\tvalid_1's rmse: 1.54286\n",
      "[2100]\ttraining's rmse: 1.35062\tvalid_1's rmse: 1.54279\n",
      "[2200]\ttraining's rmse: 1.34327\tvalid_1's rmse: 1.54271\n",
      "[2300]\ttraining's rmse: 1.33602\tvalid_1's rmse: 1.54272\n",
      "[2400]\ttraining's rmse: 1.32873\tvalid_1's rmse: 1.5427\n",
      "[2500]\ttraining's rmse: 1.32166\tvalid_1's rmse: 1.54267\n",
      "[2600]\ttraining's rmse: 1.3145\tvalid_1's rmse: 1.54265\n",
      "[2700]\ttraining's rmse: 1.30755\tvalid_1's rmse: 1.54261\n",
      "[2800]\ttraining's rmse: 1.30069\tvalid_1's rmse: 1.54257\n",
      "[2900]\ttraining's rmse: 1.29386\tvalid_1's rmse: 1.54265\n",
      "[3000]\ttraining's rmse: 1.28703\tvalid_1's rmse: 1.54277\n",
      "Early stopping, best iteration is:\n",
      "[2790]\ttraining's rmse: 1.30137\tvalid_1's rmse: 1.54252\n",
      "[-0.29965163 -0.27168814 -0.44455371 ...  0.90109923 -0.70524761\n",
      "  0.12724924]\n",
      "CV mean score: 1.5490, std: 0.0137.\n"
     ]
    }
   ],
   "source": [
    "print('='*10,'without outliers 回归模型','='*10)\n",
    "oof_nlgb, predictions_nlgb, scores_nlgb = train_model(X_ntrain, X_test, y_ntrain, params=lgb_params, folds=folds, model_type='lgb', eval_type='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 分类模型 ==========\n",
      "Fold 0 started at Fri Dec 10 17:51:30 2021\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's binary_logloss: 0.0342954\tvalid_1's binary_logloss: 0.0492323\n",
      "[200]\ttraining's binary_logloss: 0.026838\tvalid_1's binary_logloss: 0.0471868\n",
      "[300]\ttraining's binary_logloss: 0.0222938\tvalid_1's binary_logloss: 0.0467354\n",
      "[400]\ttraining's binary_logloss: 0.0191484\tvalid_1's binary_logloss: 0.0467137\n",
      "[500]\ttraining's binary_logloss: 0.0167015\tvalid_1's binary_logloss: 0.0468944\n",
      "[600]\ttraining's binary_logloss: 0.0146738\tvalid_1's binary_logloss: 0.0471354\n",
      "Early stopping, best iteration is:\n",
      "[345]\ttraining's binary_logloss: 0.020748\tvalid_1's binary_logloss: 0.0466689\n",
      "[0.00711265 0.0002918  0.00202541 ... 0.00105985 0.01470362 0.0006471 ]\n",
      "Fold 1 started at Fri Dec 10 17:52:54 2021\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's binary_logloss: 0.0350895\tvalid_1's binary_logloss: 0.0456453\n",
      "[200]\ttraining's binary_logloss: 0.0276059\tvalid_1's binary_logloss: 0.0437306\n",
      "[300]\ttraining's binary_logloss: 0.0229982\tvalid_1's binary_logloss: 0.0431605\n",
      "[400]\ttraining's binary_logloss: 0.0198022\tvalid_1's binary_logloss: 0.0430241\n",
      "[500]\ttraining's binary_logloss: 0.0172975\tvalid_1's binary_logloss: 0.0430742\n",
      "[600]\ttraining's binary_logloss: 0.0152197\tvalid_1's binary_logloss: 0.0432746\n",
      "[700]\ttraining's binary_logloss: 0.0134587\tvalid_1's binary_logloss: 0.0435276\n",
      "Early stopping, best iteration is:\n",
      "[438]\ttraining's binary_logloss: 0.0188055\tvalid_1's binary_logloss: 0.0430019\n",
      "[0.01399422 0.00072365 0.00359592 ... 0.00198836 0.02363653 0.00125136]\n",
      "Fold 2 started at Fri Dec 10 17:54:31 2021\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's binary_logloss: 0.0348939\tvalid_1's binary_logloss: 0.0467379\n",
      "[200]\ttraining's binary_logloss: 0.027435\tvalid_1's binary_logloss: 0.044713\n",
      "[300]\ttraining's binary_logloss: 0.0228231\tvalid_1's binary_logloss: 0.0441817\n",
      "[400]\ttraining's binary_logloss: 0.0196469\tvalid_1's binary_logloss: 0.0440611\n",
      "[500]\ttraining's binary_logloss: 0.0171433\tvalid_1's binary_logloss: 0.0440998\n",
      "[600]\ttraining's binary_logloss: 0.0150744\tvalid_1's binary_logloss: 0.0443023\n",
      "[700]\ttraining's binary_logloss: 0.0133215\tvalid_1's binary_logloss: 0.0445326\n",
      "Early stopping, best iteration is:\n",
      "[442]\ttraining's binary_logloss: 0.0185326\tvalid_1's binary_logloss: 0.0440435\n",
      "[0.01925519 0.00104783 0.00504172 ... 0.00301264 0.03439724 0.00199249]\n",
      "Fold 3 started at Fri Dec 10 17:56:07 2021\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's binary_logloss: 0.0355286\tvalid_1's binary_logloss: 0.0440746\n",
      "[200]\ttraining's binary_logloss: 0.0278521\tvalid_1's binary_logloss: 0.0422092\n",
      "[300]\ttraining's binary_logloss: 0.0232321\tvalid_1's binary_logloss: 0.0416952\n",
      "[400]\ttraining's binary_logloss: 0.0200171\tvalid_1's binary_logloss: 0.0416345\n",
      "[500]\ttraining's binary_logloss: 0.0174975\tvalid_1's binary_logloss: 0.0417254\n",
      "[600]\ttraining's binary_logloss: 0.0154332\tvalid_1's binary_logloss: 0.0418939\n",
      "Early stopping, best iteration is:\n",
      "[392]\ttraining's binary_logloss: 0.0202413\tvalid_1's binary_logloss: 0.0416212\n",
      "[0.02621175 0.00136207 0.00617833 ... 0.00397193 0.04592331 0.00249446]\n",
      "Fold 4 started at Fri Dec 10 17:57:35 2021\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's binary_logloss: 0.0344132\tvalid_1's binary_logloss: 0.0481878\n",
      "[200]\ttraining's binary_logloss: 0.0269722\tvalid_1's binary_logloss: 0.0462323\n",
      "[300]\ttraining's binary_logloss: 0.0224957\tvalid_1's binary_logloss: 0.0456538\n",
      "[400]\ttraining's binary_logloss: 0.0193828\tvalid_1's binary_logloss: 0.0455806\n",
      "[500]\ttraining's binary_logloss: 0.0169227\tvalid_1's binary_logloss: 0.0457021\n",
      "[600]\ttraining's binary_logloss: 0.0148824\tvalid_1's binary_logloss: 0.0459512\n",
      "[700]\ttraining's binary_logloss: 0.0131668\tvalid_1's binary_logloss: 0.046225\n",
      "Early stopping, best iteration is:\n",
      "[412]\ttraining's binary_logloss: 0.0190598\tvalid_1's binary_logloss: 0.0455742\n",
      "[0.03182457 0.00164898 0.00748484 ... 0.00491424 0.06613099 0.00327218]\n",
      "CV mean score: 0.0442, std: 0.0018.\n"
     ]
    }
   ],
   "source": [
    "print('='*10,'分类模型','='*10)\n",
    "lgb_params['objective'] = 'binary'\n",
    "lgb_params['metric']    = 'binary_logloss'\n",
    "oof_blgb, predictions_blgb, scores_blgb = train_model(X_train , X_test, y_train_binary, params=lgb_params, folds=folds, model_type='lgb', eval_type='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后将所有预测结果进行保存，包括一个分类模型、以及两个回归模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = predictions_lgb\n",
    "sub_df.to_csv('predictions_lgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_lgb  = pd.DataFrame(oof_lgb)\n",
    "oof_nlgb = pd.DataFrame(oof_nlgb)\n",
    "oof_blgb = pd.DataFrame(oof_blgb)\n",
    "\n",
    "predictions_lgb  = pd.DataFrame(predictions_lgb)\n",
    "predictions_nlgb = pd.DataFrame(predictions_nlgb)\n",
    "predictions_blgb = pd.DataFrame(predictions_blgb)\n",
    "\n",
    "oof_lgb.to_csv('./result/oof_lgb.csv',header=None,index=False)\n",
    "oof_blgb.to_csv('./result/oof_blgb.csv',header=None,index=False)\n",
    "oof_nlgb.to_csv('./result/oof_nlgb.csv',header=None,index=False)\n",
    "\n",
    "predictions_lgb.to_csv('./result/predictions_lgb.csv',header=None,index=False)\n",
    "predictions_nlgb.to_csv('./result/predictions_nlgb.csv',header=None,index=False)\n",
    "predictions_blgb.to_csv('./result/predictions_blgb.csv',header=None,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和上一节类似，接下来我们考虑进行结果提交，查看经过特征优化后单模型的建模效果能否得到改善："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/10/3ePRrd78vUzCuVk.png\" alt=\"image-20211210185918996\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 模型 | Private Score | Public Score |\n",
    "| ------ | ------ | ------ |\n",
    "| randomforest | 3.65455 | 3.74969 |\n",
    "| randomforest+validation | 3.65173 | 3.74954 |\n",
    "| LightGBM | 3.69723 | 3.80436 |\n",
    "| LightGBM+validation | 3.64403 | 3.73875 |\n",
    "| XGBoost | 3.62832 | 3.72358 |\n",
    "| Voting_avr | 3.63650 | 3.73251 |\n",
    "| Voting_wei | 3.633307 | 3.72877 |\n",
    "| Stacking | 3.62798 | 3.72055 |\n",
    "| 第二阶段 | 特征优化 | 两阶段建模 |\n",
    "| LightGBM | 3.61177 | 3.68321 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- xgboost模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来进一步进行XGB模型训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 回归模型 ==========\n",
      "Fold 0 started at Fri Dec 10 14:42:42 2021\n",
      "[14:42:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:42:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.91280\tvalid_data-rmse:3.93511\n",
      "[100]\ttrain-rmse:3.11076\tvalid_data-rmse:3.66921\n",
      "[200]\ttrain-rmse:2.94294\tvalid_data-rmse:3.67428\n",
      "[269]\ttrain-rmse:2.83470\tvalid_data-rmse:3.67897\n",
      "[-0.52778709 -0.02903404 -0.14910837 ...  0.16122425 -0.6228838\n",
      " -0.0374747 ]\n",
      "Fold 1 started at Fri Dec 10 14:52:40 2021\n",
      "[14:52:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:52:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.92749\tvalid_data-rmse:3.88161\n",
      "[100]\ttrain-rmse:3.13011\tvalid_data-rmse:3.61806\n",
      "[200]\ttrain-rmse:2.94163\tvalid_data-rmse:3.62870\n",
      "[297]\ttrain-rmse:2.78714\tvalid_data-rmse:3.63452\n",
      "[-0.68882735 -0.04339781 -0.27126572 ...  0.32617965 -1.56695157\n",
      " -0.02613225]\n",
      "Fold 2 started at Fri Dec 10 15:03:41 2021\n",
      "[15:03:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:03:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.91551\tvalid_data-rmse:3.92386\n",
      "[100]\ttrain-rmse:3.11446\tvalid_data-rmse:3.64251\n",
      "[200]\ttrain-rmse:2.93679\tvalid_data-rmse:3.64379\n",
      "[288]\ttrain-rmse:2.79779\tvalid_data-rmse:3.64629\n",
      "[-0.7934759  -0.0617529  -0.51275339 ...  0.47117299 -2.13647419\n",
      "  0.01525615]\n",
      "Fold 3 started at Fri Dec 10 15:14:23 2021\n",
      "[15:14:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:14:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.91979\tvalid_data-rmse:3.91682\n",
      "[100]\ttrain-rmse:3.14526\tvalid_data-rmse:3.62376\n",
      "[200]\ttrain-rmse:2.97177\tvalid_data-rmse:3.62676\n",
      "[300]\ttrain-rmse:2.81016\tvalid_data-rmse:3.63205\n",
      "[328]\ttrain-rmse:2.76400\tvalid_data-rmse:3.63302\n",
      "[-1.12153718 -0.06475663 -0.89851882 ...  0.66298851 -3.06915349\n",
      "  0.04042776]\n",
      "Fold 4 started at Fri Dec 10 15:26:26 2021\n",
      "[15:26:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:26:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.90284\tvalid_data-rmse:3.97906\n",
      "[100]\ttrain-rmse:3.12338\tvalid_data-rmse:3.70050\n",
      "[200]\ttrain-rmse:2.93996\tvalid_data-rmse:3.70575\n",
      "[287]\ttrain-rmse:2.80584\tvalid_data-rmse:3.71070\n",
      "[-1.50740474 -0.08236873 -1.12767107 ...  0.82137597 -3.66280705\n",
      "  0.10617083]\n",
      "CV mean score: 3.6495, std: 0.0307.\n",
      "========== without outliers 回归模型 ==========\n",
      "Fold 0 started at Fri Dec 10 15:36:59 2021\n",
      "[15:37:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:37:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:1.77370\tvalid_data-rmse:1.77778\n",
      "[100]\ttrain-rmse:1.35114\tvalid_data-rmse:1.55516\n",
      "[200]\ttrain-rmse:1.28709\tvalid_data-rmse:1.55601\n",
      "[300]\ttrain-rmse:1.22624\tvalid_data-rmse:1.55789\n",
      "[336]\ttrain-rmse:1.20167\tvalid_data-rmse:1.55871\n",
      "[-0.06756948 -0.04891101 -0.13096142 ...  0.14636049 -0.09830584\n",
      "  0.07206193]\n",
      "Fold 1 started at Fri Dec 10 15:49:42 2021\n",
      "[15:49:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:49:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:1.77420\tvalid_data-rmse:1.77641\n",
      "[100]\ttrain-rmse:1.35321\tvalid_data-rmse:1.55252\n",
      "[200]\ttrain-rmse:1.28841\tvalid_data-rmse:1.55354\n",
      "[300]\ttrain-rmse:1.22668\tvalid_data-rmse:1.55520\n",
      "[323]\ttrain-rmse:1.21330\tvalid_data-rmse:1.55572\n",
      "[-0.12837723 -0.13266747 -0.25065669 ...  0.27455248 -0.20806175\n",
      "  0.10240338]\n",
      "Fold 2 started at Fri Dec 10 16:01:52 2021\n",
      "[16:01:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:01:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:1.77630\tvalid_data-rmse:1.76708\n",
      "[100]\ttrain-rmse:1.35935\tvalid_data-rmse:1.54777\n",
      "[200]\ttrain-rmse:1.29239\tvalid_data-rmse:1.54895\n",
      "[300]\ttrain-rmse:1.22741\tvalid_data-rmse:1.55099\n",
      "[322]\ttrain-rmse:1.21481\tvalid_data-rmse:1.55128\n",
      "[-0.19980675 -0.14491702 -0.3423338  ...  0.39823987 -0.34590432\n",
      "  0.12995213]\n",
      "Fold 3 started at Fri Dec 10 16:14:05 2021\n",
      "[16:14:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:14:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:1.77012\tvalid_data-rmse:1.79367\n",
      "[100]\ttrain-rmse:1.35320\tvalid_data-rmse:1.56222\n",
      "[200]\ttrain-rmse:1.28926\tvalid_data-rmse:1.56285\n",
      "[300]\ttrain-rmse:1.22814\tvalid_data-rmse:1.56479\n",
      "[329]\ttrain-rmse:1.21202\tvalid_data-rmse:1.56540\n",
      "[-0.27627151 -0.19159377 -0.49871144 ...  0.55880515 -0.50439461\n",
      "  0.18084717]\n",
      "Fold 4 started at Fri Dec 10 16:26:23 2021\n",
      "[16:26:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:26:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:1.77521\tvalid_data-rmse:1.77200\n",
      "[100]\ttrain-rmse:1.35361\tvalid_data-rmse:1.54979\n",
      "[200]\ttrain-rmse:1.28846\tvalid_data-rmse:1.55029\n",
      "[300]\ttrain-rmse:1.22990\tvalid_data-rmse:1.55223\n",
      "[312]\ttrain-rmse:1.22221\tvalid_data-rmse:1.55248\n",
      "[-0.34155425 -0.21998161 -0.64989349 ...  0.78263196 -0.62289572\n",
      "  0.20943911]\n",
      "CV mean score: 1.5531, std: 0.0049.\n",
      "========== 分类模型 ==========\n",
      "Fold 0 started at Fri Dec 10 16:38:03 2021\n",
      "[16:38:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"metric\", \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:0.47596\tvalid_data-rmse:0.47609\n",
      "[100]\ttrain-rmse:0.09525\tvalid_data-rmse:0.10027\n",
      "[200]\ttrain-rmse:0.09072\tvalid_data-rmse:0.10021\n",
      "[300]\ttrain-rmse:0.08598\tvalid_data-rmse:0.10034\n",
      "[329]\ttrain-rmse:0.08463\tvalid_data-rmse:0.10041\n",
      "[0.01119628 0.00077406 0.00198021 ... 0.0015383  0.01258104 0.0014235 ]\n",
      "Fold 1 started at Fri Dec 10 16:46:46 2021\n",
      "[16:46:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"metric\", \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:0.47595\tvalid_data-rmse:0.47605\n",
      "[100]\ttrain-rmse:0.09565\tvalid_data-rmse:0.09782\n",
      "[200]\ttrain-rmse:0.09119\tvalid_data-rmse:0.09810\n",
      "[300]\ttrain-rmse:0.08654\tvalid_data-rmse:0.09827\n",
      "[314]\ttrain-rmse:0.08590\tvalid_data-rmse:0.09831\n",
      "[0.02243626 0.00151659 0.00403876 ... 0.00301257 0.02937779 0.00321147]\n",
      "Fold 2 started at Fri Dec 10 16:55:04 2021\n",
      "[16:55:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"metric\", \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:0.47615\tvalid_data-rmse:0.47608\n",
      "[100]\ttrain-rmse:0.09558\tvalid_data-rmse:0.09912\n",
      "[200]\ttrain-rmse:0.09104\tvalid_data-rmse:0.09904\n",
      "[300]\ttrain-rmse:0.08632\tvalid_data-rmse:0.09917\n",
      "[400]\ttrain-rmse:0.08174\tvalid_data-rmse:0.09935\n",
      "[431]\ttrain-rmse:0.08036\tvalid_data-rmse:0.09942\n",
      "[0.0305647  0.00196168 0.00790104 ... 0.00402639 0.04723352 0.0047453 ]\n",
      "Fold 3 started at Fri Dec 10 17:06:45 2021\n",
      "[17:06:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"metric\", \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:0.47614\tvalid_data-rmse:0.47607\n",
      "[100]\ttrain-rmse:0.09591\tvalid_data-rmse:0.09838\n",
      "[200]\ttrain-rmse:0.09149\tvalid_data-rmse:0.09856\n",
      "[300]\ttrain-rmse:0.08680\tvalid_data-rmse:0.09874\n",
      "[324]\ttrain-rmse:0.08565\tvalid_data-rmse:0.09879\n",
      "[0.04241783 0.00265251 0.01039481 ... 0.00533087 0.06383207 0.00631813]\n",
      "Fold 4 started at Fri Dec 10 17:15:22 2021\n",
      "[17:15:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"metric\", \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:0.47596\tvalid_data-rmse:0.47610\n",
      "[100]\ttrain-rmse:0.09511\tvalid_data-rmse:0.10076\n",
      "[200]\ttrain-rmse:0.09078\tvalid_data-rmse:0.10076\n",
      "[300]\ttrain-rmse:0.08607\tvalid_data-rmse:0.10097\n",
      "[348]\ttrain-rmse:0.08386\tvalid_data-rmse:0.10108\n",
      "[0.05003678 0.00315248 0.01173432 ... 0.00681464 0.07953846 0.0077831 ]\n",
      "CV mean score: 0.0442, std: 0.0008.\n"
     ]
    }
   ],
   "source": [
    "#### xgb\n",
    "xgb_params = {'eta':0.05, 'max_leaves':47, 'max_depth':10, 'subsample':0.8, 'colsample_bytree':0.8,\n",
    "              'min_child_weight':40, 'max_bin':128, 'reg_alpha':2.0, 'reg_lambda':2.0, \n",
    "              'objective':'reg:linear', 'eval_metric':'rmse', 'silent': True, 'nthread':4}\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "print('='*10,'回归模型','='*10)\n",
    "oof_xgb , predictions_xgb , scores_xgb  = train_model(X_train , X_test, y_train , params=xgb_params, folds=folds, model_type='xgb', eval_type='regression')\n",
    "print('='*10,'without outliers 回归模型','='*10)\n",
    "oof_nxgb, predictions_nxgb, scores_nxgb = train_model(X_ntrain, X_test, y_ntrain, params=xgb_params, folds=folds, model_type='xgb', eval_type='regression')\n",
    "print('='*10,'分类模型','='*10)\n",
    "xgb_params['objective'] = 'binary:logistic'\n",
    "xgb_params['metric']    = 'binary_logloss'\n",
    "oof_bxgb, predictions_bxgb, scores_bxgb = train_model(X_train , X_test, y_train_binary, params=xgb_params, folds=folds, model_type='xgb', eval_type='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后进行结果保存："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = predictions_xgb\n",
    "sub_df.to_csv('predictions_xgb.csv', index=False)\n",
    "\n",
    "oof_xgb  = pd.DataFrame(oof_xgb)\n",
    "oof_nxgb = pd.DataFrame(oof_nxgb)\n",
    "oof_bxgb = pd.DataFrame(oof_bxgb)\n",
    "\n",
    "predictions_xgb  = pd.DataFrame(predictions_xgb)\n",
    "predictions_nxgb = pd.DataFrame(predictions_nxgb)\n",
    "predictions_bxgb = pd.DataFrame(predictions_bxgb)\n",
    "\n",
    "oof_xgb.to_csv('./result/oof_xgb.csv',header=None,index=False)\n",
    "oof_bxgb.to_csv('./result/oof_bxgb.csv',header=None,index=False)\n",
    "oof_nxgb.to_csv('./result/oof_nxgb.csv',header=None,index=False)\n",
    "\n",
    "predictions_xgb.to_csv('./result/predictions_xgb.csv',header=None,index=False)\n",
    "predictions_nxgb.to_csv('./result/predictions_nxgb.csv',header=None,index=False)\n",
    "predictions_bxgb.to_csv('./result/predictions_bxgb.csv',header=None,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来提交单模型预测结果："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/10/YC6S7MjxDWKat3v.png\" alt=\"image-20211210190126611\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 模型 | Private Score | Public Score |\n",
    "| ------ | ------ | ------ |\n",
    "| randomforest | 3.65455 | 3.74969 |\n",
    "| randomforest+validation | 3.65173 | 3.74954 |\n",
    "| LightGBM | 3.69723 | 3.80436 |\n",
    "| LightGBM+validation | 3.64403 | 3.73875 |\n",
    "| XGBoost | 3.62832 | 3.72358 |\n",
    "| Voting_avr | 3.63650 | 3.73251 |\n",
    "| Voting_wei | 3.633307 | 3.72877 |\n",
    "| Stacking | 3.62798 | 3.72055 |\n",
    "| 第二阶段 | 特征优化 | 两阶段建模 |\n",
    "| LightGBM | 3.61177 | 3.68321 |\n",
    "| XGBoost | 3.61048 | 3.68938 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来进行CatBoost模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 回归模型 ==========\n",
      "Fold 0 started at Fri Dec 10 14:10:51 2021\n",
      "0:\tlearn: 3.8381846\ttest: 3.8311905\tbest: 3.8311905 (0)\ttotal: 236ms\tremaining: 1h 18m 44s\n",
      "100:\tlearn: 3.5804567\ttest: 3.6395017\tbest: 3.6395017 (100)\ttotal: 18.3s\tremaining: 1h 11s\n",
      "200:\tlearn: 3.5310391\ttest: 3.6300466\tbest: 3.6300466 (200)\ttotal: 35.8s\tremaining: 58m 47s\n",
      "300:\tlearn: 3.4982712\ttest: 3.6272877\tbest: 3.6272877 (300)\ttotal: 53s\tremaining: 57m 45s\n",
      "400:\tlearn: 3.4686121\ttest: 3.6259688\tbest: 3.6258482 (397)\ttotal: 1m 9s\tremaining: 56m 59s\n",
      "500:\tlearn: 3.4333208\ttest: 3.6241788\tbest: 3.6239916 (487)\ttotal: 1m 27s\tremaining: 56m 35s\n",
      "600:\tlearn: 3.4075105\ttest: 3.6227705\tbest: 3.6227273 (598)\ttotal: 1m 44s\tremaining: 56m 5s\n",
      "700:\tlearn: 3.3807630\ttest: 3.6213419\tbest: 3.6213094 (697)\ttotal: 2m 1s\tremaining: 55m 41s\n",
      "800:\tlearn: 3.3501182\ttest: 3.6215946\tbest: 3.6211724 (756)\ttotal: 2m 18s\tremaining: 55m 21s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.621172363\n",
      "bestIteration = 756\n",
      "\n",
      "Shrink model to first 757 iterations.\n",
      "[-0.65279696 -0.09954155 -0.20841098 ...  0.15204254 -0.55215467\n",
      "  0.02314466]\n",
      "Fold 1 started at Fri Dec 10 14:13:37 2021\n",
      "0:\tlearn: 3.8360447\ttest: 3.8403212\tbest: 3.8403212 (0)\ttotal: 228ms\tremaining: 1h 15m 56s\n",
      "100:\tlearn: 3.5722456\ttest: 3.6660867\tbest: 3.6660867 (100)\ttotal: 18.3s\tremaining: 1h 13s\n",
      "200:\tlearn: 3.5249906\ttest: 3.6600855\tbest: 3.6600052 (199)\ttotal: 35.6s\tremaining: 58m 22s\n",
      "300:\tlearn: 3.4933038\ttest: 3.6563544\tbest: 3.6563314 (295)\ttotal: 52.6s\tremaining: 57m 24s\n",
      "400:\tlearn: 3.4669326\ttest: 3.6551952\tbest: 3.6551952 (400)\ttotal: 1m 9s\tremaining: 56m 36s\n",
      "500:\tlearn: 3.4367649\ttest: 3.6543272\tbest: 3.6542704 (482)\ttotal: 1m 26s\tremaining: 56m 13s\n",
      "600:\tlearn: 3.3987205\ttest: 3.6526854\tbest: 3.6524586 (592)\ttotal: 1m 43s\tremaining: 55m 52s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.652458571\n",
      "bestIteration = 592\n",
      "\n",
      "Shrink model to first 593 iterations.\n",
      "[-1.24115261 -0.19470337 -0.37904966 ...  0.34714556 -1.32674214\n",
      "  0.07462047]\n",
      "Fold 2 started at Fri Dec 10 14:15:53 2021\n",
      "0:\tlearn: 3.8366814\ttest: 3.8397769\tbest: 3.8397769 (0)\ttotal: 222ms\tremaining: 1h 13m 53s\n",
      "100:\tlearn: 3.5719968\ttest: 3.6685589\tbest: 3.6685589 (100)\ttotal: 18.3s\tremaining: 1h 13s\n",
      "200:\tlearn: 3.5286335\ttest: 3.6645985\tbest: 3.6645899 (199)\ttotal: 35.5s\tremaining: 58m 21s\n",
      "300:\tlearn: 3.5003415\ttest: 3.6620718\tbest: 3.6620288 (297)\ttotal: 52.6s\tremaining: 57m 24s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.661529129\n",
      "bestIteration = 345\n",
      "\n",
      "Shrink model to first 346 iterations.\n",
      "[-1.8242473  -0.23891476 -0.53753361 ...  0.50192887 -2.0036456\n",
      "  0.10636305]\n",
      "Fold 3 started at Fri Dec 10 14:17:27 2021\n",
      "0:\tlearn: 3.8403769\ttest: 3.8225309\tbest: 3.8225309 (0)\ttotal: 218ms\tremaining: 1h 12m 30s\n",
      "100:\tlearn: 3.5763907\ttest: 3.6377947\tbest: 3.6377947 (100)\ttotal: 18.4s\tremaining: 1h 15s\n",
      "200:\tlearn: 3.5289219\ttest: 3.6299710\tbest: 3.6299510 (199)\ttotal: 35.9s\tremaining: 58m 56s\n",
      "300:\tlearn: 3.4923428\ttest: 3.6265291\tbest: 3.6264918 (299)\ttotal: 53.1s\tremaining: 57m 57s\n",
      "400:\tlearn: 3.4572797\ttest: 3.6254034\tbest: 3.6251948 (398)\ttotal: 1m 10s\tremaining: 57m 28s\n",
      "500:\tlearn: 3.4236551\ttest: 3.6242258\tbest: 3.6242009 (499)\ttotal: 1m 27s\tremaining: 56m 56s\n",
      "600:\tlearn: 3.3901851\ttest: 3.6240402\tbest: 3.6236073 (585)\ttotal: 1m 45s\tremaining: 56m 29s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.623607289\n",
      "bestIteration = 585\n",
      "\n",
      "Shrink model to first 586 iterations.\n",
      "[-2.41554127 -0.32083648 -0.72403641 ...  0.63439928 -2.51925915\n",
      "  0.11511037]\n",
      "Fold 4 started at Fri Dec 10 14:19:44 2021\n",
      "0:\tlearn: 3.8322753\ttest: 3.8564668\tbest: 3.8564668 (0)\ttotal: 219ms\tremaining: 1h 13m 6s\n",
      "100:\tlearn: 3.5717087\ttest: 3.6865938\tbest: 3.6865555 (99)\ttotal: 18.2s\tremaining: 59m 48s\n",
      "200:\tlearn: 3.5242182\ttest: 3.6814597\tbest: 3.6814281 (198)\ttotal: 35.7s\tremaining: 58m 33s\n",
      "300:\tlearn: 3.4934559\ttest: 3.6780353\tbest: 3.6779068 (286)\ttotal: 52.9s\tremaining: 57m 43s\n",
      "400:\tlearn: 3.4690108\ttest: 3.6763199\tbest: 3.6762201 (397)\ttotal: 1m 9s\tremaining: 56m 53s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.675877799\n",
      "bestIteration = 441\n",
      "\n",
      "Shrink model to first 442 iterations.\n",
      "[-3.08624121 -0.3760431  -0.94135768 ...  0.80912461 -3.1452267\n",
      "  0.13862159]\n",
      "CV mean score: 3.6469, std: 0.0214.\n",
      "========== without outliers 回归模型 ==========\n",
      "Fold 0 started at Fri Dec 10 14:21:35 2021\n",
      "0:\tlearn: 1.7052120\ttest: 1.7150498\tbest: 1.7150498 (0)\ttotal: 221ms\tremaining: 1h 13m 45s\n",
      "100:\tlearn: 1.5415023\ttest: 1.5677009\tbest: 1.5677009 (100)\ttotal: 18.7s\tremaining: 1h 1m 26s\n",
      "200:\tlearn: 1.5211203\ttest: 1.5618707\tbest: 1.5618546 (198)\ttotal: 36.8s\tremaining: 1h 22s\n",
      "300:\tlearn: 1.5034715\ttest: 1.5599689\tbest: 1.5599593 (298)\ttotal: 54.8s\tremaining: 59m 47s\n",
      "400:\tlearn: 1.4884908\ttest: 1.5591562\tbest: 1.5591562 (400)\ttotal: 1m 12s\tremaining: 59m 7s\n",
      "500:\tlearn: 1.4735097\ttest: 1.5587325\tbest: 1.5586729 (488)\ttotal: 1m 30s\tremaining: 58m 39s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 1.558672919\n",
      "bestIteration = 488\n",
      "\n",
      "Shrink model to first 489 iterations.\n",
      "[-0.05198294 -0.05876552 -0.10657192 ...  0.2047596  -0.1312454\n",
      "  0.05139249]\n",
      "Fold 1 started at Fri Dec 10 14:23:38 2021\n",
      "0:\tlearn: 1.7103352\ttest: 1.6930077\tbest: 1.6930077 (0)\ttotal: 220ms\tremaining: 1h 13m 22s\n",
      "100:\tlearn: 1.5454527\ttest: 1.5498429\tbest: 1.5498429 (100)\ttotal: 18.7s\tremaining: 1h 1m 21s\n",
      "200:\tlearn: 1.5253202\ttest: 1.5444099\tbest: 1.5443993 (199)\ttotal: 36.9s\tremaining: 1h 30s\n",
      "300:\tlearn: 1.5090436\ttest: 1.5426857\tbest: 1.5426834 (299)\ttotal: 54.7s\tremaining: 59m 39s\n",
      "400:\tlearn: 1.4934406\ttest: 1.5418198\tbest: 1.5417236 (384)\ttotal: 1m 12s\tremaining: 59m 3s\n",
      "500:\tlearn: 1.4788711\ttest: 1.5413138\tbest: 1.5413138 (500)\ttotal: 1m 30s\tremaining: 58m 35s\n",
      "600:\tlearn: 1.4652665\ttest: 1.5411175\tbest: 1.5410822 (596)\ttotal: 1m 48s\tremaining: 58m 10s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 1.541019643\n",
      "bestIteration = 618\n",
      "\n",
      "Shrink model to first 619 iterations.\n",
      "[-0.13457493 -0.12087788 -0.17085272 ...  0.41412351 -0.2757514\n",
      "  0.08662889]\n",
      "Fold 2 started at Fri Dec 10 14:26:04 2021\n",
      "0:\tlearn: 1.7121497\ttest: 1.6873546\tbest: 1.6873546 (0)\ttotal: 230ms\tremaining: 1h 16m 31s\n",
      "100:\tlearn: 1.5473283\ttest: 1.5439367\tbest: 1.5439367 (100)\ttotal: 19s\tremaining: 1h 2m 32s\n",
      "200:\tlearn: 1.5260258\ttest: 1.5384906\tbest: 1.5384906 (200)\ttotal: 37.3s\tremaining: 1h 1m 15s\n",
      "300:\tlearn: 1.5097682\ttest: 1.5364304\tbest: 1.5363946 (297)\ttotal: 55.2s\tremaining: 1h 12s\n",
      "400:\tlearn: 1.4936189\ttest: 1.5359010\tbest: 1.5358986 (387)\ttotal: 1m 13s\tremaining: 59m 32s\n",
      "500:\tlearn: 1.4802962\ttest: 1.5356234\tbest: 1.5355803 (494)\ttotal: 1m 30s\tremaining: 58m 55s\n",
      "600:\tlearn: 1.4661836\ttest: 1.5350710\tbest: 1.5350710 (600)\ttotal: 1m 48s\tremaining: 58m 26s\n",
      "700:\tlearn: 1.4527724\ttest: 1.5344467\tbest: 1.5344307 (692)\ttotal: 2m 6s\tremaining: 58m\n",
      "800:\tlearn: 1.4399622\ttest: 1.5344374\tbest: 1.5343379 (758)\ttotal: 2m 24s\tremaining: 57m 39s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 1.534337921\n",
      "bestIteration = 758\n",
      "\n",
      "Shrink model to first 759 iterations.\n",
      "[-0.18771868 -0.17862032 -0.28029316 ...  0.58437527 -0.43914693\n",
      "  0.12334909]\n",
      "Fold 3 started at Fri Dec 10 14:28:55 2021\n",
      "0:\tlearn: 1.7015705\ttest: 1.7298048\tbest: 1.7298048 (0)\ttotal: 230ms\tremaining: 1h 16m 49s\n",
      "100:\tlearn: 1.5405937\ttest: 1.5721758\tbest: 1.5721758 (100)\ttotal: 18.7s\tremaining: 1h 1m 28s\n",
      "200:\tlearn: 1.5199343\ttest: 1.5652099\tbest: 1.5652099 (200)\ttotal: 36.9s\tremaining: 1h 36s\n",
      "300:\tlearn: 1.5029939\ttest: 1.5625461\tbest: 1.5625461 (300)\ttotal: 54.9s\tremaining: 59m 54s\n",
      "400:\tlearn: 1.4865008\ttest: 1.5610120\tbest: 1.5609651 (391)\ttotal: 1m 12s\tremaining: 59m 16s\n",
      "500:\tlearn: 1.4723483\ttest: 1.5604946\tbest: 1.5603232 (485)\ttotal: 1m 30s\tremaining: 58m 43s\n",
      "600:\tlearn: 1.4584435\ttest: 1.5597562\tbest: 1.5597511 (598)\ttotal: 1m 48s\tremaining: 58m 27s\n",
      "700:\tlearn: 1.4449818\ttest: 1.5595495\tbest: 1.5594746 (689)\ttotal: 2m 6s\tremaining: 58m 8s\n",
      "800:\tlearn: 1.4323918\ttest: 1.5592901\tbest: 1.5592509 (797)\ttotal: 2m 24s\tremaining: 57m 44s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 1.55925089\n",
      "bestIteration = 797\n",
      "\n",
      "Shrink model to first 798 iterations.\n",
      "[-0.24981714 -0.22482568 -0.39894296 ...  0.81516561 -0.57439708\n",
      "  0.18916753]\n",
      "Fold 4 started at Fri Dec 10 14:31:53 2021\n",
      "0:\tlearn: 1.7059794\ttest: 1.7106634\tbest: 1.7106634 (0)\ttotal: 229ms\tremaining: 1h 16m 13s\n",
      "100:\tlearn: 1.5411250\ttest: 1.5673404\tbest: 1.5673404 (100)\ttotal: 18.7s\tremaining: 1h 1m 20s\n",
      "200:\tlearn: 1.5207652\ttest: 1.5618362\tbest: 1.5618362 (200)\ttotal: 36.9s\tremaining: 1h 30s\n",
      "300:\tlearn: 1.5043864\ttest: 1.5600173\tbest: 1.5600173 (300)\ttotal: 54.8s\tremaining: 59m 46s\n",
      "400:\tlearn: 1.4893808\ttest: 1.5593987\tbest: 1.5593987 (400)\ttotal: 1m 12s\tremaining: 59m 6s\n",
      "500:\tlearn: 1.4744062\ttest: 1.5588896\tbest: 1.5588699 (498)\ttotal: 1m 30s\tremaining: 58m 38s\n",
      "600:\tlearn: 1.4605963\ttest: 1.5586720\tbest: 1.5586720 (600)\ttotal: 1m 48s\tremaining: 58m 14s\n",
      "700:\tlearn: 1.4462134\ttest: 1.5586705\tbest: 1.5584470 (662)\ttotal: 2m 6s\tremaining: 57m 54s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 1.558447013\n",
      "bestIteration = 662\n",
      "\n",
      "Shrink model to first 663 iterations.\n",
      "[-0.31761126 -0.29341366 -0.47203202 ...  1.02352247 -0.7374635\n",
      "  0.23168438]\n",
      "CV mean score: 1.5503, std: 0.0106.\n",
      "========== 分类模型 ==========\n",
      "Fold 0 started at Fri Dec 10 14:34:27 2021\n",
      "0:\tlearn: 0.5881900\ttest: 0.5883451\tbest: 0.5883451 (0)\ttotal: 223ms\tremaining: 1h 14m 21s\n",
      "100:\tlearn: 0.0413496\ttest: 0.0441783\tbest: 0.0441783 (100)\ttotal: 19.1s\tremaining: 1h 2m 34s\n",
      "200:\tlearn: 0.0383345\ttest: 0.0436243\tbest: 0.0436243 (199)\ttotal: 37.6s\tremaining: 1h 1m 42s\n",
      "300:\tlearn: 0.0369849\ttest: 0.0435559\tbest: 0.0435538 (299)\ttotal: 55.5s\tremaining: 1h 33s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.04352843255\n",
      "bestIteration = 348\n",
      "\n",
      "Shrink model to first 349 iterations.\n",
      "[0.01512715 0.00037626 0.00099802 ... 0.00131343 0.01226378 0.00116028]\n",
      "Fold 1 started at Fri Dec 10 14:36:06 2021\n",
      "0:\tlearn: 0.5882134\ttest: 0.5883123\tbest: 0.5883123 (0)\ttotal: 224ms\tremaining: 1h 14m 44s\n",
      "100:\tlearn: 0.0414016\ttest: 0.0456306\tbest: 0.0456236 (97)\ttotal: 18.9s\tremaining: 1h 2m 12s\n",
      "200:\tlearn: 0.0391184\ttest: 0.0451740\tbest: 0.0451581 (193)\ttotal: 37.1s\tremaining: 1h 52s\n",
      "300:\tlearn: 0.0373915\ttest: 0.0450145\tbest: 0.0450114 (295)\ttotal: 55.2s\tremaining: 1h 11s\n",
      "400:\tlearn: 0.0361931\ttest: 0.0449611\tbest: 0.0449598 (399)\ttotal: 1m 13s\tremaining: 59m 30s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.04495927972\n",
      "bestIteration = 427\n",
      "\n",
      "Shrink model to first 428 iterations.\n",
      "[0.03724621 0.00081654 0.00180298 ... 0.00267721 0.02398923 0.00215875]\n",
      "Fold 2 started at Fri Dec 10 14:37:59 2021\n",
      "0:\tlearn: 0.5873628\ttest: 0.5874927\tbest: 0.5874927 (0)\ttotal: 220ms\tremaining: 1h 13m 10s\n",
      "100:\tlearn: 0.0414205\ttest: 0.0450185\tbest: 0.0450185 (100)\ttotal: 19.1s\tremaining: 1h 2m 38s\n",
      "200:\tlearn: 0.0389088\ttest: 0.0445833\tbest: 0.0445772 (186)\ttotal: 37.2s\tremaining: 1h 1m 2s\n",
      "300:\tlearn: 0.0373618\ttest: 0.0444011\tbest: 0.0443980 (298)\ttotal: 55.1s\tremaining: 1h 8s\n",
      "400:\tlearn: 0.0359941\ttest: 0.0442768\tbest: 0.0442767 (397)\ttotal: 1m 13s\tremaining: 59m 33s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.04425339457\n",
      "bestIteration = 444\n",
      "\n",
      "Shrink model to first 445 iterations.\n",
      "[0.05257821 0.00123257 0.00266437 ... 0.00415555 0.03226137 0.00372386]\n",
      "Fold 3 started at Fri Dec 10 14:39:56 2021\n",
      "0:\tlearn: 0.5864983\ttest: 0.5866568\tbest: 0.5866568 (0)\ttotal: 222ms\tremaining: 1h 13m 55s\n",
      "100:\tlearn: 0.0411054\ttest: 0.0439417\tbest: 0.0439417 (100)\ttotal: 19.1s\tremaining: 1h 2m 50s\n",
      "200:\tlearn: 0.0381717\ttest: 0.0434563\tbest: 0.0434563 (200)\ttotal: 37.6s\tremaining: 1h 1m 42s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.0433662965\n",
      "bestIteration = 230\n",
      "\n",
      "Shrink model to first 231 iterations.\n",
      "[0.06493956 0.00168451 0.00368316 ... 0.00597921 0.04235914 0.00486713]\n",
      "Fold 4 started at Fri Dec 10 14:41:14 2021\n",
      "0:\tlearn: 0.5868816\ttest: 0.5870558\tbest: 0.5870558 (0)\ttotal: 220ms\tremaining: 1h 13m 27s\n",
      "100:\tlearn: 0.0415364\ttest: 0.0452882\tbest: 0.0452882 (100)\ttotal: 18.9s\tremaining: 1h 2m 9s\n",
      "200:\tlearn: 0.0388722\ttest: 0.0447076\tbest: 0.0447076 (200)\ttotal: 37.4s\tremaining: 1h 1m 19s\n",
      "300:\tlearn: 0.0372053\ttest: 0.0446412\tbest: 0.0446020 (284)\ttotal: 55.6s\tremaining: 1h 35s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.04460201579\n",
      "bestIteration = 284\n",
      "\n",
      "Shrink model to first 285 iterations.\n",
      "[0.08571563 0.00224264 0.00487816 ... 0.00716189 0.05275265 0.00570325]\n",
      "CV mean score: 0.0441, std: 0.0006.\n"
     ]
    }
   ],
   "source": [
    "#### cat\n",
    "cat_params = {'learning_rate': 0.05, 'depth': 9, 'l2_leaf_reg': 10, 'bootstrap_type': 'Bernoulli',\n",
    "              'od_type': 'Iter', 'od_wait': 50, 'random_seed': 11, 'allow_writing_files': False}\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=18)\n",
    "print('='*10,'回归模型','='*10)\n",
    "oof_cat , predictions_cat , scores_cat  = train_model(X_train , X_test, y_train , params=cat_params, folds=folds, model_type='cat', eval_type='regression')\n",
    "print('='*10,'without outliers 回归模型','='*10)\n",
    "oof_ncat, predictions_ncat, scores_ncat = train_model(X_ntrain, X_test, y_ntrain, params=cat_params, folds=folds, model_type='cat', eval_type='regression')\n",
    "print('='*10,'分类模型','='*10)\n",
    "oof_bcat, predictions_bcat, scores_bcat = train_model(X_train , X_test, y_train_binary, params=cat_params, folds=folds, model_type='cat', eval_type='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时保存模型结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = predictions_cat\n",
    "sub_df.to_csv('predictions_cat.csv', index=False)\n",
    "\n",
    "oof_cat  = pd.DataFrame(oof_cat)\n",
    "oof_ncat = pd.DataFrame(oof_ncat)\n",
    "oof_bcat = pd.DataFrame(oof_bcat)\n",
    "\n",
    "predictions_cat  = pd.DataFrame(predictions_cat)\n",
    "predictions_ncat = pd.DataFrame(predictions_ncat)\n",
    "predictions_bcat = pd.DataFrame(predictions_bcat)\n",
    "\n",
    "oof_cat.to_csv('./result/oof_cat.csv',header=None,index=False)\n",
    "oof_bcat.to_csv('./result/oof_bcat.csv',header=None,index=False)\n",
    "oof_ncat.to_csv('./result/oof_ncat.csv',header=None,index=False)\n",
    "\n",
    "predictions_cat.to_csv('./result/predictions_cat.csv',header=None,index=False)\n",
    "predictions_ncat.to_csv('./result/predictions_ncat.csv',header=None,index=False)\n",
    "predictions_bcat.to_csv('./result/predictions_bcat.csv',header=None,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试单模型建模效果："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/10/IfC6ehVKURszlY1.png\" alt=\"image-20211210190346541\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 模型 | Private Score | Public Score |\n",
    "| ------ | ------ | ------ |\n",
    "| randomforest | 3.65455 | 3.74969 |\n",
    "| randomforest+validation | 3.65173 | 3.74954 |\n",
    "| LightGBM | 3.69723 | 3.80436 |\n",
    "| LightGBM+validation | 3.64403 | 3.73875 |\n",
    "| XGBoost | 3.62832 | 3.72358 |\n",
    "| Voting_avr | 3.63650 | 3.73251 |\n",
    "| Voting_wei | 3.633307 | 3.72877 |\n",
    "| Stacking | 3.62798 | 3.72055 |\n",
    "| 第二阶段 | 特征优化 | 两阶段建模 |\n",
    "| LightGBM | 3.61177 | 3.68321 |\n",
    "| XGBoost | 3.61048 | 3.68938 |\n",
    "| CatBoost | 3.61154 | 3.69951 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.融合阶段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 加权融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = (predictions_lgb + predictions_xgb.values.flatten() + predictions_cat.values.flatten()) / 3\n",
    "sub_df.to_csv('predictions_wei_average.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/10/61oY7cBnlZrJMvk.png\" alt=\"image-20211210190555770\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 模型 | Private Score | Public Score |\n",
    "| ------ | ------ | ------ |\n",
    "| randomforest | 3.65455 | 3.74969 |\n",
    "| randomforest+validation | 3.65173 | 3.74954 |\n",
    "| LightGBM | 3.69723 | 3.80436 |\n",
    "| LightGBM+validation | 3.64403 | 3.73875 |\n",
    "| XGBoost | 3.62832 | 3.72358 |\n",
    "| Voting_avr | 3.63650 | 3.73251 |\n",
    "| Voting_wei | 3.633307 | 3.72877 |\n",
    "| Stacking | 3.62798 | 3.72055 |\n",
    "| 第二阶段 | 特征优化 | 两阶段建模 |\n",
    "| LightGBM | 3.61177 | 3.68321 |\n",
    "| XGBoost | 3.61048 | 3.68938 |\n",
    "| CatBoost | 3.61154 | 3.69951 |\n",
    "| Voting | 3.60640 | 3.68697 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stacking融合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/08/ALF3cfuSwmB7b8z.png\" alt=\"image-20211208192640281\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_model(oof_1, oof_2, oof_3, predictions_1, predictions_2, predictions_3, y, eval_type='regression'):\n",
    "   \n",
    "    # Part 1.数据准备\n",
    "    # 按行拼接列，拼接验证集所有预测结果\n",
    "    # train_stack就是final model的训练数据\n",
    "    train_stack = np.hstack([oof_1, oof_2, oof_3])\n",
    "    # 按行拼接列，拼接测试集上所有预测结果\n",
    "    # test_stack就是final model的测试数据\n",
    "    test_stack = np.hstack([predictions_1, predictions_2, predictions_3])\n",
    "    # 创建一个和验证集行数相同的全零数组\n",
    "    oof = np.zeros(train_stack.shape[0])\n",
    "    # 创建一个和测试集行数相同的全零数组\n",
    "    predictions = np.zeros(test_stack.shape[0])\n",
    "    \n",
    "    # Part 2.多轮交叉验证\n",
    "    from sklearn.model_selection import RepeatedKFold\n",
    "    folds = RepeatedKFold(n_splits=5, n_repeats=2, random_state=2020)\n",
    "    \n",
    "    # fold_为折数，trn_idx为每一折训练集index，val_idx为每一折验证集index\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_stack, y)):\n",
    "        # 打印折数信息\n",
    "        print(\"fold n°{}\".format(fold_+1))\n",
    "        # 训练集中划分为训练数据的特征和标签\n",
    "        trn_data, trn_y = train_stack[trn_idx], y[trn_idx]\n",
    "        # 训练集中划分为验证数据的特征和标签\n",
    "        val_data, val_y = train_stack[val_idx], y[val_idx]\n",
    "        # 开始训练时提示\n",
    "        print(\"-\" * 10 + \"Stacking \" + str(fold_+1) + \"-\" * 10)\n",
    "        # 采用贝叶斯回归作为结果融合的模型（final model）\n",
    "        clf = BayesianRidge()\n",
    "        # 在训练数据上进行训练\n",
    "        clf.fit(trn_data, trn_y)\n",
    "        # 在验证数据上进行预测，并将结果记录在oof对应位置\n",
    "        # oof[val_idx] = clf.predict(val_data)\n",
    "        # 对测试集数据进行预测，每一轮预测结果占比额外的1/10\n",
    "        predictions += clf.predict(test_stack) / (5 * 2)\n",
    "        \n",
    "    if eval_type == 'regression':\n",
    "        print('mean: ',np.sqrt(mean_squared_error(y, oof)))\n",
    "    if eval_type == 'binary':\n",
    "        print('mean: ',log_loss(y, oof))\n",
    "    \n",
    "    # 返回测试集的预测结果\n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "fold n°1\n",
      "----------Stacking 1----------\n",
      "fold n°2\n",
      "----------Stacking 2----------\n",
      "fold n°3\n",
      "----------Stacking 3----------\n",
      "fold n°4\n",
      "----------Stacking 4----------\n",
      "fold n°5\n",
      "----------Stacking 5----------\n",
      "fold n°6\n",
      "----------Stacking 6----------\n",
      "fold n°7\n",
      "----------Stacking 7----------\n",
      "fold n°8\n",
      "----------Stacking 8----------\n",
      "fold n°9\n",
      "----------Stacking 9----------\n",
      "fold n°10\n",
      "----------Stacking 10----------\n",
      "mean:  3.8705589161316296\n",
      "==============================\n",
      "fold n°1\n",
      "----------Stacking 1----------\n",
      "fold n°2\n",
      "----------Stacking 2----------\n",
      "fold n°3\n",
      "----------Stacking 3----------\n",
      "fold n°4\n",
      "----------Stacking 4----------\n",
      "fold n°5\n",
      "----------Stacking 5----------\n",
      "fold n°6\n",
      "----------Stacking 6----------\n",
      "fold n°7\n",
      "----------Stacking 7----------\n",
      "fold n°8\n",
      "----------Stacking 8----------\n",
      "fold n°9\n",
      "----------Stacking 9----------\n",
      "fold n°10\n",
      "----------Stacking 10----------\n",
      "mean:  1.718066151175359\n",
      "==============================\n",
      "fold n°1\n",
      "----------Stacking 1----------\n",
      "fold n°2\n",
      "----------Stacking 2----------\n",
      "fold n°3\n",
      "----------Stacking 3----------\n",
      "fold n°4\n",
      "----------Stacking 4----------\n",
      "fold n°5\n",
      "----------Stacking 5----------\n",
      "fold n°6\n",
      "----------Stacking 6----------\n",
      "fold n°7\n",
      "----------Stacking 7----------\n",
      "fold n°8\n",
      "----------Stacking 8----------\n",
      "fold n°9\n",
      "----------Stacking 9----------\n",
      "fold n°10\n",
      "----------Stacking 10----------\n",
      "mean:  0.3775168980500308\n"
     ]
    }
   ],
   "source": [
    "print('='*30)\n",
    "oof_stack , predictions_stack  = stack_model(oof_lgb , oof_xgb , oof_cat , predictions_lgb , predictions_xgb , predictions_cat , target)\n",
    "print('='*30)\n",
    "oof_nstack, predictions_nstack = stack_model(oof_nlgb, oof_nxgb, oof_ncat, predictions_nlgb, predictions_nxgb, predictions_ncat, ntarget)\n",
    "print('='*30)\n",
    "oof_bstack, predictions_bstack = stack_model(oof_blgb, oof_bxgb, oof_bcat, predictions_blgb, predictions_bxgb, predictions_bcat, target_binary, eval_type='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = predictions_stack\n",
    "sub_df.to_csv('predictions_stack.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/10/gIGwtZskjqmYOCh.png\" alt=\"image-20211210190813208\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 模型 | Private Score | Public Score |\n",
    "| ------ | ------ | ------ |\n",
    "| randomforest | 3.65455 | 3.74969 |\n",
    "| randomforest+validation | 3.65173 | 3.74954 |\n",
    "| LightGBM | 3.69723 | 3.80436 |\n",
    "| LightGBM+validation | 3.64403 | 3.73875 |\n",
    "| XGBoost | 3.62832 | 3.72358 |\n",
    "| Voting_avr | 3.63650 | 3.73251 |\n",
    "| Voting_wei | 3.633307 | 3.72877 |\n",
    "| Stacking | 3.62798 | 3.72055 |\n",
    "| 第二阶段 | 特征优化 | 两阶段建模 |\n",
    "| LightGBM | 3.61177 | 3.68321 |\n",
    "| XGBoost | 3.61048 | 3.68938 |\n",
    "| CatBoost | 3.61154 | 3.69951 |\n",
    "| Voting | 3.60640 | 3.68697 |\n",
    "| Stacking | 3.60683 | 3.68217 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trick融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = predictions_bstack*-33.219281 + (1-predictions_bstack)*predictions_nstack\n",
    "sub_df.to_csv('predictions_trick.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/10/BY7C8IaF2zoASst.png\" alt=\"image-20211210190941459\" style=\"zoom: 50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 模型 | Private Score | Public Score |\n",
    "| ------ | ------ | ------ |\n",
    "| randomforest | 3.65455 | 3.74969 |\n",
    "| randomforest+validation | 3.65173 | 3.74954 |\n",
    "| LightGBM | 3.69723 | 3.80436 |\n",
    "| LightGBM+validation | 3.64403 | 3.73875 |\n",
    "| XGBoost | 3.62832 | 3.72358 |\n",
    "| Voting_avr | 3.63650 | 3.73251 |\n",
    "| Voting_wei | 3.633307 | 3.72877 |\n",
    "| Stacking | 3.62798 | 3.72055 |\n",
    "| 第二阶段 | 特征优化 | 两阶段建模 |\n",
    "| LightGBM | 3.61177 | 3.68321 |\n",
    "| XGBoost | 3.61048 | 3.68938 |\n",
    "| CatBoost | 3.61154 | 3.69951 |\n",
    "| Voting | 3.60640 | 3.68697 |\n",
    "| Stacking | 3.60683 | 3.68217 |\n",
    "| Trick | 3.60232 | 3.67452 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TrickStacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = (predictions_bstack*-33.219281 + (1-predictions_bstack)*predictions_nstack)*0.5 + predictions_stack*0.5\n",
    "sub_df.to_csv('predictions_trick&stacking.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/10/T3VM6OZ9PlkhxLG.png\" alt=\"image-20211210191045474\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 模型 | Private Score | Public Score |\n",
    "| ------ | ------ | ------ |\n",
    "| randomforest | 3.65455 | 3.74969 |\n",
    "| randomforest+validation | 3.65173 | 3.74954 |\n",
    "| LightGBM | 3.69723 | 3.80436 |\n",
    "| LightGBM+validation | 3.64403 | 3.73875 |\n",
    "| XGBoost | 3.62832 | 3.72358 |\n",
    "| Voting_avr | 3.63650 | 3.73251 |\n",
    "| Voting_wei | 3.633307 | 3.72877 |\n",
    "| Stacking | 3.62798 | 3.72055 |\n",
    "| 第二阶段 | 特征优化 | 两阶段建模 |\n",
    "| LightGBM | 3.61177 | 3.68321 |\n",
    "| XGBoost | 3.61048 | 3.68938 |\n",
    "| CatBoost | 3.61154 | 3.69951 |\n",
    "| Voting | 3.60640 | 3.68697 |\n",
    "| Stacking | 3.60683 | 3.68217 |\n",
    "| Trick | 3.60232 | 3.67452 |\n",
    "| Trick+Voting | 3.60206 | 3.67580 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比目前该比赛的榜单，我们能够看到最后的成绩排名，私榜排名如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://tva1.sinaimg.cn/large/008i3skNly1gx8ylfiay1j31f80dstat.jpg\" alt=\"image-20211023185341867\" style=\"zoom:40%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006784589290041192"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28 / 4127"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "排名约在前0.6%，而公榜排名如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://tva1.sinaimg.cn/large/008i3skNly1gx8yll24v2j31g60doq4g.jpg\" alt=\"image-20211023185341867\" style=\"zoom:40%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.033196026169130116"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "137 / 4127"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "约在前3%左右。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 后续优化策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;无论目前结果如何，永远可以有更进一步优化的地方。相信细心的小伙伴已经观察到，本节特征构建和建模策略和此前5天的策略略有差异，如果能融合两部分的方法，则能取得更好的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然，如果对机器学习算法或者机器学习竞赛感兴趣，也欢迎各位小伙伴参与付费课程的学习！《机器学习实战训练营》（第二期）即将开课，十八周80+小时体系大课，由我和菜菜老师主讲，从零开始补充机器学习必备基础，深入讲解集成算法与特征工程方法，并包含四项大型Kaggle案例/企业级应用案例！对机器学习感兴趣的小伙伴扫描下方二维码查看课程详情哦！也可以添加客服VX：little_bird0229咨询课程，还有惊喜优惠券哦~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.loli.net/2021/10/23/wFuDVpfX8bRcKdT.png\" alt=\"image-20211023185341867\" style=\"zoom:40%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.loli.net/2021/10/23/A74GwdQtTah8jcm.png\" alt=\"image-20211023190156083\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>👇扫码查看更多课程详情👇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.loli.net/2021/10/23/FeQxliuA2o6bg7k.png\" alt=\"image-20211023185732319\" style=\"zoom:67%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次公开课圆满结束！接下来有任何想听的内容，也欢迎在我的B站账号下留言~各位小伙伴，大家下次见！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
