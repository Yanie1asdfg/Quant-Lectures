{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4753494c-8f0c-466e-ba38-7b19f02b3889",
   "metadata": {},
   "source": [
    "# <center>《Kaggle Top 1%方案精讲与实践》"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbc2867-20dc-4256-9e91-e4003d987b2d",
   "metadata": {},
   "source": [
    "# <font face=\"仿宋\">课程说明："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139ce0e1-933f-4bc0-a43a-e3ca6e0bcdf5",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">小伙伴好呀\\~欢迎来到《2021机器学习实战训练营》试学体验课！我是课程主讲老师，九天。       \n",
    "&emsp;&emsp;本次体验课为期三天（12月8-10号），期间每晚8点在我的B站直播间公开直播，直播间地址:https://live.bilibili.com/22678166      \n",
    "&emsp;&emsp;本期公开课的内容是在上一轮Kaggle竞赛公开课（《Elo Merchant Category Recommendation》）的基础上，进一步探讨如何进一步将排名提升至1%。在接下来的三天内容中，我们将进一步尝试多模型建模与优化、模型融合方法、特征优化与Trick优化等方法，从而在此前的结果基础上大幅提高模型预测准确率。        \n",
    "&emsp;&emsp;当然，没有参与上一轮公开课的小伙伴也不用担心，本轮公开课将在开始时我们将快速回顾上一轮公开课内容，并将提供上一轮公开课最终完成的数据处理结果与相关代码，帮助大家无门槛进入本轮课程内容的学习中。当然，如果时间允许，也希望大家能够通过观看上一轮公开课的直播录屏，以巩固相关知识。上一轮公开课直播录屏地址：https://www.bilibili.com/video/BV1QU4y1u7Ph      \n",
    "&emsp;&emsp;课程资料/赛题数据/课程代码/付费课程信息，扫码添加客服“小可爱”回复【kaggle】即可领取哦~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebd2b9e-91d6-4144-b682-2525e9a643ac",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.loli.net/2021/10/20/ZWTgxSiNY1db9eL.png\" alt=\"二维码\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063ebbed-1567-482d-be7c-1a05cd67268c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">另外，双十二年终大促持续进行中，十八周80+课时体系大课限时七折，扫码咨询小可爱回复“优惠”，还可领取额外折上折优惠，课程主页：https://appze9inzwc2314.pc.xiaoe-tech.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c335abd-7ea1-4aef-88ef-13f16befc512",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e83d60-0a8e-4824-a9f7-27f733cc48bc",
   "metadata": {},
   "source": [
    "# <center>【Kaggle】Elo Merchant Category Recommendation    \n",
    "# <center> 竞赛案例解析公开课 Part II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9839bd-47d0-4f62-87c2-10a59bd2f5c6",
   "metadata": {},
   "source": [
    "# <center>Day 5.集成学习与模型融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22c10658-bc84-4e0e-b12f-1a4a21fb57d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from hyperopt import hp, fmin, tpe\n",
    "from numpy.random import RandomState\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21a2ac2f-fb5d-4465-9641-ba7e7d0bfaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('preprocess/train.csv')\n",
    "test = pd.read_csv('preprocess/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea161cb6-77cf-49f1-beb5-62ecffdc3156",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576269ff-834e-4fb4-85a4-7092416b081d",
   "metadata": {},
   "source": [
    "## <center> 一、单模训练策略（二）\n",
    "## <center>**Wrapper特征筛选+LightGBM建模+TPE调优**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9be7190-a2bc-402e-959e-8e522e84ea6f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来我们进一步尝试Wrapper特征筛选+LightGBM建模+TPE调优的建模策略。相比随机森林这种老牌集成模型，LightGBM则算是Boosting家族中的新秀，LightGBM源自微软旗下的一个项目（Distributed Machine Learning Toolkit （DMKT）），同样是GBDT的一种实现方式，LightGBM支持高效率的并行训练，并且具有更快的训练速度、更低的内存消耗、更好的准确率、支持分布式可以快速处理海量数据等优点。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04ff7cd-99a1-4931-9615-d542b3c773f4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首次使用LightGBM时需要对其单独进行安装，可以直接使用pip工具按照如下指令完成安装："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ca2f6c-6c31-4f21-9e70-81b468533f67",
   "metadata": {},
   "source": [
    "<center>pip install LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c246316d-3653-430d-8e72-8946cbe41d4a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;安装完即可按照如下方式进行导入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbbc2d97-612a-40f1-a6c2-82ced7f17c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb66793-5ab8-4578-820d-fb81debfc9cd",
   "metadata": {},
   "source": [
    "&emsp;&emsp;此外，本次建模过程中将使用hyperopt优化器进行超参数搜索，hyperopt优化器也是贝叶斯优化器的一种，可以进行连续变量和离散变量的搜索，目前支持的搜索算法包括随机搜索（random search）、模拟退火（simulated annealing）和TPE（Tree of Parzen Estimator）算法，相比网格搜索，hyperopt效率更快、精度更高。首次使用hyperopt库可使用pip进行安装："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6c2cba-5877-4df3-93a5-21dd1dfe23a2",
   "metadata": {},
   "source": [
    "<center>pip install hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2ab935-cab3-4d8a-8889-2da7578d375b",
   "metadata": {},
   "source": [
    "安装完成后按照如下方式进行导入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc3a0351-6f36-40dc-9f73-cd9015864f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df17a8d8-d2e0-4f68-863c-308e6ca7d23a",
   "metadata": {},
   "source": [
    "其中hp是参数空间创建函数，fmin是参数搜索函数，tpe则是一种基于贝叶斯过程的搜索策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eb81de-c712-42e6-a582-c58f4b2a6449",
   "metadata": {},
   "source": [
    "&emsp;&emsp;同时，在本次建模中，我们也将采用wrapper方法进行特征筛选，即根据模型输出结果来进行特征筛选，由于很多时候相关系数并不能很好的衡量特征实际对于标签的重要性，因此wrapper筛选的特征往往更加有效。当然，如果希望我们特征筛选结果更加具有可信度，则可以配合交叉验证过程对其进行筛选。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb9f174-b2e9-4674-8194-a3c2ddacaca3",
   "metadata": {},
   "source": [
    "### 1.Wrapper特征筛选"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3f3e90-d033-4f73-b27f-997a7317974c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来是特征筛选过程，此处先择使用Wrapper方法进行特征筛选，通过带入全部数据训练一个LightGBM模型，然后通过观察特征重要性，选取最重要的300个特征。当然，为了进一步确保挑选过程的有效性，此处我们考虑使用交叉验证的方法来进行多轮验证。实际多轮验证特征重要性的过程也较为清晰，我们只需要记录每一轮特征重要性，并在最后进行简单汇总即可。我们可以通过定义如下函数完成该过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "df280c5e-78dd-4bb4-8b05-8f19749fbbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_select_wrapper(train, test):\n",
    "    \"\"\"\n",
    "    lgm特征重要性筛选函数\n",
    "    :param train:训练数据集\n",
    "    :param test:测试数据集\n",
    "    :return:特征筛选后的训练集和测试集\n",
    "    \"\"\"\n",
    "    \n",
    "    # Part 1.划分特征名称，删除ID列和标签列\n",
    "    print('feature_select_wrapper...')\n",
    "    label = 'target'\n",
    "    features = train.columns.tolist()\n",
    "    features.remove('card_id')\n",
    "    features.remove('target')\n",
    "\n",
    "    # Step 2.配置lgb参数\n",
    "    # 模型参数\n",
    "    params_initial = {\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.1,\n",
    "        'boosting': 'gbdt',\n",
    "        'min_child_samples': 20,\n",
    "        'bagging_seed': 2020,\n",
    "        'bagging_fraction': 0.7,\n",
    "        'bagging_freq': 1,\n",
    "        'feature_fraction': 0.7,\n",
    "        'max_depth': -1,\n",
    "        'metric': 'rmse',\n",
    "        'reg_alpha': 0,\n",
    "        'reg_lambda': 1,\n",
    "        'objective': 'regression'\n",
    "    }\n",
    "    # 控制参数\n",
    "    # 提前验证迭代效果或停止\n",
    "    ESR = 30\n",
    "    # 迭代次数\n",
    "    NBR = 10000\n",
    "    # 打印间隔\n",
    "    VBE = 50\n",
    "    \n",
    "    # Part 3.交叉验证过程\n",
    "    # 实例化评估器\n",
    "    kf = KFold(n_splits=5, random_state=2020, shuffle=True)\n",
    "    # 创建空容器\n",
    "    fse = pd.Series(0, index=features)\n",
    "    \n",
    "    for train_part_index, eval_index in kf.split(train[features], train[label]):\n",
    "        # 封装训练数据集\n",
    "        train_part = lgb.Dataset(train[features].loc[train_part_index],\n",
    "                                 train[label].loc[train_part_index])\n",
    "        # 封装验证数据集\n",
    "        eval = lgb.Dataset(train[features].loc[eval_index],\n",
    "                           train[label].loc[eval_index])\n",
    "        # 在训练集上进行训练，并同时进行验证\n",
    "        bst = lgb.train(params_initial, train_part, num_boost_round=NBR,\n",
    "                        valid_sets=[train_part, eval],\n",
    "                        valid_names=['train', 'valid'],\n",
    "                        early_stopping_rounds=ESR, verbose_eval=VBE)\n",
    "        # 输出特征重要性计算结果，并进行累加\n",
    "        fse += pd.Series(bst.feature_importance(), features)\n",
    "    \n",
    "    # Part 4.选择最重要的300个特征\n",
    "    feature_select = ['card_id'] + fse.sort_values(ascending=False).index.tolist()[:300]\n",
    "    print('done')\n",
    "    return train[feature_select + ['target']], test[feature_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "728c1ac6-9c71-45b8-9894-7e45952aeaae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_select_wrapper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vdmion\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\vdmion\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.676599 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 227016\n",
      "[LightGBM] [Info] Number of data points in the train set: 161533, number of used features: 1626\n",
      "[LightGBM] [Info] Start training from score -0.390986\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\ttrain's rmse: 3.43695\tvalid's rmse: 3.70629\n",
      "Early stopping, best iteration is:\n",
      "[66]\ttrain's rmse: 3.39251\tvalid's rmse: 3.70281\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.583384 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 227122\n",
      "[LightGBM] [Info] Number of data points in the train set: 161533, number of used features: 1629\n",
      "[LightGBM] [Info] Start training from score -0.396781\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\ttrain's rmse: 3.45546\tvalid's rmse: 3.67176\n",
      "[100]\ttrain's rmse: 3.33017\tvalid's rmse: 3.67221\n",
      "Early stopping, best iteration is:\n",
      "[82]\ttrain's rmse: 3.37387\tvalid's rmse: 3.66794\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.677969 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 227089\n",
      "[LightGBM] [Info] Number of data points in the train set: 161534, number of used features: 1631\n",
      "[LightGBM] [Info] Start training from score -0.390348\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\ttrain's rmse: 3.43208\tvalid's rmse: 3.72216\n",
      "Early stopping, best iteration is:\n",
      "[68]\ttrain's rmse: 3.38524\tvalid's rmse: 3.71798\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.687552 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 227023\n",
      "[LightGBM] [Info] Number of data points in the train set: 161534, number of used features: 1624\n",
      "[LightGBM] [Info] Start training from score -0.391392\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\ttrain's rmse: 3.42022\tvalid's rmse: 3.7901\n",
      "Early stopping, best iteration is:\n",
      "[59]\ttrain's rmse: 3.39133\tvalid's rmse: 3.78884\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.689366 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 227023\n",
      "[LightGBM] [Info] Number of data points in the train set: 161534, number of used features: 1625\n",
      "[LightGBM] [Info] Start training from score -0.398675\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\ttrain's rmse: 3.46321\tvalid's rmse: 3.60039\n",
      "Early stopping, best iteration is:\n",
      "[61]\ttrain's rmse: 3.43002\tvalid's rmse: 3.59887\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "train_LGBM, test_LGBM = feature_select_wrapper(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b64ead-cbfd-4523-b734-a39670d8ccbe",
   "metadata": {},
   "source": [
    "查看最终输出结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e937669-3971-44b5-ab12-c8ec9a02ee6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201917, 302)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_LGBM.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2eadd4-702c-409e-bfee-b563b42ce845",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，我们即可带入经过筛选的特征进行建模。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bc9e12-18d6-4eaf-9e08-22a1e3745cb0",
   "metadata": {},
   "source": [
    "### 2.LightGBM模型训练与TPE参数优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d33bc58-7767-4561-9fa4-4a338cef632c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，我们进行LightGBM的模型训练过程，和此前的随机森林建模过程类似，我们需要在训练模型的过程同时进行超参数的搜索调优。为了能够更好的借助hyperopt进行超参数搜索，此处我们考虑使用LightGBM的原生算法库进行建模，并将整个算法建模流程封装在若干个函数  内执行。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea898113-91ec-4d1f-8cc6-1714b4fd006d",
   "metadata": {},
   "source": [
    "- 参数回调函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0597a8fa-35fd-42b5-bc94-dc4130f1a915",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先对于lgb模型来说，并不是所有的超参数都需要进行搜索，为了防止多次实例化模型过程中部分超参数被设置成默认参数，此处我们首先需要创建一个参数回调函数，用于在后续多次实例化模型过程中反复申明这部分参数的固定取值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cf8503b-3f9c-440d-a30a-90a956b3fa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_append(params):\n",
    "    \"\"\"\n",
    "    动态回调参数函数，params视作字典\n",
    "    :param params:lgb参数字典\n",
    "    :return params:修正后的lgb参数字典\n",
    "    \"\"\"\n",
    "    params['feature_pre_filter'] = False\n",
    "    params['objective'] = 'regression'\n",
    "    params['metric'] = 'rmse'\n",
    "    params['bagging_seed'] = 2020\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38198834-3eb4-4561-b3c6-464b541dd6b0",
   "metadata": {},
   "source": [
    "- 模型训练与参数优化函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6524a82-2e19-4a80-8a3a-4f7225896ca3",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来就是更加复杂的模型训练与超参数调优的的过程。不同于sklearn内部的调参过程，此处由于涉及多个不同的库相互协同，外加本身lgb模型参数就较为复杂，因此整体模型训练与优化过程较为复杂，我们可以通过下述函数来执行该过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "102ef0a9-ce3e-4c75-a6e2-dfa02b3cc5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_hyperopt(train):\n",
    "    \"\"\"\n",
    "    模型参数搜索与优化函数\n",
    "    :param train:训练数据集\n",
    "    :return params_best:lgb最优参数\n",
    "    \"\"\"\n",
    "    # Part 1.划分特征名称，删除ID列和标签列\n",
    "    label = 'target'\n",
    "    features = train.columns.tolist()\n",
    "    features.remove('card_id')\n",
    "    features.remove('target')\n",
    "    \n",
    "    # Part 2.封装训练数据\n",
    "    train_data = lgb.Dataset(train[features], train[label])\n",
    "    \n",
    "    # Part 3.内部函数，输入模型超参数损失值输出函数\n",
    "    def hyperopt_objective(params):\n",
    "        \"\"\"\n",
    "        输入超参数，输出对应损失值\n",
    "        :param params:\n",
    "        :return:最小rmse\n",
    "        \"\"\"\n",
    "        # 创建参数集\n",
    "        params = params_append(params)\n",
    "        print(params)\n",
    "        \n",
    "        # 借助lgb的cv过程，输出某一组超参数下损失值的最小值\n",
    "        res = lgb.cv(params, train_data, 1000,\n",
    "                     nfold=2,\n",
    "                     stratified=False,\n",
    "                     shuffle=True,\n",
    "                     metrics='rmse',\n",
    "                     early_stopping_rounds=20,\n",
    "                     verbose_eval=False,\n",
    "                     show_stdv=False,\n",
    "                     seed=2020)\n",
    "        return min(res['rmse-mean']) # res是个字典\n",
    "\n",
    "    # Part 4.lgb超参数空间\n",
    "    params_space = {\n",
    "        'learning_rate': hp.uniform('learning_rate', 1e-2, 5e-1),\n",
    "        'bagging_fraction': hp.uniform('bagging_fraction', 0.5, 1),\n",
    "        'feature_fraction': hp.uniform('feature_fraction', 0.5, 1),\n",
    "        'num_leaves': hp.choice('num_leaves', list(range(10, 300, 10))),\n",
    "        'reg_alpha': hp.randint('reg_alpha', 0, 10),\n",
    "        'reg_lambda': hp.uniform('reg_lambda', 0, 10),\n",
    "        'bagging_freq': hp.randint('bagging_freq', 1, 10),\n",
    "        'min_child_samples': hp.choice('min_child_samples', list(range(1, 30, 5)))\n",
    "    }\n",
    "    \n",
    "    # Part 5.TPE超参数搜索\n",
    "    params_best = fmin(\n",
    "        hyperopt_objective,\n",
    "        space=params_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=30,\n",
    "        rstate=RandomState(2020))\n",
    "    \n",
    "    # 返回最佳参数\n",
    "    return params_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918eb7a9-00c7-46c7-9fca-90caf656b42b",
   "metadata": {},
   "source": [
    "接下来我们带入训练数据，测试函数性能："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c860c2d6-9e32-4a4e-939b-a1eca83cab6d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bagging_fraction': 0.7253952770621912, 'bagging_freq': 5, 'feature_fraction': 0.6972128940985931, 'learning_rate': 0.43437628238508774, 'min_child_samples': 6, 'num_leaves': 60, 'reg_alpha': 0, 'reg_lambda': 1.6139256132729207, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "  0%|          | 0/30 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vdmion\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:577: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.183863 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.188092 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931 \n",
      "[LightGBM] [Info] Start training from score -0.390344 \n",
      "  0%|          | 0/30 [00:02<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vdmion\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:620: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bagging_fraction': 0.557619162794617, 'bagging_freq': 6, 'feature_fraction': 0.768520768296847, 'learning_rate': 0.4484899481964635, 'min_child_samples': 1, 'num_leaves': 250, 'reg_alpha': 1, 'reg_lambda': 1.9478998979854978, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.222429 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                             \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.217881 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                             \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                          \n",
      "[LightGBM] [Info] Start training from score -0.390344                          \n",
      "{'bagging_fraction': 0.6089577218903197, 'bagging_freq': 8, 'feature_fraction': 0.9078041789694723, 'learning_rate': 0.4000654698261548, 'min_child_samples': 26, 'num_leaves': 60, 'reg_alpha': 4, 'reg_lambda': 9.739071431781486, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.219708 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                             \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.223190 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                             \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                          \n",
      "[LightGBM] [Info] Start training from score -0.390344                          \n",
      "{'bagging_fraction': 0.5823692068559665, 'bagging_freq': 1, 'feature_fraction': 0.9863236467851518, 'learning_rate': 0.44826974213381765, 'min_child_samples': 6, 'num_leaves': 190, 'reg_alpha': 8, 'reg_lambda': 6.424961074528348, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.225601 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.192807 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                           \n",
      "[LightGBM] [Info] Start training from score -0.390344                           \n",
      "{'bagging_fraction': 0.598225211619325, 'bagging_freq': 4, 'feature_fraction': 0.5548282810909009, 'learning_rate': 0.33699901703548696, 'min_child_samples': 6, 'num_leaves': 100, 'reg_alpha': 5, 'reg_lambda': 8.56187185369514, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.217945 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.213833 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                           \n",
      "[LightGBM] [Info] Start training from score -0.390344                           \n",
      "{'bagging_fraction': 0.9653168636499914, 'bagging_freq': 2, 'feature_fraction': 0.7958338878675355, 'learning_rate': 0.49191469387124415, 'min_child_samples': 6, 'num_leaves': 120, 'reg_alpha': 0, 'reg_lambda': 6.281210819853246, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.222392 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.227109 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                           \n",
      "[LightGBM] [Info] Start training from score -0.390344                           \n",
      "{'bagging_fraction': 0.9746881685291604, 'bagging_freq': 5, 'feature_fraction': 0.6646803214513162, 'learning_rate': 0.07529481190470996, 'min_child_samples': 1, 'num_leaves': 270, 'reg_alpha': 8, 'reg_lambda': 5.37208795616958, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.225851 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.219836 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                           \n",
      "[LightGBM] [Info] Start training from score -0.390344                           \n",
      "{'bagging_fraction': 0.9594369823462314, 'bagging_freq': 1, 'feature_fraction': 0.5205458529613629, 'learning_rate': 0.10394361538901523, 'min_child_samples': 11, 'num_leaves': 180, 'reg_alpha': 9, 'reg_lambda': 8.54732560156507, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.208976 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.208101 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                           \n",
      "[LightGBM] [Info] Start training from score -0.390344                           \n",
      "{'bagging_fraction': 0.5239402528699113, 'bagging_freq': 4, 'feature_fraction': 0.7242087970134551, 'learning_rate': 0.1388688694778799, 'min_child_samples': 16, 'num_leaves': 120, 'reg_alpha': 0, 'reg_lambda': 7.4552808665141175, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.222443 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.227098 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                           \n",
      "[LightGBM] [Info] Start training from score -0.390344                           \n",
      "{'bagging_fraction': 0.7002922718759523, 'bagging_freq': 1, 'feature_fraction': 0.6995945996396362, 'learning_rate': 0.13683006962337202, 'min_child_samples': 1, 'num_leaves': 170, 'reg_alpha': 8, 'reg_lambda': 9.573241379549104, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221007 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.223261 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                           \n",
      "[LightGBM] [Info] Start training from score -0.390344                           \n",
      "{'bagging_fraction': 0.9740731891987577, 'bagging_freq': 9, 'feature_fraction': 0.8294212073704622, 'learning_rate': 0.4788786165054211, 'min_child_samples': 21, 'num_leaves': 180, 'reg_alpha': 3, 'reg_lambda': 9.025378633668943, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221019 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.189511 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.6595660453511591, 'bagging_freq': 3, 'feature_fraction': 0.7067334738059636, 'learning_rate': 0.03476014344160344, 'min_child_samples': 6, 'num_leaves': 280, 'reg_alpha': 7, 'reg_lambda': 1.5530335205049572, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.223196 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.224191 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.6633836872563099, 'bagging_freq': 6, 'feature_fraction': 0.5393971364268482, 'learning_rate': 0.4475746464086467, 'min_child_samples': 21, 'num_leaves': 240, 'reg_alpha': 5, 'reg_lambda': 9.96781356536865, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.213618 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.210733 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                           \n",
      "[LightGBM] [Info] Start training from score -0.390344                           \n",
      "{'bagging_fraction': 0.8489543397224686, 'bagging_freq': 7, 'feature_fraction': 0.7582778265282211, 'learning_rate': 0.2222331623940236, 'min_child_samples': 21, 'num_leaves': 120, 'reg_alpha': 7, 'reg_lambda': 9.191685626283043, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.219768 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.218532 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                           \n",
      "[LightGBM] [Info] Start training from score -0.390344                           \n",
      "{'bagging_fraction': 0.8489434703946281, 'bagging_freq': 5, 'feature_fraction': 0.8206802208124425, 'learning_rate': 0.04617153690285374, 'min_child_samples': 26, 'num_leaves': 270, 'reg_alpha': 0, 'reg_lambda': 6.1154939604145255, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.217947 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.227017 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                           \n",
      "[LightGBM] [Info] Start training from score -0.390344                           \n",
      "{'bagging_fraction': 0.5722064005547208, 'bagging_freq': 9, 'feature_fraction': 0.6351076955262891, 'learning_rate': 0.24123365491719742, 'min_child_samples': 16, 'num_leaves': 220, 'reg_alpha': 4, 'reg_lambda': 5.435241604857383, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.220000 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.191012 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.876943709541165, 'bagging_freq': 1, 'feature_fraction': 0.7520250111432751, 'learning_rate': 0.27978139134976193, 'min_child_samples': 16, 'num_leaves': 250, 'reg_alpha': 0, 'reg_lambda': 9.889444738399005, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.190174 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.180522 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.7292457974834843, 'bagging_freq': 1, 'feature_fraction': 0.5605178583746184, 'learning_rate': 0.0750382696349759, 'min_child_samples': 26, 'num_leaves': 30, 'reg_alpha': 3, 'reg_lambda': 8.434389500726724, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.178226 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.211947 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.7623620409611478, 'bagging_freq': 5, 'feature_fraction': 0.9037817698710914, 'learning_rate': 0.11935511419050072, 'min_child_samples': 21, 'num_leaves': 50, 'reg_alpha': 6, 'reg_lambda': 9.525599018301525, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.227502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.229222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.6959365208552413, 'bagging_freq': 4, 'feature_fraction': 0.8682613864600894, 'learning_rate': 0.3540190069136353, 'min_child_samples': 11, 'num_leaves': 70, 'reg_alpha': 8, 'reg_lambda': 1.8364446123307276, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.223646 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.222851 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.7790707500769768, 'bagging_freq': 5, 'feature_fraction': 0.9823224422761904, 'learning_rate': 0.18097980740543457, 'min_child_samples': 26, 'num_leaves': 50, 'reg_alpha': 6, 'reg_lambda': 3.70583513118315, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.191651 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221642 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.7657229397838268, 'bagging_freq': 3, 'feature_fraction': 0.6048086694416244, 'learning_rate': 0.18083227043013056, 'min_child_samples': 21, 'num_leaves': 30, 'reg_alpha': 3, 'reg_lambda': 7.765420130465819, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.190535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.223369 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.7941491322120129, 'bagging_freq': 2, 'feature_fraction': 0.9314060907023993, 'learning_rate': 0.02215568922918173, 'min_child_samples': 26, 'num_leaves': 80, 'reg_alpha': 2, 'reg_lambda': 3.921799580171095, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.224339 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.184048 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.8156411479061829, 'bagging_freq': 2, 'feature_fraction': 0.9453575093796732, 'learning_rate': 0.015016007831064087, 'min_child_samples': 26, 'num_leaves': 80, 'reg_alpha': 2, 'reg_lambda': 3.6758012630853103, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.190266 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.224118 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.9022336069269954, 'bagging_freq': 2, 'feature_fraction': 0.9373662317255621, 'learning_rate': 0.014947332175194025, 'min_child_samples': 26, 'num_leaves': 80, 'reg_alpha': 2, 'reg_lambda': 3.5907566887206896, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.225978 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.223914 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.9092910898548142, 'bagging_freq': 2, 'feature_fraction': 0.9587253380203777, 'learning_rate': 0.01594569445481636, 'min_child_samples': 26, 'num_leaves': 80, 'reg_alpha': 2, 'reg_lambda': 0.5437059106621431, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.220067 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.222246 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.9243731638563496, 'bagging_freq': 2, 'feature_fraction': 0.8579304338996762, 'learning_rate': 0.07045436429758375, 'min_child_samples': 26, 'num_leaves': 10, 'reg_alpha': 2, 'reg_lambda': 3.6181498300907533, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.186302 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.224051 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.8208518394986082, 'bagging_freq': 2, 'feature_fraction': 0.9942064788262652, 'learning_rate': 0.17680977265314285, 'min_child_samples': 26, 'num_leaves': 230, 'reg_alpha': 2, 'reg_lambda': 0.014934884031477136, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221006 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.228092 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.9087334379512036, 'bagging_freq': 8, 'feature_fraction': 0.9435725455149249, 'learning_rate': 0.28640982898261946, 'min_child_samples': 26, 'num_leaves': 130, 'reg_alpha': 2, 'reg_lambda': 2.818017545083271, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.192196 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.217484 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.8733261239697325, 'bagging_freq': 2, 'feature_fraction': 0.8775960565074609, 'learning_rate': 0.011582875040907454, 'min_child_samples': 11, 'num_leaves': 210, 'reg_alpha': 9, 'reg_lambda': 4.553491961304738, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.222999 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.220148 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "100%|██████████| 30/30 [02:51<00:00,  5.71s/trial, best loss: 3.6844192003528775]\n"
     ]
    }
   ],
   "source": [
    "best_clf = param_hyperopt(train_LGBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2825b4e5-02e6-46b2-978b-b979b57f9c58",
   "metadata": {},
   "source": [
    "此时best_clf即为lgb模型的最优参数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db06186c-6815-4fe9-8920-ceb79ebebe4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.9022336069269954,\n",
       " 'bagging_freq': 2,\n",
       " 'feature_fraction': 0.9373662317255621,\n",
       " 'learning_rate': 0.014947332175194025,\n",
       " 'min_child_samples': 5,\n",
       " 'num_leaves': 7,\n",
       " 'reg_alpha': 2,\n",
       " 'reg_lambda': 3.5907566887206896}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3fb4ad-5ac7-4230-8224-80cdd1a63a77",
   "metadata": {},
   "source": [
    "### 3.LightGBM模型预测与结果排名"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa124f63-2cbf-46f6-9d67-3170c8ccaa58",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在搜索出最优参数后，接下来即可进行模型预测了。和此前一样，在实际执行预测时有两种思路，其一是单模型预测，即直接针对测试集进行预测并提交结果，其二则是通过交叉验证提交平均得分，并且在此过程中能同时保留下后续用于stacking集成时所需要用到的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ce9b46-a538-4716-a3b9-dd0cc86624eb",
   "metadata": {},
   "source": [
    "- 单模型预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad69f435-057c-40c1-b7af-e89c274c13e0",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先测试单独模型在测试集上的预测效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "326f09dd-7323-4766-9311-2b2356bf7b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再次申明固定参数\n",
    "best_clf = params_append(best_clf)\n",
    "\n",
    "# 数据准备过程\n",
    "label = 'target'\n",
    "features = train_LGBM.columns.tolist()\n",
    "features.remove('card_id')\n",
    "features.remove('target')\n",
    "\n",
    "# 数据封装\n",
    "lgb_train = lgb.Dataset(train_LGBM[features], train_LGBM[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3139fe5a-3b82-4ff2-b131-0dcc2d73d6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.426494 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975\n",
      "[LightGBM] [Info] Number of data points in the train set: 201917, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.393636\n"
     ]
    }
   ],
   "source": [
    "# 在全部数据集上训练模型\n",
    "bst = lgb.train(best_clf, lgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6fe1824-10e0-4b1c-a24f-9bf6a877b5d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.24511938, -2.01595283,  0.1053809 , ..., -0.18190679,\n",
       "       -1.11870804, -0.24511938])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在测试集上完成预测\n",
    "bst.predict(train_LGBM[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d8c3f25-eb32-4a0d-b870-570085e0279a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7213768397255365"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 简单查看训练集RMSE\n",
    "np.sqrt(mean_squared_error(train_LGBM[label], bst.predict(train_LGBM[features])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a351566-bf6f-4bfe-bb08-2b1b3f722ac9",
   "metadata": {},
   "source": [
    "接下来，对测试集进行预测，并将结果写入本地文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93884a9c-386e-43d9-b212-688a014202db",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-a18f256f8f74>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_LGBM['target'] = bst.predict(test_LGBM[features])\n"
     ]
    }
   ],
   "source": [
    "test_LGBM['target'] = bst.predict(test_LGBM[features])\n",
    "test_LGBM[['card_id', 'target']].to_csv(\"result/submission_LGBM.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30950193-4511-4003-88e4-9a4e0aeaa2d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C_ID_0ab67a22ab</td>\n",
       "      <td>-2.418856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C_ID_130fd0cbdd</td>\n",
       "      <td>-0.752626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C_ID_b709037bc5</td>\n",
       "      <td>-0.030933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C_ID_d27d835a9f</td>\n",
       "      <td>-0.245119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C_ID_2b5e3df5c2</td>\n",
       "      <td>-0.366990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           card_id    target\n",
       "0  C_ID_0ab67a22ab -2.418856\n",
       "1  C_ID_130fd0cbdd -0.752626\n",
       "2  C_ID_b709037bc5 -0.030933\n",
       "3  C_ID_d27d835a9f -0.245119\n",
       "4  C_ID_2b5e3df5c2 -0.366990"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_LGBM[['card_id', 'target']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6532f39-3c0c-479e-8d97-cb6b19ae9f40",
   "metadata": {},
   "source": [
    "提交该结果，得到公榜、私榜结果如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2178c6-cb0c-4280-96ed-5dfea768861c",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/09/GowrHnvJWMOpxB4.png\" alt=\"image-20211209172112868\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ad4e65-0f6b-4302-a0aa-e72bad389c4f",
   "metadata": {},
   "source": [
    "对比此前的随机森林提交的两组结果，汇总情况如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee7ff30-dd49-4d2b-91e6-9f33a55d1a2d",
   "metadata": {},
   "source": [
    "| 模型 | Private Score | Public Score |\n",
    "| ------ | ------ | ------ |\n",
    "| randomforest | 3.65455 | 3.74969 |\n",
    "| randomforest+validation | 3.65173 | 3.74954 |\n",
    "| LightGBM | 3.69723 | 3.80436 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b092449-d7ac-446a-a69e-c5048ac3dd02",
   "metadata": {},
   "source": [
    "能够发现，在单模型预测情况下，lgb要略弱于rf，接下来考虑进行交叉验证，以提高lgb模型预测效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e369804-2018-47bf-862b-786a4166a8a3",
   "metadata": {},
   "source": [
    "- 结合交叉验证进行模型预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34fb7c6-ca15-4595-b803-8be7e0ffa90d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;和随机森林借助交叉验证进行模型预测的过程类似，lgb也需要遵照如下流程进行训练和预测，并同时创建后续集成所需数据集以及预测结果的平均值（作为最终预测结果）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736cd447-b38e-4bb8-b057-9caa8b2a5a20",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/08/ALF3cfuSwmB7b8z.png\" alt=\"image-20211208192640281\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336cca17-ff2e-41a5-b60d-632eb798bdd0",
   "metadata": {},
   "source": [
    "执行过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a099a86f-f013-4d39-bd37-e4802e1e6342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(train, test, params):\n",
    "    \"\"\"\n",
    "\n",
    "    :param train:\n",
    "    :param test:\n",
    "    :param params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Part 1.选择特征\n",
    "    label = 'target'\n",
    "    features = train.columns.tolist()\n",
    "    features.remove('card_id')\n",
    "    features.remove('target')\n",
    "    \n",
    "    # Part 2.再次申明固定参数与控制迭代参数\n",
    "    params = params_append(params)\n",
    "    ESR = 30\n",
    "    NBR = 10000\n",
    "    VBE = 50\n",
    "    \n",
    "    # Part 3.创建结果存储容器\n",
    "    # 测试集预测结果存储器，后保存至本地文件\n",
    "    prediction_test = 0\n",
    "    # 验证集的模型表现，作为展示用\n",
    "    cv_score = []\n",
    "    # 验证集的预测结果存储器，后保存至本地文件\n",
    "    prediction_train = pd.Series()\n",
    "    \n",
    "    # Part 3.交叉验证\n",
    "    kf = KFold(n_splits=5, random_state=2020, shuffle=True)\n",
    "    for train_part_index, eval_index in kf.split(train[features], train[label]):\n",
    "        # 训练数据封装\n",
    "        train_part = lgb.Dataset(train[features].loc[train_part_index],\n",
    "                                 train[label].loc[train_part_index])\n",
    "        # 测试数据封装\n",
    "        eval = lgb.Dataset(train[features].loc[eval_index],\n",
    "                           train[label].loc[eval_index])\n",
    "        # 依据验证集训练模型\n",
    "        bst = lgb.train(params, train_part, num_boost_round=NBR,\n",
    "                        valid_sets=[train_part, eval],\n",
    "                        valid_names=['train', 'valid'],\n",
    "                        early_stopping_rounds=ESR, verbose_eval=VBE)\n",
    "        # 测试集预测结果并纳入prediction_test容器\n",
    "        prediction_test += bst.predict(test[features])\n",
    "        # 验证集预测结果并纳入prediction_train容器\n",
    "        prediction_train = prediction_train.append(pd.Series(bst.predict(train[features].loc[eval_index]),\n",
    "                                                             index=eval_index))\n",
    "        # 验证集预测结果\n",
    "        eval_pre = bst.predict(train[features].loc[eval_index])\n",
    "        # 计算验证集上得分\n",
    "        score = np.sqrt(mean_squared_error(train[label].loc[eval_index].values, eval_pre))\n",
    "        # 纳入cv_score容器\n",
    "        cv_score.append(score)\n",
    "        \n",
    "    # Part 4.打印/输出结果\n",
    "    # 打印验证集得分与平均得分\n",
    "    print(cv_score, sum(cv_score) / 5)\n",
    "    # 将验证集上预测结果写入本地文件\n",
    "    pd.Series(prediction_train.sort_index().values).to_csv(\"preprocess/train_lightgbm.csv\", index=False)\n",
    "    # 将测试集上预测结果写入本地文件\n",
    "    pd.Series(prediction_test / 5).to_csv(\"preprocess/test_lightgbm.csv\", index=False)\n",
    "    # 测试集平均得分作为模型最终预测结果\n",
    "    test['target'] = prediction_test / 5\n",
    "    # 将测试集预测结果写成竞赛要求格式并保存至本地\n",
    "    test[['card_id', 'target']].to_csv(\"result/submission_lightgbm.csv\", index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3fad7d3-0153-47c0-90ae-9d2eb1de06a2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_select_wrapper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vdmion\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\vdmion\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.676517 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 227016\n",
      "[LightGBM] [Info] Number of data points in the train set: 161533, number of used features: 1626\n",
      "[LightGBM] [Info] Start training from score -0.390986\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\ttrain's rmse: 3.43695\tvalid's rmse: 3.70629\n",
      "Early stopping, best iteration is:\n",
      "[66]\ttrain's rmse: 3.39251\tvalid's rmse: 3.70281\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.595659 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 227122\n",
      "[LightGBM] [Info] Number of data points in the train set: 161533, number of used features: 1629\n",
      "[LightGBM] [Info] Start training from score -0.396781\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\ttrain's rmse: 3.45546\tvalid's rmse: 3.67176\n",
      "[100]\ttrain's rmse: 3.33017\tvalid's rmse: 3.67221\n",
      "Early stopping, best iteration is:\n",
      "[82]\ttrain's rmse: 3.37387\tvalid's rmse: 3.66794\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.572918 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 227089\n",
      "[LightGBM] [Info] Number of data points in the train set: 161534, number of used features: 1631\n",
      "[LightGBM] [Info] Start training from score -0.390348\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\ttrain's rmse: 3.43208\tvalid's rmse: 3.72216\n",
      "Early stopping, best iteration is:\n",
      "[68]\ttrain's rmse: 3.38524\tvalid's rmse: 3.71798\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.694756 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 227023\n",
      "[LightGBM] [Info] Number of data points in the train set: 161534, number of used features: 1624\n",
      "[LightGBM] [Info] Start training from score -0.391392\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\ttrain's rmse: 3.42022\tvalid's rmse: 3.7901\n",
      "Early stopping, best iteration is:\n",
      "[59]\ttrain's rmse: 3.39133\tvalid's rmse: 3.78884\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.567025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 227023\n",
      "[LightGBM] [Info] Number of data points in the train set: 161534, number of used features: 1625\n",
      "[LightGBM] [Info] Start training from score -0.398675\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\ttrain's rmse: 3.46321\tvalid's rmse: 3.60039\n",
      "Early stopping, best iteration is:\n",
      "[61]\ttrain's rmse: 3.43002\tvalid's rmse: 3.59887\n",
      "done\n",
      "{'bagging_fraction': 0.7253952770621912, 'bagging_freq': 5, 'feature_fraction': 0.6972128940985931, 'learning_rate': 0.43437628238508774, 'min_child_samples': 6, 'num_leaves': 60, 'reg_alpha': 0, 'reg_lambda': 1.6139256132729207, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "  0%|          | 0/30 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vdmion\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:577: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.224564 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.223605 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931 \n",
      "[LightGBM] [Info] Start training from score -0.390344 \n",
      "  0%|          | 0/30 [00:02<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vdmion\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:620: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bagging_fraction': 0.557619162794617, 'bagging_freq': 6, 'feature_fraction': 0.768520768296847, 'learning_rate': 0.4484899481964635, 'min_child_samples': 1, 'num_leaves': 250, 'reg_alpha': 1, 'reg_lambda': 1.9478998979854978, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.216744 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                             \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.184350 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                             \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                          \n",
      "[LightGBM] [Info] Start training from score -0.390344                          \n",
      "{'bagging_fraction': 0.6089577218903197, 'bagging_freq': 8, 'feature_fraction': 0.9078041789694723, 'learning_rate': 0.4000654698261548, 'min_child_samples': 26, 'num_leaves': 60, 'reg_alpha': 4, 'reg_lambda': 9.739071431781486, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221681 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                             \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.226362 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                             \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                          \n",
      "[LightGBM] [Info] Start training from score -0.390344                          \n",
      "{'bagging_fraction': 0.5823692068559665, 'bagging_freq': 1, 'feature_fraction': 0.9863236467851518, 'learning_rate': 0.44826974213381765, 'min_child_samples': 6, 'num_leaves': 190, 'reg_alpha': 8, 'reg_lambda': 6.424961074528348, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.225456 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.226152 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                           \n",
      "[LightGBM] [Info] Start training from score -0.390344                           \n",
      "{'bagging_fraction': 0.598225211619325, 'bagging_freq': 4, 'feature_fraction': 0.5548282810909009, 'learning_rate': 0.33699901703548696, 'min_child_samples': 6, 'num_leaves': 100, 'reg_alpha': 5, 'reg_lambda': 8.56187185369514, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.178458 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.217750 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                           \n",
      "[LightGBM] [Info] Start training from score -0.390344                           \n",
      "{'bagging_fraction': 0.9653168636499914, 'bagging_freq': 2, 'feature_fraction': 0.7958338878675355, 'learning_rate': 0.49191469387124415, 'min_child_samples': 6, 'num_leaves': 120, 'reg_alpha': 0, 'reg_lambda': 6.281210819853246, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187187 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.223091 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                           \n",
      "[LightGBM] [Info] Start training from score -0.390344                           \n",
      "{'bagging_fraction': 0.9746881685291604, 'bagging_freq': 5, 'feature_fraction': 0.6646803214513162, 'learning_rate': 0.07529481190470996, 'min_child_samples': 1, 'num_leaves': 270, 'reg_alpha': 8, 'reg_lambda': 5.37208795616958, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.220083 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187460 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                           \n",
      "[LightGBM] [Info] Start training from score -0.390344                           \n",
      "{'bagging_fraction': 0.9594369823462314, 'bagging_freq': 1, 'feature_fraction': 0.5205458529613629, 'learning_rate': 0.10394361538901523, 'min_child_samples': 11, 'num_leaves': 180, 'reg_alpha': 9, 'reg_lambda': 8.54732560156507, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.184174 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.217248 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                           \n",
      "[LightGBM] [Info] Start training from score -0.390344                           \n",
      "{'bagging_fraction': 0.5239402528699113, 'bagging_freq': 4, 'feature_fraction': 0.7242087970134551, 'learning_rate': 0.1388688694778799, 'min_child_samples': 16, 'num_leaves': 120, 'reg_alpha': 0, 'reg_lambda': 7.4552808665141175, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.223045 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.191535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                           \n",
      "[LightGBM] [Info] Start training from score -0.390344                           \n",
      "{'bagging_fraction': 0.7002922718759523, 'bagging_freq': 1, 'feature_fraction': 0.6995945996396362, 'learning_rate': 0.13683006962337202, 'min_child_samples': 1, 'num_leaves': 170, 'reg_alpha': 8, 'reg_lambda': 9.573241379549104, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.215911 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.224025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                           \n",
      "[LightGBM] [Info] Start training from score -0.390344                           \n",
      "{'bagging_fraction': 0.9740731891987577, 'bagging_freq': 9, 'feature_fraction': 0.8294212073704622, 'learning_rate': 0.4788786165054211, 'min_child_samples': 21, 'num_leaves': 180, 'reg_alpha': 3, 'reg_lambda': 9.025378633668943, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.196117 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.227535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.6595660453511591, 'bagging_freq': 3, 'feature_fraction': 0.7067334738059636, 'learning_rate': 0.03476014344160344, 'min_child_samples': 6, 'num_leaves': 280, 'reg_alpha': 7, 'reg_lambda': 1.5530335205049572, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.224923 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.222820 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.6633836872563099, 'bagging_freq': 6, 'feature_fraction': 0.5393971364268482, 'learning_rate': 0.4475746464086467, 'min_child_samples': 21, 'num_leaves': 240, 'reg_alpha': 5, 'reg_lambda': 9.96781356536865, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.215749 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.217729 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                           \n",
      "[LightGBM] [Info] Start training from score -0.390344                           \n",
      "{'bagging_fraction': 0.8489543397224686, 'bagging_freq': 7, 'feature_fraction': 0.7582778265282211, 'learning_rate': 0.2222331623940236, 'min_child_samples': 21, 'num_leaves': 120, 'reg_alpha': 7, 'reg_lambda': 9.191685626283043, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.224593 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.225676 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                           \n",
      "[LightGBM] [Info] Start training from score -0.390344                           \n",
      "{'bagging_fraction': 0.8489434703946281, 'bagging_freq': 5, 'feature_fraction': 0.8206802208124425, 'learning_rate': 0.04617153690285374, 'min_child_samples': 26, 'num_leaves': 270, 'reg_alpha': 0, 'reg_lambda': 6.1154939604145255, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.191245 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.225752 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                           \n",
      "[LightGBM] [Info] Start training from score -0.390344                           \n",
      "{'bagging_fraction': 0.5722064005547208, 'bagging_freq': 9, 'feature_fraction': 0.6351076955262891, 'learning_rate': 0.24123365491719742, 'min_child_samples': 16, 'num_leaves': 220, 'reg_alpha': 4, 'reg_lambda': 5.435241604857383, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.224731 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.227394 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.876943709541165, 'bagging_freq': 1, 'feature_fraction': 0.7520250111432751, 'learning_rate': 0.27978139134976193, 'min_child_samples': 16, 'num_leaves': 250, 'reg_alpha': 0, 'reg_lambda': 9.889444738399005, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.191765 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.220467 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.7292457974834843, 'bagging_freq': 1, 'feature_fraction': 0.5605178583746184, 'learning_rate': 0.0750382696349759, 'min_child_samples': 26, 'num_leaves': 30, 'reg_alpha': 3, 'reg_lambda': 8.434389500726724, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.214009 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.212526 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.7623620409611478, 'bagging_freq': 5, 'feature_fraction': 0.9037817698710914, 'learning_rate': 0.11935511419050072, 'min_child_samples': 21, 'num_leaves': 50, 'reg_alpha': 6, 'reg_lambda': 9.525599018301525, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.226875 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.223876 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.6959365208552413, 'bagging_freq': 4, 'feature_fraction': 0.8682613864600894, 'learning_rate': 0.3540190069136353, 'min_child_samples': 11, 'num_leaves': 70, 'reg_alpha': 8, 'reg_lambda': 1.8364446123307276, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.223655 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.220512 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.7790707500769768, 'bagging_freq': 5, 'feature_fraction': 0.9823224422761904, 'learning_rate': 0.18097980740543457, 'min_child_samples': 26, 'num_leaves': 50, 'reg_alpha': 6, 'reg_lambda': 3.70583513118315, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.226858 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.223562 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.7657229397838268, 'bagging_freq': 3, 'feature_fraction': 0.6048086694416244, 'learning_rate': 0.18083227043013056, 'min_child_samples': 21, 'num_leaves': 30, 'reg_alpha': 3, 'reg_lambda': 7.765420130465819, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.226953 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.217789 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.7941491322120129, 'bagging_freq': 2, 'feature_fraction': 0.9314060907023993, 'learning_rate': 0.02215568922918173, 'min_child_samples': 26, 'num_leaves': 80, 'reg_alpha': 2, 'reg_lambda': 3.921799580171095, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.223357 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.224412 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.8156411479061829, 'bagging_freq': 2, 'feature_fraction': 0.9453575093796732, 'learning_rate': 0.015016007831064087, 'min_child_samples': 26, 'num_leaves': 80, 'reg_alpha': 2, 'reg_lambda': 3.6758012630853103, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.225276 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.191730 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.9022336069269954, 'bagging_freq': 2, 'feature_fraction': 0.9373662317255621, 'learning_rate': 0.014947332175194025, 'min_child_samples': 26, 'num_leaves': 80, 'reg_alpha': 2, 'reg_lambda': 3.5907566887206896, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.224434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.227178 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.9092910898548142, 'bagging_freq': 2, 'feature_fraction': 0.9587253380203777, 'learning_rate': 0.01594569445481636, 'min_child_samples': 26, 'num_leaves': 80, 'reg_alpha': 2, 'reg_lambda': 0.5437059106621431, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.228737 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.192622 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.9243731638563496, 'bagging_freq': 2, 'feature_fraction': 0.8579304338996762, 'learning_rate': 0.07045436429758375, 'min_child_samples': 26, 'num_leaves': 10, 'reg_alpha': 2, 'reg_lambda': 3.6181498300907533, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.225265 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.220405 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.8208518394986082, 'bagging_freq': 2, 'feature_fraction': 0.9942064788262652, 'learning_rate': 0.17680977265314285, 'min_child_samples': 26, 'num_leaves': 230, 'reg_alpha': 2, 'reg_lambda': 0.014934884031477136, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.219674 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.225700 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.9087334379512036, 'bagging_freq': 8, 'feature_fraction': 0.9435725455149249, 'learning_rate': 0.28640982898261946, 'min_child_samples': 26, 'num_leaves': 130, 'reg_alpha': 2, 'reg_lambda': 2.818017545083271, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.224072 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.188394 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "{'bagging_fraction': 0.8733261239697325, 'bagging_freq': 2, 'feature_fraction': 0.8775960565074609, 'learning_rate': 0.011582875040907454, 'min_child_samples': 11, 'num_leaves': 210, 'reg_alpha': 9, 'reg_lambda': 4.553491961304738, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.224417 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.183489 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65975                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396931                            \n",
      "[LightGBM] [Info] Start training from score -0.390344                            \n",
      "100%|██████████| 30/30 [02:54<00:00,  5.83s/trial, best loss: 3.6844192003528775]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-3f2f3376e96b>:27: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  prediction_train = pd.Series()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.340990 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65666\n",
      "[LightGBM] [Info] Number of data points in the train set: 161533, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.390986\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\ttrain's rmse: 3.75892\tvalid's rmse: 3.77103\n",
      "[100]\ttrain's rmse: 3.71868\tvalid's rmse: 3.73851\n",
      "[150]\ttrain's rmse: 3.69462\tvalid's rmse: 3.721\n",
      "[200]\ttrain's rmse: 3.67853\tvalid's rmse: 3.71087\n",
      "[250]\ttrain's rmse: 3.66669\tvalid's rmse: 3.70432\n",
      "[300]\ttrain's rmse: 3.65751\tvalid's rmse: 3.69995\n",
      "[350]\ttrain's rmse: 3.65049\tvalid's rmse: 3.69762\n",
      "[400]\ttrain's rmse: 3.64309\tvalid's rmse: 3.69551\n",
      "[450]\ttrain's rmse: 3.63597\tvalid's rmse: 3.69377\n",
      "[500]\ttrain's rmse: 3.63008\tvalid's rmse: 3.6925\n",
      "[550]\ttrain's rmse: 3.62462\tvalid's rmse: 3.69105\n",
      "[600]\ttrain's rmse: 3.6198\tvalid's rmse: 3.69013\n",
      "[650]\ttrain's rmse: 3.61443\tvalid's rmse: 3.68931\n",
      "[700]\ttrain's rmse: 3.61005\tvalid's rmse: 3.68884\n",
      "[750]\ttrain's rmse: 3.60491\tvalid's rmse: 3.68812\n",
      "[800]\ttrain's rmse: 3.59974\tvalid's rmse: 3.68759\n",
      "[850]\ttrain's rmse: 3.59474\tvalid's rmse: 3.68714\n",
      "Early stopping, best iteration is:\n",
      "[831]\ttrain's rmse: 3.5965\tvalid's rmse: 3.68697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vdmion\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\vdmion\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.349296 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65677\n",
      "[LightGBM] [Info] Number of data points in the train set: 161533, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396781\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\ttrain's rmse: 3.76719\tvalid's rmse: 3.73854\n",
      "[100]\ttrain's rmse: 3.72832\tvalid's rmse: 3.70102\n",
      "[150]\ttrain's rmse: 3.70615\tvalid's rmse: 3.68157\n",
      "[200]\ttrain's rmse: 3.68981\tvalid's rmse: 3.6711\n",
      "[250]\ttrain's rmse: 3.67731\tvalid's rmse: 3.6642\n",
      "[300]\ttrain's rmse: 3.6682\tvalid's rmse: 3.66021\n",
      "[350]\ttrain's rmse: 3.66021\tvalid's rmse: 3.65732\n",
      "[400]\ttrain's rmse: 3.653\tvalid's rmse: 3.65522\n",
      "[450]\ttrain's rmse: 3.6464\tvalid's rmse: 3.65337\n",
      "[500]\ttrain's rmse: 3.63997\tvalid's rmse: 3.65184\n",
      "[550]\ttrain's rmse: 3.63353\tvalid's rmse: 3.6508\n",
      "[600]\ttrain's rmse: 3.62768\tvalid's rmse: 3.64985\n",
      "[650]\ttrain's rmse: 3.62235\tvalid's rmse: 3.64887\n",
      "[700]\ttrain's rmse: 3.61719\tvalid's rmse: 3.64818\n",
      "[750]\ttrain's rmse: 3.61244\tvalid's rmse: 3.64785\n",
      "[800]\ttrain's rmse: 3.60741\tvalid's rmse: 3.6471\n",
      "[850]\ttrain's rmse: 3.60239\tvalid's rmse: 3.64632\n",
      "[900]\ttrain's rmse: 3.59807\tvalid's rmse: 3.64592\n",
      "[950]\ttrain's rmse: 3.59319\tvalid's rmse: 3.64498\n",
      "[1000]\ttrain's rmse: 3.58882\tvalid's rmse: 3.64452\n",
      "Early stopping, best iteration is:\n",
      "[998]\ttrain's rmse: 3.58903\tvalid's rmse: 3.64451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vdmion\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\vdmion\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.353090 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65684\n",
      "[LightGBM] [Info] Number of data points in the train set: 161534, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.390348\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\ttrain's rmse: 3.75231\tvalid's rmse: 3.78794\n",
      "[100]\ttrain's rmse: 3.71093\tvalid's rmse: 3.75345\n",
      "[150]\ttrain's rmse: 3.68701\tvalid's rmse: 3.73658\n",
      "[200]\ttrain's rmse: 3.67191\tvalid's rmse: 3.72698\n",
      "[250]\ttrain's rmse: 3.66002\tvalid's rmse: 3.72123\n",
      "[300]\ttrain's rmse: 3.65114\tvalid's rmse: 3.71716\n",
      "[350]\ttrain's rmse: 3.64347\tvalid's rmse: 3.71463\n",
      "[400]\ttrain's rmse: 3.63661\tvalid's rmse: 3.71266\n",
      "[450]\ttrain's rmse: 3.63053\tvalid's rmse: 3.71135\n",
      "[500]\ttrain's rmse: 3.62438\tvalid's rmse: 3.71013\n",
      "[550]\ttrain's rmse: 3.61892\tvalid's rmse: 3.70927\n",
      "[600]\ttrain's rmse: 3.61344\tvalid's rmse: 3.70822\n",
      "[650]\ttrain's rmse: 3.60779\tvalid's rmse: 3.70713\n",
      "[700]\ttrain's rmse: 3.60292\tvalid's rmse: 3.70661\n",
      "[750]\ttrain's rmse: 3.59785\tvalid's rmse: 3.70588\n",
      "[800]\ttrain's rmse: 3.59275\tvalid's rmse: 3.70488\n",
      "[850]\ttrain's rmse: 3.588\tvalid's rmse: 3.70413\n",
      "[900]\ttrain's rmse: 3.58365\tvalid's rmse: 3.70404\n",
      "Early stopping, best iteration is:\n",
      "[870]\ttrain's rmse: 3.58625\tvalid's rmse: 3.70395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vdmion\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\vdmion\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.305438 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65642\n",
      "[LightGBM] [Info] Number of data points in the train set: 161534, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.391392\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\ttrain's rmse: 3.73315\tvalid's rmse: 3.86928\n",
      "[100]\ttrain's rmse: 3.69309\tvalid's rmse: 3.83508\n",
      "[150]\ttrain's rmse: 3.66977\tvalid's rmse: 3.81704\n",
      "[200]\ttrain's rmse: 3.65402\tvalid's rmse: 3.80625\n",
      "[250]\ttrain's rmse: 3.64247\tvalid's rmse: 3.79953\n",
      "[300]\ttrain's rmse: 3.63288\tvalid's rmse: 3.79472\n",
      "[350]\ttrain's rmse: 3.62507\tvalid's rmse: 3.79137\n",
      "[400]\ttrain's rmse: 3.61782\tvalid's rmse: 3.78885\n",
      "[450]\ttrain's rmse: 3.61148\tvalid's rmse: 3.78654\n",
      "[500]\ttrain's rmse: 3.60518\tvalid's rmse: 3.78439\n",
      "[550]\ttrain's rmse: 3.59867\tvalid's rmse: 3.78269\n",
      "[600]\ttrain's rmse: 3.59283\tvalid's rmse: 3.78127\n",
      "[650]\ttrain's rmse: 3.58721\tvalid's rmse: 3.77968\n",
      "[700]\ttrain's rmse: 3.58204\tvalid's rmse: 3.7786\n",
      "[750]\ttrain's rmse: 3.57702\tvalid's rmse: 3.77795\n",
      "[800]\ttrain's rmse: 3.57227\tvalid's rmse: 3.7771\n",
      "[850]\ttrain's rmse: 3.56756\tvalid's rmse: 3.77617\n",
      "[900]\ttrain's rmse: 3.56254\tvalid's rmse: 3.77507\n",
      "[950]\ttrain's rmse: 3.55836\tvalid's rmse: 3.77424\n",
      "[1000]\ttrain's rmse: 3.55367\tvalid's rmse: 3.7737\n",
      "[1050]\ttrain's rmse: 3.54943\tvalid's rmse: 3.77328\n",
      "[1100]\ttrain's rmse: 3.54517\tvalid's rmse: 3.77277\n",
      "[1150]\ttrain's rmse: 3.54089\tvalid's rmse: 3.77242\n",
      "Early stopping, best iteration is:\n",
      "[1144]\ttrain's rmse: 3.54163\tvalid's rmse: 3.77227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vdmion\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\vdmion\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.350972 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65695\n",
      "[LightGBM] [Info] Number of data points in the train set: 161534, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.398675\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\ttrain's rmse: 3.78477\tvalid's rmse: 3.65732\n",
      "[100]\ttrain's rmse: 3.74396\tvalid's rmse: 3.62525\n",
      "[150]\ttrain's rmse: 3.72006\tvalid's rmse: 3.6094\n",
      "[200]\ttrain's rmse: 3.7039\tvalid's rmse: 3.59961\n",
      "[250]\ttrain's rmse: 3.69286\tvalid's rmse: 3.59368\n",
      "[300]\ttrain's rmse: 3.68357\tvalid's rmse: 3.59002\n",
      "[350]\ttrain's rmse: 3.67565\tvalid's rmse: 3.58705\n",
      "[400]\ttrain's rmse: 3.66878\tvalid's rmse: 3.58453\n",
      "[450]\ttrain's rmse: 3.66276\tvalid's rmse: 3.5828\n",
      "[500]\ttrain's rmse: 3.65653\tvalid's rmse: 3.58089\n",
      "[550]\ttrain's rmse: 3.65107\tvalid's rmse: 3.57947\n",
      "[600]\ttrain's rmse: 3.64527\tvalid's rmse: 3.57869\n",
      "[650]\ttrain's rmse: 3.64029\tvalid's rmse: 3.57741\n",
      "[700]\ttrain's rmse: 3.63434\tvalid's rmse: 3.57659\n",
      "[750]\ttrain's rmse: 3.62887\tvalid's rmse: 3.57541\n",
      "[800]\ttrain's rmse: 3.62374\tvalid's rmse: 3.57487\n",
      "[850]\ttrain's rmse: 3.61877\tvalid's rmse: 3.57409\n",
      "[900]\ttrain's rmse: 3.61421\tvalid's rmse: 3.57325\n",
      "[950]\ttrain's rmse: 3.60936\tvalid's rmse: 3.57294\n",
      "[1000]\ttrain's rmse: 3.60497\tvalid's rmse: 3.57253\n",
      "Early stopping, best iteration is:\n",
      "[1000]\ttrain's rmse: 3.60497\tvalid's rmse: 3.57253\n",
      "[3.6869706135632367, 3.6445135024361908, 3.7039487803864932, 3.772271999086466, 3.5725331932165627] 3.6760476177377894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-3f2f3376e96b>:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['target'] = prediction_test / 5\n"
     ]
    }
   ],
   "source": [
    "train_LGBM, test_LGBM = feature_select_wrapper(train, test)\n",
    "best_clf = param_hyperopt(train_LGBM)\n",
    "train_predict(train_LGBM, test_LGBM, best_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b855e1f-e0a8-4d0a-bbee-1eed6a798443",
   "metadata": {},
   "source": [
    "接下来即可在竞赛主页提交预测结果。最终公榜私榜评分如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e413b62-3fb2-4737-a7cc-4d73a0d476d9",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/09/EnekwUaMIVKQfDt.png\" alt=\"image-20211209173249232\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fccb3dc-ae3d-409b-893f-77b18919281c",
   "metadata": {},
   "source": [
    "对比此前结果："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a17ad27-432b-4e25-aa8e-a23150bfbe4d",
   "metadata": {},
   "source": [
    "| 模型 | Private Score | Public Score |\n",
    "| ------ | ------ | ------ |\n",
    "| randomforest | 3.65455 | 3.74969 |\n",
    "| randomforest+validation | 3.65173 | 3.74954 |\n",
    "| LightGBM | 3.69723 | 3.80436 |\n",
    "| LightGBM+validation | 3.64403 | 3.73875 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ae8118-f11c-477b-bfd7-3890006a86f8",
   "metadata": {},
   "source": [
    "能够看出，经过交叉验证后输出的平均值结果，较此前的预测评分，有较大提升，这也是目前我们跑出的最好成绩。同时，交叉验证的作用已得到充分征明，后续在进行其他模型训练时仅考虑模型+交叉验证的输出结果，不再进行单模型结果输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea0d7ba-6379-461e-9e93-1966c1c7f539",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1588ccc8-a6be-4d81-b189-f8a01f2b1707",
   "metadata": {},
   "source": [
    "## <center> 二、单模训练策略（三）\n",
    "## <center> **NLP特征优化+XGBoost建模+贝叶斯优化器**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09c5025-7153-4cbd-8841-55fab5d3ca85",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在执行完随机森林与LightGBM后，我们已经对不同集成算法的竞赛建模流程有了一定的了解，大家可以照此思路继续尝试其他集成模型。当然，如果想进一步优化提升模型效果，我们可以考虑围绕数据集中的部分ID字段进行NLP特征优化。因此，接下来，我们考虑采用CountVectorizer, TfidfVectorizer两种方法对数据集中部分特征进行NLP特征衍生，并且采用XGBoost模型进行预测，同时考虑进一步使用另一种贝叶斯优化器（bayes_opt）来进行模型参数调优。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89d22dc-c68a-4360-98a9-411411ec2ca0",
   "metadata": {},
   "source": [
    "> 当然，此处训练的三个模型分别采用了不同的优化器、甚至是采用了不同的特征衍生的方法，也是为了令这三个模型尽可能的存在一定的差异性，从而为后续的模型融合的融合效果做铺垫。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcdbb57-d637-41dd-99a7-e72702c8495c",
   "metadata": {},
   "source": [
    "### 1.NLP特征优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54580d2-fd36-48a4-a648-1fba99c97b51",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先我们注意到，在数据集中存在大量的ID相关的列（除了card_id外），包括'merchant_id'、'merchant_category_id'、'state_id'、'subsector_id'、'city_id'等，考虑到这些ID在出现频率方面都和用户实际的交易行为息息相关，例如对于单独用户A来说，在其交易记录中频繁出现某商户id（假设为B），则说明该用户A对商户B情有独钟，而如果在不同的用户交易数据中，都频繁的出现了商户B，则说明这家商户受到广泛欢迎，而进一步的说明A的喜好可能和大多数用户一致，而反之则说明A用户的喜好较为独特。为了能够挖掘出类似信息，我们可以考虑采用NLP中CountVector和TF-IDF两种方法来进行进一步特征衍生，其中CountVector可以挖掘类似某用户钟爱某商铺的信息，而TF-IDF则可进一步挖掘出类似某用户的喜好是否普遍或一致等信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae865ce-7860-4e8f-8389-e450c22abb15",
   "metadata": {},
   "source": [
    "&emsp;&emsp;此外，若要借助NLP方法进行进一步特征衍生，则需要考虑到新创建的特征数量过大所导致的问题，因此我们建议在使用上述方法的同时，考虑借助scipy中的稀疏矩阵相关方法，来进行新特征的存储与读取。即采用CSR格式来创建稀疏矩阵，用npz格式来进行本地数据文件保存。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b775edb2-2f97-4206-b218-284b6d34edc4",
   "metadata": {},
   "source": [
    "需要导入的包如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "15058d4a-d8b8-4e9f-95c0-9468907680f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572e8e25-9802-47c4-8b39-197b79e56328",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来进行NLP特征创建。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebbcc614-3fbd-4d24-8c59-a271c2a382b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merchant_id_new\n",
      "merchant_id_hist\n",
      "merchant_id_all\n",
      "merchant_category_id_new\n",
      "merchant_category_id_hist\n",
      "merchant_category_id_all\n",
      "state_id_new\n",
      "state_id_hist\n",
      "state_id_all\n",
      "subsector_id_new\n",
      "subsector_id_hist\n",
      "subsector_id_all\n",
      "city_id_new\n",
      "city_id_hist\n",
      "city_id_all\n"
     ]
    }
   ],
   "source": [
    "# 创建空DataFrame用于保存NLP特征\n",
    "train_x = pd.DataFrame()\n",
    "test_x = pd.DataFrame()\n",
    "\n",
    "# 实例化CountVectorizer评估器与TfidfVectorizer评估器\n",
    "cntv = CountVectorizer()\n",
    "tfv = TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.9, use_idf=1, smooth_idf=1, sublinear_tf=1)\n",
    "\n",
    "# 创建空列表用户保存修正后的列名称\n",
    "vector_feature =[]\n",
    "for co in ['merchant_id', 'merchant_category_id', 'state_id', 'subsector_id', 'city_id']:\n",
    "    vector_feature.extend([co+'_new', co+'_hist', co+'_all'])\n",
    "    \n",
    "# 提取每一列进行新特征衍生\n",
    "for feature in vector_feature:\n",
    "    print(feature)\n",
    "    cntv.fit(train[feature].append(test[feature]))\n",
    "    train_x = sparse.hstack((train_x, cntv.transform(train[feature]))).tocsr()\n",
    "    test_x = sparse.hstack((test_x, cntv.transform(test[feature]))).tocsr()\n",
    "    \n",
    "    tfv.fit(train[feature].append(test[feature]))\n",
    "    train_x = sparse.hstack((train_x, tfv.transform(train[feature]))).tocsr()\n",
    "    test_x = sparse.hstack((test_x, tfv.transform(test[feature]))).tocsr()\n",
    "    \n",
    "# 保存NLP特征衍生结果\n",
    "sparse.save_npz(\"preprocess/train_nlp.npz\", train_x)\n",
    "sparse.save_npz(\"preprocess/test_nlp.npz\", test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b22752d7-b5cb-411f-a0c8-64f2c1c041f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201917, 1846286)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4d7984-bb60-4e40-8af6-f19fed4b923b",
   "metadata": {},
   "source": [
    "### 2.XGBoost模型训练与优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b89fcb-247a-4fb0-b9e3-727ac08524f6",
   "metadata": {},
   "source": [
    "- 数据读取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbd82d8-7d76-404a-820a-c25f4ff5baff",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，来进行XGBoost的模型训练与优化，首先需要导入必要的包以及进行数据读取："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fbaf209-2a63-40a0-8863-5cf106117294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import f_regression\n",
    "from numpy.random import RandomState\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e43f799e-d011-42ec-b9bc-c99a083a9c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('preprocess/train.csv')\n",
    "test = pd.read_csv('preprocess/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cbc35a4-6d7c-48d6-abc9-4b6e06c955bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train.columns.tolist()\n",
    "features.remove('card_id')\n",
    "features.remove('target')\n",
    "\n",
    "train_x = sparse.load_npz(\"preprocess/train_nlp.npz\")\n",
    "test_x = sparse.load_npz(\"preprocess/test_nlp.npz\")\n",
    "\n",
    "train_x = sparse.hstack((train_x, train[features])).tocsr()\n",
    "test_x = sparse.hstack((test_x, test[features])).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770c0e1f-b08f-42c8-abc2-053d09121b28",
   "metadata": {},
   "source": [
    "- 模型训练与优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c9c647-ae5c-4b41-953a-0b3d1bce884f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来进行模型训练，本轮训练的流程和lgb模型流程类似，首先需要创建用于重复申明固定参数的函数，然后定义搜索和优化函数，并在优化函数内部调用参数回调函数，最后定义模型预测函数，该函数将借助交叉验证过程来进行测试集预测，并同步创建验证集预测结果与每个模型对测试集的预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37c97de2-bffa-4e26-bbb8-c77c8f2dcb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数回调函数\n",
    "def params_append(params):\n",
    "    \"\"\"\n",
    "\n",
    "    :param params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    params['objective'] = 'reg:squarederror'\n",
    "    params['eval_metric'] = 'rmse'\n",
    "    params[\"min_child_weight\"] = int(params[\"min_child_weight\"])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    return params\n",
    "\n",
    "# 模型优化函数\n",
    "def param_beyesian(train):\n",
    "    \"\"\"\n",
    "\n",
    "    :param train:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Part 1.数据准备\n",
    "    train_y = pd.read_csv(\"data/train.csv\")['target']\n",
    "    # 数据封装\n",
    "    sample_index = train_y.sample(frac=0.1, random_state=2020).index.tolist()\n",
    "    train_data = xgb.DMatrix(train.tocsr()[sample_index, :\n",
    "                             ], train_y.loc[sample_index].values, silent=True)\n",
    "    \n",
    "    # 借助cv过程构建目标函数\n",
    "    def xgb_cv(colsample_bytree, subsample, min_child_weight, max_depth,\n",
    "               reg_alpha, eta,\n",
    "               reg_lambda):\n",
    "        \"\"\"\n",
    "\n",
    "        :param colsample_bytree:\n",
    "        :param subsample:\n",
    "        :param min_child_weight:\n",
    "        :param max_depth:\n",
    "        :param reg_alpha:\n",
    "        :param eta:\n",
    "        :param reg_lambda:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        params = {'objective': 'reg:squarederror',\n",
    "                  'early_stopping_round': 50,\n",
    "                  'eval_metric': 'rmse'}\n",
    "        params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "        params['subsample'] = max(min(subsample, 1), 0)\n",
    "        params[\"min_child_weight\"] = int(min_child_weight)\n",
    "        params['max_depth'] = int(max_depth)\n",
    "        params['eta'] = float(eta)\n",
    "        params['reg_alpha'] = max(reg_alpha, 0)\n",
    "        params['reg_lambda'] = max(reg_lambda, 0)\n",
    "        print(params)\n",
    "        cv_result = xgb.cv(params, train_data,\n",
    "                           num_boost_round=1000,\n",
    "                           nfold=2, seed=2,\n",
    "                           stratified=False,\n",
    "                           shuffle=True,\n",
    "                           early_stopping_rounds=30,\n",
    "                           verbose_eval=False)\n",
    "        return -min(cv_result['test-rmse-mean'])\n",
    "    \n",
    "    # 调用贝叶斯优化器进行模型优化\n",
    "    xgb_bo = BayesianOptimization(\n",
    "        xgb_cv,\n",
    "        {'colsample_bytree': (0.5, 1),\n",
    "         'subsample': (0.5, 1),\n",
    "         'min_child_weight': (1, 30),\n",
    "         'max_depth': (5, 12),\n",
    "         'reg_alpha': (0, 5),\n",
    "         'eta':(0.02, 0.2),\n",
    "         'reg_lambda': (0, 5)}\n",
    "    )\n",
    "    xgb_bo.maximize(init_points=21, n_iter=5)  # init_points表示初始点，n_iter代表迭代次数（即采样数）\n",
    "    print(xgb_bo.max['target'], xgb_bo.max['params'])\n",
    "    return xgb_bo.max['params']\n",
    "\n",
    "# 交叉验证预测函数\n",
    "def train_predict(train, test, params):\n",
    "    \"\"\"\n",
    "\n",
    "    :param train:\n",
    "    :param test:\n",
    "    :param params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    train_y = pd.read_csv(\"data/train.csv\")['target']\n",
    "    test_data = xgb.DMatrix(test)\n",
    "\n",
    "    params = params_append(params)\n",
    "    kf = KFold(n_splits=5, random_state=2020, shuffle=True)\n",
    "    prediction_test = 0\n",
    "    cv_score = []\n",
    "    prediction_train = pd.Series()\n",
    "    ESR = 30\n",
    "    NBR = 10000\n",
    "    VBE = 50\n",
    "    for train_part_index, eval_index in kf.split(train, train_y):\n",
    "        # 模型训练\n",
    "        train_part = xgb.DMatrix(train.tocsr()[train_part_index, :],\n",
    "                                 train_y.loc[train_part_index])\n",
    "        eval = xgb.DMatrix(train.tocsr()[eval_index, :],\n",
    "                           train_y.loc[eval_index])\n",
    "        bst = xgb.train(params, train_part, NBR, [(train_part, 'train'),\n",
    "                                                          (eval, 'eval')], verbose_eval=VBE,\n",
    "                        maximize=False, early_stopping_rounds=ESR, )\n",
    "        prediction_test += bst.predict(test_data)\n",
    "        eval_pre = bst.predict(eval)\n",
    "        prediction_train = prediction_train.append(pd.Series(eval_pre, index=eval_index))\n",
    "        score = np.sqrt(mean_squared_error(train_y.loc[eval_index].values, eval_pre))\n",
    "        cv_score.append(score)\n",
    "    print(cv_score, sum(cv_score) / 5)\n",
    "    pd.Series(prediction_train.sort_index().values).to_csv(\"preprocess/train_xgboost.csv\", index=False)\n",
    "    pd.Series(prediction_test / 5).to_csv(\"preprocess/test_xgboost.csv\", index=False)\n",
    "    test = pd.read_csv('data/test.csv')\n",
    "    test['target'] = prediction_test / 5\n",
    "    test[['card_id', 'target']].to_csv(\"result/submission_xgboost.csv\", index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1add86ca-6169-4932-888d-9204bc962657",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |    eta    | max_depth | min_ch... | reg_alpha | reg_la... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "{'objective': 'reg:squarederror', 'early_stopping_round': 50, 'eval_metric': 'rmse', 'colsample_bytree': 0.9008280232486963, 'subsample': 0.8904106813206965, 'min_child_weight': 29, 'max_depth': 9, 'eta': 0.15949688000040066, 'reg_alpha': 1.8850960576404892, 'reg_lambda': 4.436160789463865}\n",
      "[12:52:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:52:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-3.828   \u001b[0m | \u001b[0m 0.9008  \u001b[0m | \u001b[0m 0.1595  \u001b[0m | \u001b[0m 9.636   \u001b[0m | \u001b[0m 29.93   \u001b[0m | \u001b[0m 1.885   \u001b[0m | \u001b[0m 4.436   \u001b[0m | \u001b[0m 0.8904  \u001b[0m |\n",
      "{'objective': 'reg:squarederror', 'early_stopping_round': 50, 'eval_metric': 'rmse', 'colsample_bytree': 0.8733613720817149, 'subsample': 0.6142495949337223, 'min_child_weight': 29, 'max_depth': 8, 'eta': 0.1802456543940385, 'reg_alpha': 0.8072782146576124, 'reg_lambda': 2.284382168716956}\n",
      "[12:54:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:54:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-3.861   \u001b[0m | \u001b[0m 0.8734  \u001b[0m | \u001b[0m 0.1802  \u001b[0m | \u001b[0m 8.376   \u001b[0m | \u001b[0m 29.33   \u001b[0m | \u001b[0m 0.8073  \u001b[0m | \u001b[0m 2.284   \u001b[0m | \u001b[0m 0.6142  \u001b[0m |\n",
      "{'objective': 'reg:squarederror', 'early_stopping_round': 50, 'eval_metric': 'rmse', 'colsample_bytree': 0.9268730478860339, 'subsample': 0.9187182128076231, 'min_child_weight': 18, 'max_depth': 10, 'eta': 0.10485164882690341, 'reg_alpha': 1.4052740152170013, 'reg_lambda': 4.420615175732838}\n",
      "[12:55:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:55:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-3.848   \u001b[0m | \u001b[0m 0.9269  \u001b[0m | \u001b[0m 0.1049  \u001b[0m | \u001b[0m 10.6    \u001b[0m | \u001b[0m 18.53   \u001b[0m | \u001b[0m 1.405   \u001b[0m | \u001b[0m 4.421   \u001b[0m | \u001b[0m 0.9187  \u001b[0m |\n",
      "{'objective': 'reg:squarederror', 'early_stopping_round': 50, 'eval_metric': 'rmse', 'colsample_bytree': 0.625422443194849, 'subsample': 0.6710318901133161, 'min_child_weight': 27, 'max_depth': 8, 'eta': 0.05185743596319062, 'reg_alpha': 1.1690459082772615, 'reg_lambda': 3.562136294750095}\n",
      "[12:56:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:56:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-3.829   \u001b[0m | \u001b[0m 0.6254  \u001b[0m | \u001b[0m 0.05186 \u001b[0m | \u001b[0m 8.038   \u001b[0m | \u001b[0m 27.47   \u001b[0m | \u001b[0m 1.169   \u001b[0m | \u001b[0m 3.562   \u001b[0m | \u001b[0m 0.671   \u001b[0m |\n",
      "{'objective': 'reg:squarederror', 'early_stopping_round': 50, 'eval_metric': 'rmse', 'colsample_bytree': 0.902497211065604, 'subsample': 0.8413464797085647, 'min_child_weight': 27, 'max_depth': 8, 'eta': 0.09748251874712145, 'reg_alpha': 2.9238818257239756, 'reg_lambda': 0.6901256237971481}\n",
      "[12:58:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:58:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m-3.824   \u001b[0m | \u001b[95m 0.9025  \u001b[0m | \u001b[95m 0.09748 \u001b[0m | \u001b[95m 8.835   \u001b[0m | \u001b[95m 27.77   \u001b[0m | \u001b[95m 2.924   \u001b[0m | \u001b[95m 0.6901  \u001b[0m | \u001b[95m 0.8413  \u001b[0m |\n",
      "{'objective': 'reg:squarederror', 'early_stopping_round': 50, 'eval_metric': 'rmse', 'colsample_bytree': 0.9977024155365132, 'subsample': 0.8115473489069764, 'min_child_weight': 29, 'max_depth': 8, 'eta': 0.08573985701358497, 'reg_alpha': 4.39911724928385, 'reg_lambda': 0.98249230645374}\n",
      "[12:59:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:59:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-3.827   \u001b[0m | \u001b[0m 0.9977  \u001b[0m | \u001b[0m 0.08574 \u001b[0m | \u001b[0m 8.21    \u001b[0m | \u001b[0m 29.41   \u001b[0m | \u001b[0m 4.399   \u001b[0m | \u001b[0m 0.9825  \u001b[0m | \u001b[0m 0.8115  \u001b[0m |\n",
      "{'objective': 'reg:squarederror', 'early_stopping_round': 50, 'eval_metric': 'rmse', 'colsample_bytree': 0.9407066205905263, 'subsample': 0.827880546540135, 'min_child_weight': 6, 'max_depth': 9, 'eta': 0.1061485706403698, 'reg_alpha': 2.139211485717212, 'reg_lambda': 3.2008526648164737}\n",
      "[13:00:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:00:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-3.851   \u001b[0m | \u001b[0m 0.9407  \u001b[0m | \u001b[0m 0.1061  \u001b[0m | \u001b[0m 9.67    \u001b[0m | \u001b[0m 6.107   \u001b[0m | \u001b[0m 2.139   \u001b[0m | \u001b[0m 3.201   \u001b[0m | \u001b[0m 0.8279  \u001b[0m |\n",
      "{'objective': 'reg:squarederror', 'early_stopping_round': 50, 'eval_metric': 'rmse', 'colsample_bytree': 0.6566786860590916, 'subsample': 0.6452363126120251, 'min_child_weight': 18, 'max_depth': 8, 'eta': 0.14417434852557892, 'reg_alpha': 4.3160287883092066, 'reg_lambda': 3.5328339088472847}\n",
      "[13:02:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:02:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-3.848   \u001b[0m | \u001b[0m 0.6567  \u001b[0m | \u001b[0m 0.1442  \u001b[0m | \u001b[0m 8.496   \u001b[0m | \u001b[0m 18.79   \u001b[0m | \u001b[0m 4.316   \u001b[0m | \u001b[0m 3.533   \u001b[0m | \u001b[0m 0.6452  \u001b[0m |\n",
      "{'objective': 'reg:squarederror', 'early_stopping_round': 50, 'eval_metric': 'rmse', 'colsample_bytree': 0.611191859916479, 'subsample': 0.9607934404730234, 'min_child_weight': 10, 'max_depth': 5, 'eta': 0.16887507984941832, 'reg_alpha': 1.8949997250691952, 'reg_lambda': 1.9700264363894808}\n",
      "[13:03:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:03:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-3.84    \u001b[0m | \u001b[0m 0.6112  \u001b[0m | \u001b[0m 0.1689  \u001b[0m | \u001b[0m 5.955   \u001b[0m | \u001b[0m 10.8    \u001b[0m | \u001b[0m 1.895   \u001b[0m | \u001b[0m 1.97    \u001b[0m | \u001b[0m 0.9608  \u001b[0m |\n",
      "{'objective': 'reg:squarederror', 'early_stopping_round': 50, 'eval_metric': 'rmse', 'colsample_bytree': 0.5688557898775366, 'subsample': 0.792738671048387, 'min_child_weight': 8, 'max_depth': 8, 'eta': 0.029707284678996262, 'reg_alpha': 2.13929302944516, 'reg_lambda': 1.716127401031825}\n",
      "[13:03:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:03:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-3.833   \u001b[0m | \u001b[0m 0.5689  \u001b[0m | \u001b[0m 0.02971 \u001b[0m | \u001b[0m 8.436   \u001b[0m | \u001b[0m 8.559   \u001b[0m | \u001b[0m 2.139   \u001b[0m | \u001b[0m 1.716   \u001b[0m | \u001b[0m 0.7927  \u001b[0m |\n",
      "{'objective': 'reg:squarederror', 'early_stopping_round': 50, 'eval_metric': 'rmse', 'colsample_bytree': 0.8909115661247835, 'subsample': 0.838354468762074, 'min_child_weight': 24, 'max_depth': 6, 'eta': 0.11054484780152028, 'reg_alpha': 4.1364437329554455, 'reg_lambda': 3.1715376094875305}\n",
      "[13:05:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:05:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "| \u001b[95m 11      \u001b[0m | \u001b[95m-3.821   \u001b[0m | \u001b[95m 0.8909  \u001b[0m | \u001b[95m 0.1105  \u001b[0m | \u001b[95m 6.204   \u001b[0m | \u001b[95m 24.25   \u001b[0m | \u001b[95m 4.136   \u001b[0m | \u001b[95m 3.172   \u001b[0m | \u001b[95m 0.8384  \u001b[0m |\n",
      "{'objective': 'reg:squarederror', 'early_stopping_round': 50, 'eval_metric': 'rmse', 'colsample_bytree': 0.9465569893212517, 'subsample': 0.5866450532823893, 'min_child_weight': 13, 'max_depth': 7, 'eta': 0.1596273004501417, 'reg_alpha': 2.364218860201553, 'reg_lambda': 1.7722559715723234}\n",
      "[13:06:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:06:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-3.881   \u001b[0m | \u001b[0m 0.9466  \u001b[0m | \u001b[0m 0.1596  \u001b[0m | \u001b[0m 7.621   \u001b[0m | \u001b[0m 13.79   \u001b[0m | \u001b[0m 2.364   \u001b[0m | \u001b[0m 1.772   \u001b[0m | \u001b[0m 0.5866  \u001b[0m |\n",
      "{'objective': 'reg:squarederror', 'early_stopping_round': 50, 'eval_metric': 'rmse', 'colsample_bytree': 0.9627435399195992, 'subsample': 0.9808950735528328, 'min_child_weight': 27, 'max_depth': 6, 'eta': 0.05741987032403584, 'reg_alpha': 3.9968620363457683, 'reg_lambda': 0.4830516515499872}\n",
      "[13:07:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:07:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "| \u001b[95m 13      \u001b[0m | \u001b[95m-3.812   \u001b[0m | \u001b[95m 0.9627  \u001b[0m | \u001b[95m 0.05742 \u001b[0m | \u001b[95m 6.407   \u001b[0m | \u001b[95m 27.35   \u001b[0m | \u001b[95m 3.997   \u001b[0m | \u001b[95m 0.4831  \u001b[0m | \u001b[95m 0.9809  \u001b[0m |\n",
      "{'objective': 'reg:squarederror', 'early_stopping_round': 50, 'eval_metric': 'rmse', 'colsample_bytree': 0.9044844729820345, 'subsample': 0.5762009686960115, 'min_child_weight': 24, 'max_depth': 5, 'eta': 0.16975603999498512, 'reg_alpha': 3.085476522155029, 'reg_lambda': 3.5171910395636363}\n",
      "[13:09:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:09:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-3.838   \u001b[0m | \u001b[0m 0.9045  \u001b[0m | \u001b[0m 0.1698  \u001b[0m | \u001b[0m 5.856   \u001b[0m | \u001b[0m 24.39   \u001b[0m | \u001b[0m 3.085   \u001b[0m | \u001b[0m 3.517   \u001b[0m | \u001b[0m 0.5762  \u001b[0m |\n",
      "{'objective': 'reg:squarederror', 'early_stopping_round': 50, 'eval_metric': 'rmse', 'colsample_bytree': 0.7325643867708108, 'subsample': 0.5099111689834741, 'min_child_weight': 2, 'max_depth': 6, 'eta': 0.1764589326251524, 'reg_alpha': 1.802265951008657, 'reg_lambda': 3.2543009145085158}\n",
      "[13:09:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:09:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-3.897   \u001b[0m | \u001b[0m 0.7326  \u001b[0m | \u001b[0m 0.1765  \u001b[0m | \u001b[0m 6.418   \u001b[0m | \u001b[0m 2.489   \u001b[0m | \u001b[0m 1.802   \u001b[0m | \u001b[0m 3.254   \u001b[0m | \u001b[0m 0.5099  \u001b[0m |\n",
      "{'objective': 'reg:squarederror', 'early_stopping_round': 50, 'eval_metric': 'rmse', 'colsample_bytree': 0.6131120218876751, 'subsample': 0.5253206916632518, 'min_child_weight': 11, 'max_depth': 6, 'eta': 0.1719844380323167, 'reg_alpha': 2.357407785582822, 'reg_lambda': 3.375957679817562}\n",
      "[13:10:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:10:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-3.865   \u001b[0m | \u001b[0m 0.6131  \u001b[0m | \u001b[0m 0.172   \u001b[0m | \u001b[0m 6.231   \u001b[0m | \u001b[0m 11.49   \u001b[0m | \u001b[0m 2.357   \u001b[0m | \u001b[0m 3.376   \u001b[0m | \u001b[0m 0.5253  \u001b[0m |\n",
      "{'objective': 'reg:squarederror', 'early_stopping_round': 50, 'eval_metric': 'rmse', 'colsample_bytree': 0.6831871509139502, 'subsample': 0.9297109754890269, 'min_child_weight': 2, 'max_depth': 10, 'eta': 0.04473277839394052, 'reg_alpha': 1.3430060896665403, 'reg_lambda': 3.918534812739465}\n",
      "[13:11:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:11:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-3.822   \u001b[0m | \u001b[0m 0.6832  \u001b[0m | \u001b[0m 0.04473 \u001b[0m | \u001b[0m 10.27   \u001b[0m | \u001b[0m 2.572   \u001b[0m | \u001b[0m 1.343   \u001b[0m | \u001b[0m 3.919   \u001b[0m | \u001b[0m 0.9297  \u001b[0m |\n",
      "{'objective': 'reg:squarederror', 'early_stopping_round': 50, 'eval_metric': 'rmse', 'colsample_bytree': 0.523125333707535, 'subsample': 0.7392177712247874, 'min_child_weight': 8, 'max_depth': 5, 'eta': 0.139490251639574, 'reg_alpha': 0.14523540355518005, 'reg_lambda': 0.5513235601440791}\n",
      "[13:13:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:13:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-3.856   \u001b[0m | \u001b[0m 0.5231  \u001b[0m | \u001b[0m 0.1395  \u001b[0m | \u001b[0m 5.779   \u001b[0m | \u001b[0m 8.058   \u001b[0m | \u001b[0m 0.1452  \u001b[0m | \u001b[0m 0.5513  \u001b[0m | \u001b[0m 0.7392  \u001b[0m |\n",
      "{'objective': 'reg:squarederror', 'early_stopping_round': 50, 'eval_metric': 'rmse', 'colsample_bytree': 0.7647893523603579, 'subsample': 0.9464736694936582, 'min_child_weight': 23, 'max_depth': 5, 'eta': 0.11531533678838223, 'reg_alpha': 4.517440315400995, 'reg_lambda': 0.063527226500667}\n",
      "[13:14:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:14:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-3.821   \u001b[0m | \u001b[0m 0.7648  \u001b[0m | \u001b[0m 0.1153  \u001b[0m | \u001b[0m 5.54    \u001b[0m | \u001b[0m 23.44   \u001b[0m | \u001b[0m 4.517   \u001b[0m | \u001b[0m 0.06353 \u001b[0m | \u001b[0m 0.9465  \u001b[0m |\n",
      "{'objective': 'reg:squarederror', 'early_stopping_round': 50, 'eval_metric': 'rmse', 'colsample_bytree': 0.7663684613824646, 'subsample': 0.681336879639247, 'min_child_weight': 29, 'max_depth': 11, 'eta': 0.0434838090691807, 'reg_alpha': 4.538172496852858, 'reg_lambda': 2.9134596476240175}\n",
      "[13:15:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:15:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-3.813   \u001b[0m | \u001b[0m 0.7664  \u001b[0m | \u001b[0m 0.04348 \u001b[0m | \u001b[0m 11.57   \u001b[0m | \u001b[0m 29.03   \u001b[0m | \u001b[0m 4.538   \u001b[0m | \u001b[0m 2.913   \u001b[0m | \u001b[0m 0.6813  \u001b[0m |\n",
      "{'objective': 'reg:squarederror', 'early_stopping_round': 50, 'eval_metric': 'rmse', 'colsample_bytree': 0.8349846959575062, 'subsample': 0.9790484991878241, 'min_child_weight': 7, 'max_depth': 5, 'eta': 0.11336872232404879, 'reg_alpha': 0.027603113711953675, 'reg_lambda': 3.4360173002023804}\n",
      "[13:17:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:17:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-3.83    \u001b[0m | \u001b[0m 0.835   \u001b[0m | \u001b[0m 0.1134  \u001b[0m | \u001b[0m 5.967   \u001b[0m | \u001b[0m 7.343   \u001b[0m | \u001b[0m 0.0276  \u001b[0m | \u001b[0m 3.436   \u001b[0m | \u001b[0m 0.979   \u001b[0m |\n",
      "{'objective': 'reg:squarederror', 'early_stopping_round': 50, 'eval_metric': 'rmse', 'colsample_bytree': 0.8207939072435217, 'subsample': 1.0, 'min_child_weight': 25, 'max_depth': 7, 'eta': 0.045052085309867346, 'reg_alpha': 4.63105080372857, 'reg_lambda': 1.2866888587445227}\n",
      "[13:18:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:18:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-3.814   \u001b[0m | \u001b[0m 0.8208  \u001b[0m | \u001b[0m 0.04505 \u001b[0m | \u001b[0m 7.272   \u001b[0m | \u001b[0m 25.61   \u001b[0m | \u001b[0m 4.631   \u001b[0m | \u001b[0m 1.287   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "{'objective': 'reg:squarederror', 'early_stopping_round': 50, 'eval_metric': 'rmse', 'colsample_bytree': 0.7415820237405875, 'subsample': 0.9875025924634507, 'min_child_weight': 27, 'max_depth': 9, 'eta': 0.02, 'reg_alpha': 4.575465304651051, 'reg_lambda': 3.3858651807679534}\n",
      "[13:21:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:21:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-3.817   \u001b[0m | \u001b[0m 0.7416  \u001b[0m | \u001b[0m 0.02    \u001b[0m | \u001b[0m 9.347   \u001b[0m | \u001b[0m 27.15   \u001b[0m | \u001b[0m 4.575   \u001b[0m | \u001b[0m 3.386   \u001b[0m | \u001b[0m 0.9875  \u001b[0m |\n",
      "{'objective': 'reg:squarederror', 'early_stopping_round': 50, 'eval_metric': 'rmse', 'colsample_bytree': 0.5, 'subsample': 0.733927508291536, 'min_child_weight': 27, 'max_depth': 11, 'eta': 0.02, 'reg_alpha': 3.32497669686325, 'reg_lambda': 4.9571016974842514}\n",
      "[13:25:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:25:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "| \u001b[95m 24      \u001b[0m | \u001b[95m-3.803   \u001b[0m | \u001b[95m 0.5     \u001b[0m | \u001b[95m 0.02    \u001b[0m | \u001b[95m 11.71   \u001b[0m | \u001b[95m 27.6    \u001b[0m | \u001b[95m 3.325   \u001b[0m | \u001b[95m 4.957   \u001b[0m | \u001b[95m 0.7339  \u001b[0m |\n",
      "{'objective': 'reg:squarederror', 'early_stopping_round': 50, 'eval_metric': 'rmse', 'colsample_bytree': 0.5, 'subsample': 0.5, 'min_child_weight': 26, 'max_depth': 12, 'eta': 0.02, 'reg_alpha': 3.081921668614422, 'reg_lambda': 3.0163834184284712}\n",
      "[13:27:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:27:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-3.813   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.02    \u001b[0m | \u001b[0m 12.0    \u001b[0m | \u001b[0m 26.01   \u001b[0m | \u001b[0m 3.082   \u001b[0m | \u001b[0m 3.016   \u001b[0m | \u001b[0m 0.5     \u001b[0m |\n",
      "{'objective': 'reg:squarederror', 'early_stopping_round': 50, 'eval_metric': 'rmse', 'colsample_bytree': 1.0, 'subsample': 1.0, 'min_child_weight': 25, 'max_depth': 12, 'eta': 0.2, 'reg_alpha': 5.0, 'reg_lambda': 5.0}\n",
      "[13:30:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:30:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m-3.864   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.2     \u001b[0m | \u001b[0m 12.0    \u001b[0m | \u001b[0m 25.92   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "=============================================================================================================\n",
      "-3.8032684999999997 {'colsample_bytree': 0.5, 'eta': 0.02, 'max_depth': 11.708480184158471, 'min_child_weight': 27.598200792362622, 'reg_alpha': 3.32497669686325, 'reg_lambda': 4.9571016974842514, 'subsample': 0.733927508291536}\n"
     ]
    }
   ],
   "source": [
    "best_clf = param_beyesian(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dd77aa-6404-46c1-9cf9-31693371eb40",
   "metadata": {},
   "source": [
    "接下来输出模型预测结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07424b25-6b20-4b20-84b9-9207d7884331",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-9e501afc8b2a>:87: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  prediction_train = pd.Series()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:3.93743\teval-rmse:3.94887\n",
      "[50]\ttrain-rmse:3.49914\teval-rmse:3.73093\n",
      "[100]\ttrain-rmse:3.32144\teval-rmse:3.68839\n",
      "[150]\ttrain-rmse:3.23772\teval-rmse:3.67804\n",
      "[200]\ttrain-rmse:3.20197\teval-rmse:3.67385\n",
      "[250]\ttrain-rmse:3.17762\teval-rmse:3.67253\n",
      "[300]\ttrain-rmse:3.15443\teval-rmse:3.67234\n",
      "[302]\ttrain-rmse:3.15403\teval-rmse:3.67244\n",
      "[0]\ttrain-rmse:3.94448\teval-rmse:3.92137\n",
      "[50]\ttrain-rmse:3.50756\teval-rmse:3.69474\n",
      "[100]\ttrain-rmse:3.32726\teval-rmse:3.64895\n",
      "[150]\ttrain-rmse:3.24642\teval-rmse:3.63666\n",
      "[200]\ttrain-rmse:3.20761\teval-rmse:3.63409\n",
      "[250]\ttrain-rmse:3.18269\teval-rmse:3.63341\n",
      "[300]\ttrain-rmse:3.15935\teval-rmse:3.63319\n",
      "[350]\ttrain-rmse:3.13936\teval-rmse:3.63291\n",
      "[354]\ttrain-rmse:3.13787\teval-rmse:3.63293\n",
      "[0]\ttrain-rmse:3.93301\teval-rmse:3.96766\n",
      "[50]\ttrain-rmse:3.48744\teval-rmse:3.75187\n",
      "[100]\ttrain-rmse:3.30316\teval-rmse:3.71049\n",
      "[150]\ttrain-rmse:3.21876\teval-rmse:3.70025\n",
      "[200]\ttrain-rmse:3.18515\teval-rmse:3.69695\n",
      "[250]\ttrain-rmse:3.16190\teval-rmse:3.69627\n",
      "[300]\ttrain-rmse:3.14040\teval-rmse:3.69544\n",
      "[321]\ttrain-rmse:3.13221\teval-rmse:3.69595\n",
      "[0]\ttrain-rmse:3.91287\teval-rmse:4.04674\n",
      "[50]\ttrain-rmse:3.47627\teval-rmse:3.82653\n",
      "[100]\ttrain-rmse:3.28783\teval-rmse:3.78026\n",
      "[150]\ttrain-rmse:3.20957\teval-rmse:3.76870\n",
      "[200]\ttrain-rmse:3.17786\teval-rmse:3.76575\n",
      "[250]\ttrain-rmse:3.15028\teval-rmse:3.76448\n",
      "[300]\ttrain-rmse:3.12888\teval-rmse:3.76443\n",
      "[314]\ttrain-rmse:3.12480\teval-rmse:3.76418\n",
      "[0]\ttrain-rmse:3.96661\teval-rmse:3.83160\n",
      "[50]\ttrain-rmse:3.52079\teval-rmse:3.61955\n",
      "[100]\ttrain-rmse:3.33833\teval-rmse:3.57930\n",
      "[150]\ttrain-rmse:3.26075\teval-rmse:3.56961\n",
      "[200]\ttrain-rmse:3.22778\teval-rmse:3.56733\n",
      "[250]\ttrain-rmse:3.19642\teval-rmse:3.56590\n",
      "[289]\ttrain-rmse:3.17949\teval-rmse:3.56588\n",
      "[3.6724352915788456, 3.632935489439316, 3.6959392218600025, 3.7641771493981997, 3.5658763997083103] 3.6662727103969344\n"
     ]
    }
   ],
   "source": [
    "train_predict(train_x, test_x, best_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c73b7d3-85d0-4920-a4aa-2984108837fb",
   "metadata": {},
   "source": [
    "并将结果提交至竞赛主页"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc417de-f7da-4304-8172-983798bb8a29",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/09/qHtbVi8eURxBFuS.png\" alt=\"image-20211209153243352\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e3f223-6bd7-422b-830e-c46d36344621",
   "metadata": {},
   "source": [
    "对比此前模型结果，能够发现目前XBG的模型结果较好，这其中差异极有可能是新增的NLP特征所导致的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f69a3c-d282-42e0-9c3c-a4ab9d9dd892",
   "metadata": {},
   "source": [
    "| 模型 | Private Score | Public Score |\n",
    "| ------ | ------ | ------ |\n",
    "| randomforest | 3.65455 | 3.74969 |\n",
    "| randomforest+validation | 3.65173 | 3.74954 |\n",
    "| LightGBM | 3.69723 | 3.80436 |\n",
    "| LightGBM+validation | 3.64403 | 3.73875 |\n",
    "| XGBoost | 3.62832 | 3.72358 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea308d1a-551b-46a6-ad13-65e004b2b8e6",
   "metadata": {},
   "source": [
    "&emsp;&emsp;此外，课后同学们也可以进一步尝试在LightGBM和随机森林中带入NLP特征来进行计算，尝试是否能够进一步提分。当然，在各项单模型预测结束后，我们即可考虑进行模型融合了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49bcc18-ac13-4252-aca7-1cf441898db1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2748f25b-9ef3-45da-ad06-5a090a3ed6a1",
   "metadata": {},
   "source": [
    "## <center> 三、模型融合策略(一)\n",
    "## <center> **Voting融合**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6625c514-f671-4f06-b68c-1676e8389a10",
   "metadata": {},
   "source": [
    "&emsp;&emsp;整体来看，常用模型融合的策略有两种，分别是Voting融合与Stacking融合，模型融合的目的和集成模型中的集成过程类似，都是希望能够尽可能借助不同模型的优势，最终输出一个更加可靠的结果。在Voting过程中，我们只需要对不同模型对测试集的预测结果进行加权汇总即可，而Stacking则相对复杂，需要借助此前不同模型的验证集预测结果和测试集预测结果再次进行模型训练，以验证集预测结果为训练集、训练集标签为标签构建新的训练集，在此模型上进行训练，然后以测试集预测结果作为新的预测集，并在新预测集上进行预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca15c86-0b44-4fdc-ae28-6c2cbabf062c",
   "metadata": {},
   "source": [
    "### 1.均值融合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcdb8ea-a4e5-4577-9e5e-addec4723a5f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先我们来看Voting融合过程。一般来说Voting融合也可以分为均值融合（多组预测结果求均值）、加权融合（根据某种方式赋予不同预测结果不同权重而后进行求和）以及Trick融合（根据某种特殊的规则赋予权重而后进行求和）三种，此处先介绍均值融合与加权融合的基本过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0981fe-6057-462e-9d86-469507e6f4b6",
   "metadata": {},
   "source": [
    "- 拼接不同模型预测结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def7ff10-600f-413c-ac87-9aa628003451",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先，对随机森林、LightGBM和XGBoost模型预测结果进行读取，并简单查看三者相关系数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2964118f-c632-4878-97db-14e0e47b0477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                target  randomforest  lightgbm   xgboost\n",
      "target        1.000000      1.000000  0.956251  0.947529\n",
      "randomforest  1.000000      1.000000  0.956251  0.947529\n",
      "lightgbm      0.956251      0.956251  1.000000  0.951461\n",
      "xgboost       0.947529      0.947529  0.951461  1.000000\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"result/submission_randomforest.csv\")\n",
    "data['randomforest'] = data['target'].values\n",
    "\n",
    "temp = pd.read_csv(\"result/submission_lightgbm.csv\")\n",
    "data['lightgbm'] = temp['target'].values\n",
    "\n",
    "\n",
    "temp = pd.read_csv(\"result/submission_xgboost.csv\")\n",
    "data['xgboost'] = temp['target'].values\n",
    "\n",
    "print(data.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "012ddc2d-3046-4a00-9b48-649ab3145a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card_id</th>\n",
       "      <th>target</th>\n",
       "      <th>randomforest</th>\n",
       "      <th>lightgbm</th>\n",
       "      <th>xgboost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C_ID_0ab67a22ab</td>\n",
       "      <td>-3.528347</td>\n",
       "      <td>-3.528347</td>\n",
       "      <td>-3.576328</td>\n",
       "      <td>-3.737840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C_ID_130fd0cbdd</td>\n",
       "      <td>-0.789489</td>\n",
       "      <td>-0.789489</td>\n",
       "      <td>-0.866846</td>\n",
       "      <td>-0.540275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C_ID_b709037bc5</td>\n",
       "      <td>-0.380266</td>\n",
       "      <td>-0.380266</td>\n",
       "      <td>-0.370968</td>\n",
       "      <td>-0.433453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C_ID_d27d835a9f</td>\n",
       "      <td>-0.269844</td>\n",
       "      <td>-0.269844</td>\n",
       "      <td>-0.121097</td>\n",
       "      <td>-0.176081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C_ID_2b5e3df5c2</td>\n",
       "      <td>-1.038557</td>\n",
       "      <td>-1.038557</td>\n",
       "      <td>-1.047017</td>\n",
       "      <td>-1.027519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           card_id    target  randomforest  lightgbm   xgboost\n",
       "0  C_ID_0ab67a22ab -3.528347     -3.528347 -3.576328 -3.737840\n",
       "1  C_ID_130fd0cbdd -0.789489     -0.789489 -0.866846 -0.540275\n",
       "2  C_ID_b709037bc5 -0.380266     -0.380266 -0.370968 -0.433453\n",
       "3  C_ID_d27d835a9f -0.269844     -0.269844 -0.121097 -0.176081\n",
       "4  C_ID_2b5e3df5c2 -1.038557     -1.038557 -1.047017 -1.027519"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8712a8ac-e96e-44f3-a17e-80c20326a085",
   "metadata": {},
   "source": [
    "然后计算不同模型预测结果的均值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "32aa3998-d98f-4984-b423-81c9e2a7fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'] = (data['randomforest'] + data['lightgbm'] + data['xgboost']) / 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebea046d-36b3-4648-9587-1b854854e9bb",
   "metadata": {},
   "source": [
    "写入结果文件并提交："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6b920210-db28-4f67-b7d7-3fa0c136b8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['card_id','target']].to_csv(\"result/voting_avr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b88f08f-b09a-4ac7-8208-44cc100432f6",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/09/ulfKDHhYEkR3j8Q.png\" alt=\"image-20211209184223461\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6da6e5c-66c2-4813-9f44-a1c22c8d9b1a",
   "metadata": {},
   "source": [
    "| 模型 | Private Score | Public Score |\n",
    "| ------ | ------ | ------ |\n",
    "| randomforest | 3.65455 | 3.74969 |\n",
    "| randomforest+validation | 3.65173 | 3.74954 |\n",
    "| LightGBM | 3.69723 | 3.80436 |\n",
    "| LightGBM+validation | 3.64403 | 3.73875 |\n",
    "| XGBoost | 3.62832 | 3.72358 |\n",
    "| Voting_avr | 3.63650 | 3.73251 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bc9fad-dd76-4452-90ac-6c40680fda77",
   "metadata": {},
   "source": [
    "能够发现，简单的均值融合并不能有效果上的提升，接下来我们尝试加权融合。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc77d30-3df2-4c89-a8f2-e7cfb1db79a2",
   "metadata": {},
   "source": [
    "### 2.加权融合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b0531-d488-48d5-9839-a71db57bbbae",
   "metadata": {},
   "source": [
    "&emsp;&emsp;加权融合的思路并不复杂，从客观计算流程上来看我们将赋予不同模型训练结果以不同权重，而具体权重的分配，我们可以根据三组模型在公榜上的评分决定，即假设模型A和B分别是2分和3分（分数越低越好的情况下），则在实际加权过程中，我们将赋予A模型结果3/5权重，B模型2/5权重，因此，加权融合过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f6371765-9cb0-44a9-a8c1-34313e1e8de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'] = data['randomforest']*0.2+data['lightgbm']*0.3 + data['xgboost']*0.5\n",
    "data[['card_id','target']].to_csv(\"result/voting_wei1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab407dc-6348-4317-b907-aca7f8649a0a",
   "metadata": {},
   "source": [
    "输出结果如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c66d563-fc62-4f58-9e16-cfb28027ebcb",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/09/6ylLOD1ueoWvV4z.png\" alt=\"image-20211209185438900\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f866ac-bdf4-4469-b150-635a2219d6d0",
   "metadata": {},
   "source": [
    "| 模型 | Private Score | Public Score |\n",
    "| ------ | ------ | ------ |\n",
    "| randomforest | 3.65455 | 3.74969 |\n",
    "| randomforest+validation | 3.65173 | 3.74954 |\n",
    "| LightGBM | 3.69723 | 3.80436 |\n",
    "| LightGBM+validation | 3.64403 | 3.73875 |\n",
    "| XGBoost | 3.62832 | 3.72358 |\n",
    "| Voting_avr | 3.63650 | 3.73251 |\n",
    "| Voting_wei | 3.633307 | 3.72877 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe60c84e-f7f7-4b98-b63e-d0bc71f9183b",
   "metadata": {},
   "source": [
    "能够发现结果略微有所改善，但实际结果仍然不如单模型结果，此处预测极有可能是因额外i标签中存在异常值导致。接下来继续尝试Stacking融合。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4390e40-ee15-4b07-bb73-a1db53856d8c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92ae21a-5c39-4017-b919-ed36e06b1235",
   "metadata": {},
   "source": [
    "## <center> 四、模型融合策略（二）\n",
    "## <center> **Stacking融合**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3030ea48-72d1-4388-8f7c-b3809fb324c2",
   "metadata": {},
   "source": [
    "&emsp;&emsp;此处我们考虑手动进行Stacking融合，在此前的模型训练中，我们已经创建了predication_train和predication_test数据集，这两个数据集将作为训练集、测试集带入到下一轮的建模中，而本轮建模也被称为Stacking融合。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f6c6df-186c-4fb4-bca4-79a19e418d0f",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/08/ALF3cfuSwmB7b8z.png\" alt=\"image-20211208192640281\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bc8951-92cf-4720-b3e8-49751a0ab631",
   "metadata": {},
   "source": [
    "- 数据集校验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2f3ceb-f933-47a5-ad48-68a066dc79a4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先快速查看此前创建的相关数据集，其中oof就是训练数据集的预测结果（多轮验证集预测结果拼接而来），而predictions则是此前单模型的预测结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "03614ed9-ed4a-478e-be51-d188d6e54730",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_rf  = pd.read_csv('./preprocess/train_randomforest.csv')\n",
    "predictions_rf  = pd.read_csv('./preprocess/test_randomforest.csv')\n",
    "\n",
    "oof_lgb  = pd.read_csv('./preprocess/train_lightgbm.csv')\n",
    "predictions_lgb  = pd.read_csv('./preprocess/test_lightgbm.csv')\n",
    "\n",
    "oof_xgb  = pd.read_csv('./preprocess/train_xgboost.csv')\n",
    "predictions_xgb  = pd.read_csv('./preprocess/test_xgboost.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d4fb075d-480c-4752-bfc8-262f5261d1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.426984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.857115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.402431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.063488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.278658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0 -0.426984\n",
       "1 -1.857115\n",
       "2  0.402431\n",
       "3 -0.063488\n",
       "4 -0.278658"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_rf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f6b95102-00ba-471a-9cf6-cb527399502e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.576328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.866846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.370968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.121097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.047017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0 -3.576328\n",
       "1 -0.866846\n",
       "2 -0.370968\n",
       "3 -0.121097\n",
       "4 -1.047017"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_lgb.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8f2ef663-3a4e-41be-abce-c30cbbd17728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((201917, 1), (201917, 1))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_rf.shape, oof_lgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "47f56584-ac6e-4875-8ac4-1feda0be002b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((123623, 1), (123623, 1))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_rf.shape, predictions_lgb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e434e38d-a4cf-4aa1-8dd3-50d4cee030f5",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来进行Stacking模型融合，该过程本身并不复杂，重点需要清洗不同数据集的创建过程与调用过程，同时为了保证模型泛化能力，我们需要对其进行交叉验证："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "25a4a93e-dbb8-4a4a-b999-eae6b7279aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_model(oof_1, oof_2, oof_3, predictions_1, predictions_2, predictions_3, y):\n",
    "   \n",
    "    # Part 1.数据准备\n",
    "    # 按行拼接列，拼接验证集所有预测结果\n",
    "    # train_stack就是final model的训练数据\n",
    "    train_stack = np.hstack([oof_1, oof_2, oof_3])\n",
    "    # 按行拼接列，拼接测试集上所有预测结果\n",
    "    # test_stack就是final model的测试数据\n",
    "    test_stack = np.hstack([predictions_1, predictions_2, predictions_3])\n",
    "    # 创建一个和验证集行数相同的全零数组\n",
    "    # oof = np.zeros(train_stack.shape[0])\n",
    "    # 创建一个和测试集行数相同的全零数组\n",
    "    predictions = np.zeros(test_stack.shape[0])\n",
    "    \n",
    "    # Part 2.多轮交叉验证\n",
    "    from sklearn.model_selection import RepeatedKFold\n",
    "    folds = RepeatedKFold(n_splits=5, n_repeats=2, random_state=2020)\n",
    "    \n",
    "    # fold_为折数，trn_idx为每一折训练集index，val_idx为每一折验证集index\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_stack, y)):\n",
    "        # 打印折数信息\n",
    "        print(\"fold n°{}\".format(fold_+1))\n",
    "        # 训练集中划分为训练数据的特征和标签\n",
    "        trn_data, trn_y = train_stack[trn_idx], y[trn_idx]\n",
    "        # 训练集中划分为验证数据的特征和标签\n",
    "        val_data, val_y = train_stack[val_idx], y[val_idx]\n",
    "        # 开始训练时提示\n",
    "        print(\"-\" * 10 + \"Stacking \" + str(fold_+1) + \"-\" * 10)\n",
    "        # 采用贝叶斯回归作为结果融合的模型（final model）\n",
    "        clf = BayesianRidge()\n",
    "        # 在训练数据上进行训练\n",
    "        clf.fit(trn_data, trn_y)\n",
    "        # 在验证数据上进行预测，并将结果记录在oof对应位置\n",
    "        # oof[val_idx] = clf.predict(val_data)\n",
    "        # 对测试集数据进行预测，每一轮预测结果占比额外的1/10\n",
    "        predictions += clf.predict(test_stack) / (5 * 2)\n",
    "    \n",
    "    # 返回测试集的预测结果\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea23fe5-e663-4ee3-b0b7-fd89f91e1f2f",
   "metadata": {},
   "source": [
    "接下来执行模型融合过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "40205135-6346-4ce3-9dea-5fcf01915ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "04ed450d-6bc8-4d4c-8193-12f325badfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "----------Stacking 1----------\n",
      "fold n°2\n",
      "----------Stacking 2----------\n",
      "fold n°3\n",
      "----------Stacking 3----------\n",
      "fold n°4\n",
      "----------Stacking 4----------\n",
      "fold n°5\n",
      "----------Stacking 5----------\n",
      "fold n°6\n",
      "----------Stacking 6----------\n",
      "fold n°7\n",
      "----------Stacking 7----------\n",
      "fold n°8\n",
      "----------Stacking 8----------\n",
      "fold n°9\n",
      "----------Stacking 9----------\n",
      "fold n°10\n",
      "----------Stacking 10----------\n"
     ]
    }
   ],
   "source": [
    "predictions_stack  = stack_model(oof_rf, oof_lgb, oof_xgb, \n",
    "                                 predictions_rf, predictions_lgb, predictions_xgb, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96e7b9b-91f7-4024-b69f-c6ad3288c96d",
   "metadata": {},
   "source": [
    "查看输出结果，并将结果进行提交："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "98c8f593-bb50-4084-b746-22f60ad2b9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.05848444, -0.68964305, -0.41582154, ...,  0.65614588,\n",
       "       -2.35990886,  0.39282516])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "237aa9dc-01a8-4b0a-961b-4cb3303b1381",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = predictions_stack\n",
    "sub_df.to_csv('predictions_stack1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e966cd83-8e08-4b33-94f7-01af08ae1d65",
   "metadata": {},
   "source": [
    "能够看到最终结果如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c950e05-3248-49b6-a209-f5168eea0f23",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/09/JCjNoY47eRVGyTU.png\" alt=\"image-20211209190553412\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329ecaa9-ef5d-448f-83cd-1537fdfbe416",
   "metadata": {},
   "source": [
    "| 模型 | Private Score | Public Score |\n",
    "| ------ | ------ | ------ |\n",
    "| randomforest | 3.65455 | 3.74969 |\n",
    "| randomforest+validation | 3.65173 | 3.74954 |\n",
    "| LightGBM | 3.69723 | 3.80436 |\n",
    "| LightGBM+validation | 3.64403 | 3.73875 |\n",
    "| XGBoost | 3.62832 | 3.72358 |\n",
    "| Voting_avr | 3.63650 | 3.73251 |\n",
    "| Voting_wei | 3.633307 | 3.72877 |\n",
    "| Stacking | 3.62798 | 3.72055 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf267755-5cf5-45fa-a1bb-7001d18a6159",
   "metadata": {},
   "source": [
    "&emsp;&emsp;截至目前，我们已经尝试了在当前数据情况下的所有有效方法，而通过这一些列尝试，我们也得到了目前最好结果，而整体的排名也从Baseline的前40%提高到20%左右。当然，当这些通用方法都进行了尝试之后，接下来我们需要进一步从更加具体的角度来看当前数据集的数据情况，并据此提出一些跟进一步的优化方法，并更进一步的提高最终预测结果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
